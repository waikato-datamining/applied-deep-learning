{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Over the years, we have been working on a number of projects that involved applying deep learning algorithms to image problems. Setting up deep learning frameworks is always a slow and painstaking process, getting all the library dependencies right (CUDA, cuDNN, numpy, etc). To speed things up, we have developed (and maintain) a number of Python libraries and Docker images that can be used for various deep learning tasks. Here we are showing examples on how you can use these images on datasets to train your own models and how to apply them. The use of Docker made it a lot easier and faster to apply algorithms to new datasets. If you have not used Docker before, we recommend you to have a look at our introduction called Docker for Data Scientists . The following domains are covered with examples: Image classification Object detection Instance segmentation Image segmentation","title":"Home"},{"location":"prerequisites/","text":"The following prerequisites apply to most data domains: Annotations In order to annotate data (e.g., for object detection, instance segmentation or image segmentation), you should download the ADAMS Annotator application (you only need Java 11 installed ): Snapshot Release 2021.12.0 or later Format conversions For turning annotations from one format into another, you need to install the wai.annotations Python library: wai.annotations project wai.annotations manual NB: it is recommended to install it in a virtual environment to avoid any conflicts with the host system. Hardware For building models, a computer with NVIDIA GPU (8GB+) and Linux operating system is recommended. Only image classification models can be built in a reasonable amount of time on CPU-only machines. Directory structure Create the following directory structure for the examples of this tutorial: | +-- applied_deep_learning | +-- cache | +-- data | +-- output | +-- predictions | +-- in | +-- out","title":"Prerequisites"},{"location":"prerequisites/#annotations","text":"In order to annotate data (e.g., for object detection, instance segmentation or image segmentation), you should download the ADAMS Annotator application (you only need Java 11 installed ): Snapshot Release 2021.12.0 or later","title":"Annotations"},{"location":"prerequisites/#format-conversions","text":"For turning annotations from one format into another, you need to install the wai.annotations Python library: wai.annotations project wai.annotations manual NB: it is recommended to install it in a virtual environment to avoid any conflicts with the host system.","title":"Format conversions"},{"location":"prerequisites/#hardware","text":"For building models, a computer with NVIDIA GPU (8GB+) and Linux operating system is recommended. Only image classification models can be built in a reasonable amount of time on CPU-only machines.","title":"Hardware"},{"location":"prerequisites/#directory-structure","text":"Create the following directory structure for the examples of this tutorial: | +-- applied_deep_learning | +-- cache | +-- data | +-- output | +-- predictions | +-- in | +-- out","title":"Directory structure"},{"location":"image_classification/","text":"Image classification is the simplest and least computational expensive task, as it classifies whole images, assigning them a category. Before you delve into training models, take a look at how you need to annotate your data . The following frameworks are available: MMClassification wai.pytorchimageclass wai.tfimageclass","title":"Introduction"},{"location":"image_classification/annotate/","text":"Annotating data for image classification is straight forward. Since we are classifying whole images, we can simply use the directory layout for annotating images. In the example below, we have a dataset called flowers . The sub-directories below the flowers directory represent the labels for the images containing within the sub-directories. | +- flowers | +- daisy | +- dandelion | +- roses | +- sunflowers | +- tulip NB: To avoid any issues, the labels should be lowercase and underscores should be used instead of blanks/spaces.","title":"Annotate"},{"location":"image_classification/mmclassification/","text":"","title":"MMClassification"},{"location":"image_classification/wai.pytorchimageclass/","text":"With wai.pytorchimageclass it is possible to train various image classification network architectures and also perform inference. The code is based on this Pytorch example: https://github.com/pytorch/examples/tree/master/imagenet Specifically, this commit: 49e1a8847c8c4d8d3c576479cb2fe2fd2ac583de","title":"wai.pytorchimageclass"},{"location":"image_classification/wai.tfimageclass/","text":"The wai.tfimageclass Python library ( PyPI ) can be used for training various models that are available from the Tensorflow Hub . Prerequisites Make sure you have the directory structure created as outlined in the Prerequisites . Data In this example, we will use the 102 flowers dataset, which consists of 102 different categories (~ species) of flowers. More precisely, we will download the dataset with the flowers already split into categories from which will use a subset to speed up the training process. Download the dataset from the following URL and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/102flowers/102flowers-subdir.zip Once extracted, you can delete all sub-directories apart from: alpine_sea_holly anthurium artichoke Rename the subdir directory to 3flowers and move it into the data folder of our directory structure outlined. Training For training, we will use the following docker image: waikatodatamining/tf_image_classification:1.14 If you only have a CPU machine available, then use this one instead: waikatodatamining/tf_image_classification:1.14_cpu The training script is called tfic-retrain , for which we can invoke the help screen as follows: docker run -t waikatodatamining/tf_image_classification:1.14 tfic-retrain --help # GPU docker run -t waikatodatamining/tf_image_classification:1.14_cpu tfic-retrain --help # CPU It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: 3flowers-default The following command, issued in the applied_deep_learning directory, will map the applied_deep_learning directory (= the current working directory) onto the /workspace directory within the docker container, giving us access to all the sub-directories there, and train our 3flowers dataset ( -u $(id -u):$(id -g) maps the user and group ID): GPU: docker run \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules -u $(id -u):$(id -g) \\ --gpus=all \\ -t waikatodatamining/tf_image_classification:1.14 \\ tfic-retrain \\ --image_dir /workspace/data/3flowers \\ --output_graph /workspace/output/3flowers-default/graph.pb \\ --output_info /workspace/output/3flowers-default/graph.json \\ --saved_model_dir /workspace/output/3flowers-default/saved_model \\ --image_lists_dir /workspace/output/3flowers-default \\ --training_steps 500 CPU: docker run \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules -u $(id -u):$(id -g) \\ -t waikatodatamining/tf_image_classification:1.14_cpu \\ tfic-retrain \\ --image_dir /workspace/data/3flowers \\ --output_graph /workspace/output/3flowers-default/graph.pb \\ --output_info /workspace/output/3flowers-default/graph.json \\ --saved_model_dir /workspace/output/3flowers-default/saved_model \\ --image_lists_dir /workspace/output/3flowers-default \\ --training_steps 500 Exporting model Before we can use the model, we need to export it to Tensorflow lite or tflite : GPU: docker run \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules -u $(id -u):$(id -g) \\ --gpus=all \\ -t waikatodatamining/tf_image_classification:1.14 \\ tfic-export \\ --saved_model_dir /workspace/output/3flowers-default/saved_model \\ --tflite_model /workspace/output/3flowers-default/graph.tflite CPU: docker run \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules -u $(id -u):$(id -g) \\ -t waikatodatamining/tf_image_classification:1.14_cpu \\ tfic-export \\ --saved_model_dir /workspace/output/3flowers-default/saved_model \\ --tflite_model /workspace/output/3flowers-default/graph.tflite Predicting For making predictions for a single image, you can use the script tfic-labelimage . Since we will want to batch predict multiple images, will use the script tfic-poll instead: GPU: docker run \\ -v `pwd`:/workspace \\ -u $(id -u):$(id -g) \\ -t waikatodatamining/tf_image_classification:1.14 \\ tfic-poll \\ --graph /workspace/output/3flowers-default/graph.tflite \\ --graph_type tflite \\ --info /workspace/output/3flowers-default/graph.json \\ --in_dir /workspace/predictions/in \\ --out_dir /workspace/predictions/out CPU: docker run \\ -v `pwd`:/workspace \\ -u $(id -u):$(id -g) \\ -t waikatodatamining/tf_image_classification:1.14_cpu \\ tfic-poll \\ --graph /workspace/output/3flowers-default/graph.tflite \\ --graph_type tflite \\ --info /workspace/output/3flowers-default/graph.json \\ --in_dir /workspace/predictions/in \\ --out_dir /workspace/predictions/out E.g., for the image_01965.jpg from the anthurium , we will get a CSV file similar to this one : label probability anthurium 0.989429 artichoke 0.0060946 alpine_sea_holly 0.00447612","title":"wai.tfimageclass"},{"location":"image_classification/wai.tfimageclass/#prerequisites","text":"Make sure you have the directory structure created as outlined in the Prerequisites .","title":"Prerequisites"},{"location":"image_classification/wai.tfimageclass/#data","text":"In this example, we will use the 102 flowers dataset, which consists of 102 different categories (~ species) of flowers. More precisely, we will download the dataset with the flowers already split into categories from which will use a subset to speed up the training process. Download the dataset from the following URL and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/102flowers/102flowers-subdir.zip Once extracted, you can delete all sub-directories apart from: alpine_sea_holly anthurium artichoke Rename the subdir directory to 3flowers and move it into the data folder of our directory structure outlined.","title":"Data"},{"location":"image_classification/wai.tfimageclass/#training","text":"For training, we will use the following docker image: waikatodatamining/tf_image_classification:1.14 If you only have a CPU machine available, then use this one instead: waikatodatamining/tf_image_classification:1.14_cpu The training script is called tfic-retrain , for which we can invoke the help screen as follows: docker run -t waikatodatamining/tf_image_classification:1.14 tfic-retrain --help # GPU docker run -t waikatodatamining/tf_image_classification:1.14_cpu tfic-retrain --help # CPU It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: 3flowers-default The following command, issued in the applied_deep_learning directory, will map the applied_deep_learning directory (= the current working directory) onto the /workspace directory within the docker container, giving us access to all the sub-directories there, and train our 3flowers dataset ( -u $(id -u):$(id -g) maps the user and group ID): GPU: docker run \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules -u $(id -u):$(id -g) \\ --gpus=all \\ -t waikatodatamining/tf_image_classification:1.14 \\ tfic-retrain \\ --image_dir /workspace/data/3flowers \\ --output_graph /workspace/output/3flowers-default/graph.pb \\ --output_info /workspace/output/3flowers-default/graph.json \\ --saved_model_dir /workspace/output/3flowers-default/saved_model \\ --image_lists_dir /workspace/output/3flowers-default \\ --training_steps 500 CPU: docker run \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules -u $(id -u):$(id -g) \\ -t waikatodatamining/tf_image_classification:1.14_cpu \\ tfic-retrain \\ --image_dir /workspace/data/3flowers \\ --output_graph /workspace/output/3flowers-default/graph.pb \\ --output_info /workspace/output/3flowers-default/graph.json \\ --saved_model_dir /workspace/output/3flowers-default/saved_model \\ --image_lists_dir /workspace/output/3flowers-default \\ --training_steps 500","title":"Training"},{"location":"image_classification/wai.tfimageclass/#exporting-model","text":"Before we can use the model, we need to export it to Tensorflow lite or tflite : GPU: docker run \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules -u $(id -u):$(id -g) \\ --gpus=all \\ -t waikatodatamining/tf_image_classification:1.14 \\ tfic-export \\ --saved_model_dir /workspace/output/3flowers-default/saved_model \\ --tflite_model /workspace/output/3flowers-default/graph.tflite CPU: docker run \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules -u $(id -u):$(id -g) \\ -t waikatodatamining/tf_image_classification:1.14_cpu \\ tfic-export \\ --saved_model_dir /workspace/output/3flowers-default/saved_model \\ --tflite_model /workspace/output/3flowers-default/graph.tflite","title":"Exporting model"},{"location":"image_classification/wai.tfimageclass/#predicting","text":"For making predictions for a single image, you can use the script tfic-labelimage . Since we will want to batch predict multiple images, will use the script tfic-poll instead: GPU: docker run \\ -v `pwd`:/workspace \\ -u $(id -u):$(id -g) \\ -t waikatodatamining/tf_image_classification:1.14 \\ tfic-poll \\ --graph /workspace/output/3flowers-default/graph.tflite \\ --graph_type tflite \\ --info /workspace/output/3flowers-default/graph.json \\ --in_dir /workspace/predictions/in \\ --out_dir /workspace/predictions/out CPU: docker run \\ -v `pwd`:/workspace \\ -u $(id -u):$(id -g) \\ -t waikatodatamining/tf_image_classification:1.14_cpu \\ tfic-poll \\ --graph /workspace/output/3flowers-default/graph.tflite \\ --graph_type tflite \\ --info /workspace/output/3flowers-default/graph.json \\ --in_dir /workspace/predictions/in \\ --out_dir /workspace/predictions/out E.g., for the image_01965.jpg from the anthurium , we will get a CSV file similar to this one : label probability anthurium 0.989429 artichoke 0.0060946 alpine_sea_holly 0.00447612","title":"Predicting"},{"location":"image_segmentation/","text":"Image segmentation classifies individual pixels within an image, assigning them a label (i.e., a color). Before you delve into training models, take a look at how you need to annotate your data . The following frameworks are available: Image Segmentation Keras MMSegmentation Segmentation models","title":"Introduction"},{"location":"image_segmentation/annotate/","text":"You can use the ADAMS framework for annotating your images. ADAMS comes pre-bundled in various setups and we need the adams-annotator bundle. You can get either: snapshot release (post 21.12.0 release) In the Flow editor (available from the Tools menu in ADAMS), you can load and execute the follow workflow (which is part of your ADAMS installation when downloading it as zip file): adams-imaging-image_segmentation_annotation.flow The following video takes you through the process:","title":"Annotate"},{"location":"image_segmentation/image-segmentation-keras/","text":"","title":"Image Segmentation Keras"},{"location":"image_segmentation/mmsegmentation/","text":"","title":"MMSegmentation"},{"location":"image_segmentation/segmentation_models/","text":"","title":"Segmentation models"},{"location":"instance_segmentation/","text":"Instance segmentation not only finds objects within images, it also learns the shape of the objects rather than just a simple rectangle/bounding box. However, this makes the algorithms computationally more expensive and more memory hungry. Before you delve into training models, take a look at how you need to annotate your data . The following frameworks are available: Detectron2","title":"Introduction"},{"location":"instance_segmentation/annotate/","text":"You can use the ADAMS framework for annotating your images. ADAMS comes pre-bundled in various setups and we need the adams-annotator bundle. You can get either: snapshot release (21.12.0 or later) In the Flow editor (available from the Tools menu in ADAMS), you can load and execute the follow workflow (which is part of your ADAMS installation when downloading it as zip file): adams-imaging-annotate_objects.flow When prompted, use object_shape as Selection type . The following video takes you through the process:","title":"Annotate"},{"location":"instance_segmentation/detectron2/","text":"","title":"Detectron2"},{"location":"object_detection/","text":"Object detection detects one or more objects within an image and assigns each one of them a label, as opposed to image classification which assigns a lable to the whole image. The predicted locations are typically rectangles (aka bounding boxes). If you are looking for shapes or polygons, then have a look at instance segmentation . Before you delve into training models, take a look at how you need to annotate your data . The following frameworks are available: MMDetection Yolov5","title":"Introduction"},{"location":"object_detection/annotate/","text":"You can use the ADAMS framework for annotating your images. ADAMS comes pre-bundled in various setups and we need the adams-annotator bundle. You can get either: snapshot release (21.12.0 or later) In the Flow editor (available from the Tools menu in ADAMS), you can load and execute the follow workflow (which is part of your ADAMS installation when downloading it as zip file): adams-imaging-annotate_objects.flow When prompted, use bounding_box as Selection type . The following video takes you through the process:","title":"Annotate"},{"location":"object_detection/mmdetection/","text":"","title":"MMDetection"},{"location":"object_detection/yolov5/","text":"","title":"Yolov5"}]}