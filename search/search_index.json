{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Over the years, we have been working on a number of projects that involved applying deep learning algorithms to image problems. Setting up deep learning frameworks is always a slow and painstaking process, getting all the library dependencies right (CUDA, cuDNN, numpy, etc). To speed things up, we have developed (and maintain) a number of Python libraries and Docker images that can be used for various deep learning tasks. Here we are showing examples on how you can use these images on datasets to train your own models and how to apply them. The use of Docker made it a lot easier and faster to apply algorithms to new datasets. If you have not used Docker before, we recommend you to have a look at our introduction called Docker for Data Scientists . The following domains are covered with examples: Image classification Object detection Instance segmentation Image segmentation","title":"Home"},{"location":"prerequisites/","text":"The following prerequisites apply to most data domains: Annotations In order to annotate data (e.g., for object detection, instance segmentation or image segmentation), you should download the ADAMS Annotator application (you only need Java 11 installed ): Snapshot Release 2021.12.0 or later Format conversions For turning annotations from one format into another, you need to install the wai.annotations Python library: wai.annotations project wai.annotations manual NB: it is recommended to install it in a virtual environment to avoid any conflicts with the host system. Hardware For building models, a computer with NVIDIA GPU (8GB+) and Linux operating system is recommended. Only image classification models can be built in a reasonable amount of time on CPU-only machines. Directory structure Create the following directory structure for the examples of this tutorial: | +-- applied_deep_learning | +-- cache | | | +-- torch | | | +-- iopath_cache | +-- data | +-- models | +-- output | +-- predictions | +-- in | +-- out Use this command-line to create the directories: mkdir -p \\ applied_deep_learning/cache/torch \\ applied_deep_learning/cache/iopath_cache \\ applied_deep_learning/data \\ applied_deep_learning/models \\ applied_deep_learning/output \\ applied_deep_learning/predictions/in \\ applied_deep_learning/predictions/out Docker notes To make the Docker commands as easy as possible, they will all get issued from the within the applied_deep_learning directory. In order to get access to the data , output and predictions directories, we use the following mapping: -v `pwd`:/workspace This will map the applied_deep_learning directory onto the /workspace directory within the container. Since Docker usually runs as root within the container, we want to make sure that the user group of any files that get generated are being owned by the current user. This can be achieved by adding the following to the Docker command: -u $(id -u):$(id -g) -e USER=$USER","title":"Prerequisites"},{"location":"prerequisites/#annotations","text":"In order to annotate data (e.g., for object detection, instance segmentation or image segmentation), you should download the ADAMS Annotator application (you only need Java 11 installed ): Snapshot Release 2021.12.0 or later","title":"Annotations"},{"location":"prerequisites/#format-conversions","text":"For turning annotations from one format into another, you need to install the wai.annotations Python library: wai.annotations project wai.annotations manual NB: it is recommended to install it in a virtual environment to avoid any conflicts with the host system.","title":"Format conversions"},{"location":"prerequisites/#hardware","text":"For building models, a computer with NVIDIA GPU (8GB+) and Linux operating system is recommended. Only image classification models can be built in a reasonable amount of time on CPU-only machines.","title":"Hardware"},{"location":"prerequisites/#directory-structure","text":"Create the following directory structure for the examples of this tutorial: | +-- applied_deep_learning | +-- cache | | | +-- torch | | | +-- iopath_cache | +-- data | +-- models | +-- output | +-- predictions | +-- in | +-- out Use this command-line to create the directories: mkdir -p \\ applied_deep_learning/cache/torch \\ applied_deep_learning/cache/iopath_cache \\ applied_deep_learning/data \\ applied_deep_learning/models \\ applied_deep_learning/output \\ applied_deep_learning/predictions/in \\ applied_deep_learning/predictions/out","title":"Directory structure"},{"location":"prerequisites/#docker-notes","text":"To make the Docker commands as easy as possible, they will all get issued from the within the applied_deep_learning directory. In order to get access to the data , output and predictions directories, we use the following mapping: -v `pwd`:/workspace This will map the applied_deep_learning directory onto the /workspace directory within the container. Since Docker usually runs as root within the container, we want to make sure that the user group of any files that get generated are being owned by the current user. This can be achieved by adding the following to the Docker command: -u $(id -u):$(id -g) -e USER=$USER","title":"Docker notes"},{"location":"image_classification/","text":"Image classification is the simplest and least computational expensive task, as it classifies whole images, assigning them a category. Before you delve into training models, take a look at how you need to annotate your data . The following frameworks are available: MMClassification wai.tfimageclass","title":"Introduction"},{"location":"image_classification/annotate/","text":"Annotating data for image classification is straight forward. Since we are classifying whole images, we can simply use the directory layout for annotating images. In the example below, we have a dataset called flowers . The sub-directories below the flowers directory represent the labels for the images containing within the sub-directories. | +- flowers | +- daisy | +- dandelion | +- roses | +- sunflowers | +- tulip NB: To avoid any issues, the labels should be lowercase and underscores should be used instead of blanks/spaces.","title":"Annotate"},{"location":"image_classification/mmclassification/","text":"MMClassification is a comprehensive and flexible framework for image segmentation that offers a wide variety of models. Custom docker images with additional tools are available from here: https://github.com/waikato-datamining/mmclassification Prerequisites Make sure you have the directory structure created as outlined in the Prerequisites . Data In this example, we will use the 102 flowers dataset, which consists of 102 different categories (~ species) of flowers. More precisely, we will download the dataset with the flowers already split into categories from which we will use a subset to speed up the training process. Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/102flowers/102flowers-subdir.zip Once extracted, you can delete all sub-directories apart from: alpine_sea_holly anthurium artichoke azalea ball_moss Rename the subdir directory to 5flowers and move it into the data folder of our directory structure outlined. Split the data into train , validation and test subsets using wai.annotations : docker run -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:latest \\ wai-annotations convert \\ from-subdir-ic \\ -i \"/workspace/data/5flowers/**/*.jpg\" \\ to-subdir-ic \\ -o /workspace/data/5flowers-split \\ --split-names train val test \\ --split-ratios 70 15 15 Training For training, we will use the following docker image: waikatodatamining/mmclassification:0.23.1_cuda11.1 If you only have a CPU machine available, then use this one instead: waikatodatamining/mmclassification:0.23.1_cpu The training script is called mmcls_train , for which we can invoke the help screen as follows: docker run -t waikatodatamining/mmclassification:0.23.1_cuda11.1 mmcls_train --help # GPU docker run -t waikatodatamining/mmclassification:0.23.1_cpu mmcls_train --help # CPU It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: 5flowers-mmcl-r18 Before we can train, we will need to obtain and customize a config file. Within the container, you can find example configurations for various architectures in the following directory: /mmclassification/configs Using the mmseg_config command, we can expand and dump one of these configurations for our own purposes: CPU: docker run \\ -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -t waikatodatamining/mmclassification:0.23.1_cpu \\ mmcls_config \\ /mmclassification/configs/resnet/resnet18_b32x8_imagenet.py \\ > `pwd`/output/5flowers-mmcl-r18/resnet18_b32x8_imagenet.py GPU: docker run \\ -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -t waikatodatamining/mmclassification:0.23.1_cuda11.1 \\ mmcls_config \\ /mmclassification/configs/resnet/resnet18_b32x8_imagenet.py \\ > `pwd`/output/5flowers-mmcl-r18/resnet18_b32x8_imagenet.py Open the resnet18_b32x8_imagenet.py file in a text editor and perform the following operations: remove the lines before model = dict( change num_classes to 5 change dataset_type to ExternalDataset and any occurrences of type in the train , test , val sections of the data dictionary change dataset_prefix to /workspace/data/5flowers-split/train , ../test and ../val in the relevant sections change ann_file occurrences to None change max_epochs in the runner to a suitable value change the interval of the checkpoint_config to a value that makes sense with max_epochs GPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMCLS_CLASSES=alpine_sea_holly,anthurium,artichoke,azalea,ball_moss \\ -t waikatodatamining/mmclassification:0.23.1_cuda11.1 \\ mmcls_train \\ /workspace/output/5flowers-mmcl-r18/resnet18_b32x8_imagenet.py \\ --work-dir /workspace/output/5flowers-mmcl-r18 CPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMCLS_CLASSES=alpine_sea_holly,anthurium,artichoke,azalea,ball_moss \\ -t waikatodatamining/mmclassification:0.23.1_cpu \\ mmcls_train \\ /workspace/output/5flowers-mmcl-r18/resnet18_b32x8_imagenet.py \\ --work-dir /workspace/output/5flowers-mmcl-r18 Predicting Using the mmcls_predict_poll script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): GPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ --gpus=all \\ -v `pwd`:/workspace \\ -e MMCLS_CLASSES=alpine_sea_holly,anthurium,artichoke,azalea,ball_moss \\ -t waikatodatamining/mmclassification:0.23.1_cuda11.1 \\ mmcls_predict_poll \\ --model /workspace/output/5flowers-mmcl-r18/latest.pth \\ --config /workspace/output/5flowers-mmcl-r18/resnet18_b32x8_imagenet.py \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out CPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -e MMCLS_CLASSES=alpine_sea_holly,anthurium,artichoke,azalea,ball_moss \\ -t waikatodatamining/mmclassification:0.23.1_cpu \\ mmcls_predict_poll \\ --model /workspace/output/5flowers-mmcl-r18/latest.pth \\ --config /workspace/output/5flowers-mmcl-r18/resnet18_b32x8_imagenet.py \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out E.g., for the image_08085.jpg from the alpine_sea_holly class, we will get a JSON file similar to this one : { \"alpine_sea_holly\": 0.9981447458267212, \"anthurium\": 1.6010968465707265e-05, \"artichoke\": 0.0015235934406518936, \"azalea\": 2.2820820504421135e-06, \"ball_moss\": 0.00031330727506428957 } Troubleshooting RuntimeError: selected index k out of range This can occur if you have less than 5 class labels. You need to update the config file as follows ( source ): set model/head/topk to (1, ) rather than (1, 5) add metric_options={'topk': (1, ) to the evaluation dictionary","title":"\u00bb MMClassification"},{"location":"image_classification/mmclassification/#prerequisites","text":"Make sure you have the directory structure created as outlined in the Prerequisites .","title":"Prerequisites"},{"location":"image_classification/mmclassification/#data","text":"In this example, we will use the 102 flowers dataset, which consists of 102 different categories (~ species) of flowers. More precisely, we will download the dataset with the flowers already split into categories from which we will use a subset to speed up the training process. Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/102flowers/102flowers-subdir.zip Once extracted, you can delete all sub-directories apart from: alpine_sea_holly anthurium artichoke azalea ball_moss Rename the subdir directory to 5flowers and move it into the data folder of our directory structure outlined. Split the data into train , validation and test subsets using wai.annotations : docker run -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:latest \\ wai-annotations convert \\ from-subdir-ic \\ -i \"/workspace/data/5flowers/**/*.jpg\" \\ to-subdir-ic \\ -o /workspace/data/5flowers-split \\ --split-names train val test \\ --split-ratios 70 15 15","title":"Data"},{"location":"image_classification/mmclassification/#training","text":"For training, we will use the following docker image: waikatodatamining/mmclassification:0.23.1_cuda11.1 If you only have a CPU machine available, then use this one instead: waikatodatamining/mmclassification:0.23.1_cpu The training script is called mmcls_train , for which we can invoke the help screen as follows: docker run -t waikatodatamining/mmclassification:0.23.1_cuda11.1 mmcls_train --help # GPU docker run -t waikatodatamining/mmclassification:0.23.1_cpu mmcls_train --help # CPU It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: 5flowers-mmcl-r18 Before we can train, we will need to obtain and customize a config file. Within the container, you can find example configurations for various architectures in the following directory: /mmclassification/configs Using the mmseg_config command, we can expand and dump one of these configurations for our own purposes: CPU: docker run \\ -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -t waikatodatamining/mmclassification:0.23.1_cpu \\ mmcls_config \\ /mmclassification/configs/resnet/resnet18_b32x8_imagenet.py \\ > `pwd`/output/5flowers-mmcl-r18/resnet18_b32x8_imagenet.py GPU: docker run \\ -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -t waikatodatamining/mmclassification:0.23.1_cuda11.1 \\ mmcls_config \\ /mmclassification/configs/resnet/resnet18_b32x8_imagenet.py \\ > `pwd`/output/5flowers-mmcl-r18/resnet18_b32x8_imagenet.py Open the resnet18_b32x8_imagenet.py file in a text editor and perform the following operations: remove the lines before model = dict( change num_classes to 5 change dataset_type to ExternalDataset and any occurrences of type in the train , test , val sections of the data dictionary change dataset_prefix to /workspace/data/5flowers-split/train , ../test and ../val in the relevant sections change ann_file occurrences to None change max_epochs in the runner to a suitable value change the interval of the checkpoint_config to a value that makes sense with max_epochs GPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMCLS_CLASSES=alpine_sea_holly,anthurium,artichoke,azalea,ball_moss \\ -t waikatodatamining/mmclassification:0.23.1_cuda11.1 \\ mmcls_train \\ /workspace/output/5flowers-mmcl-r18/resnet18_b32x8_imagenet.py \\ --work-dir /workspace/output/5flowers-mmcl-r18 CPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMCLS_CLASSES=alpine_sea_holly,anthurium,artichoke,azalea,ball_moss \\ -t waikatodatamining/mmclassification:0.23.1_cpu \\ mmcls_train \\ /workspace/output/5flowers-mmcl-r18/resnet18_b32x8_imagenet.py \\ --work-dir /workspace/output/5flowers-mmcl-r18","title":"Training"},{"location":"image_classification/mmclassification/#predicting","text":"Using the mmcls_predict_poll script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): GPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ --gpus=all \\ -v `pwd`:/workspace \\ -e MMCLS_CLASSES=alpine_sea_holly,anthurium,artichoke,azalea,ball_moss \\ -t waikatodatamining/mmclassification:0.23.1_cuda11.1 \\ mmcls_predict_poll \\ --model /workspace/output/5flowers-mmcl-r18/latest.pth \\ --config /workspace/output/5flowers-mmcl-r18/resnet18_b32x8_imagenet.py \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out CPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -e MMCLS_CLASSES=alpine_sea_holly,anthurium,artichoke,azalea,ball_moss \\ -t waikatodatamining/mmclassification:0.23.1_cpu \\ mmcls_predict_poll \\ --model /workspace/output/5flowers-mmcl-r18/latest.pth \\ --config /workspace/output/5flowers-mmcl-r18/resnet18_b32x8_imagenet.py \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out E.g., for the image_08085.jpg from the alpine_sea_holly class, we will get a JSON file similar to this one : { \"alpine_sea_holly\": 0.9981447458267212, \"anthurium\": 1.6010968465707265e-05, \"artichoke\": 0.0015235934406518936, \"azalea\": 2.2820820504421135e-06, \"ball_moss\": 0.00031330727506428957 }","title":"Predicting"},{"location":"image_classification/mmclassification/#troubleshooting","text":"RuntimeError: selected index k out of range This can occur if you have less than 5 class labels. You need to update the config file as follows ( source ): set model/head/topk to (1, ) rather than (1, 5) add metric_options={'topk': (1, ) to the evaluation dictionary","title":"Troubleshooting"},{"location":"image_classification/wai.tfimageclass/","text":"The wai.tfimageclass Python library ( PyPI ) can be used for training various models that are available from the Tensorflow Hub . Prerequisites Make sure you have the directory structure created as outlined in the Prerequisites . Data In this example, we will use the 102 flowers dataset, which consists of 102 different categories (~ species) of flowers. More precisely, we will download the dataset with the flowers already split into categories from which we will use a subset to speed up the training process. Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/102flowers/102flowers-subdir.zip Once extracted, you can delete all sub-directories apart from: alpine_sea_holly anthurium artichoke Rename the subdir directory to 3flowers and move it into the data folder of our directory structure outlined. Training For training, we will use the following docker image: waikatodatamining/tf_image_classification:1.14 If you only have a CPU machine available, then use this one instead: waikatodatamining/tf_image_classification:1.14_cpu The training script is called tfic-retrain , for which we can invoke the help screen as follows: docker run -t waikatodatamining/tf_image_classification:1.14 tfic-retrain --help # GPU docker run -t waikatodatamining/tf_image_classification:1.14_cpu tfic-retrain --help # CPU It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: 3flowers-tf-default The following command will train an Inception v3 model for 500 steps: GPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules \\ -t waikatodatamining/tf_image_classification:1.14 \\ tfic-retrain \\ --image_dir /workspace/data/3flowers \\ --output_graph /workspace/output/3flowers-tf-default/graph.pb \\ --output_info /workspace/output/3flowers-tf-default/graph.json \\ --saved_model_dir /workspace/output/3flowers-tf-default/saved_model \\ --image_lists_dir /workspace/output/3flowers-tf-default \\ --tfhub_module https://tfhub.dev/google/imagenet/inception_v3/feature_vector/3 \\ --training_steps 500 CPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules \\ -t waikatodatamining/tf_image_classification:1.14_cpu \\ tfic-retrain \\ --image_dir /workspace/data/3flowers \\ --output_graph /workspace/output/3flowers-tf-default/graph.pb \\ --output_info /workspace/output/3flowers-tf-default/graph.json \\ --saved_model_dir /workspace/output/3flowers-tf-default/saved_model \\ --image_lists_dir /workspace/output/3flowers-tf-default \\ --tfhub_module https://tfhub.dev/google/imagenet/inception_v3/feature_vector/3 \\ --training_steps 500 Exporting model Before we can use the model, we need to export it to Tensorflow lite or tflite : GPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules \\ -t waikatodatamining/tf_image_classification:1.14 \\ tfic-export \\ --saved_model_dir /workspace/output/3flowers-tf-default/saved_model \\ --tflite_model /workspace/output/3flowers-tf-default/graph.tflite CPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules \\ -t waikatodatamining/tf_image_classification:1.14_cpu \\ tfic-export \\ --saved_model_dir /workspace/output/3flowers-tf-default/saved_model \\ --tflite_model /workspace/output/3flowers-tf-default/graph.tflite Predicting For making predictions for a single image, you can use the script tfic-labelimage . Since we will want to batch predict multiple images, will use the script tfic-poll instead: GPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_image_classification:1.14 \\ tfic-poll \\ --graph /workspace/output/3flowers-tf-default/graph.tflite \\ --graph_type tflite \\ --info /workspace/output/3flowers-tf-default/graph.json \\ --in_dir /workspace/predictions/in \\ --out_dir /workspace/predictions/out CPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_image_classification:1.14_cpu \\ tfic-poll \\ --graph /workspace/output/3flowers-tf-default/graph.tflite \\ --graph_type tflite \\ --info /workspace/output/3flowers-tf-default/graph.json \\ --in_dir /workspace/predictions/in \\ --out_dir /workspace/predictions/out E.g., for the image_01965.jpg from the anthurium class, we will get a CSV file similar to this one : label probability anthurium 0.989429 artichoke 0.0060946 alpine_sea_holly 0.00447612","title":"\u00bb wai.tfimageclass"},{"location":"image_classification/wai.tfimageclass/#prerequisites","text":"Make sure you have the directory structure created as outlined in the Prerequisites .","title":"Prerequisites"},{"location":"image_classification/wai.tfimageclass/#data","text":"In this example, we will use the 102 flowers dataset, which consists of 102 different categories (~ species) of flowers. More precisely, we will download the dataset with the flowers already split into categories from which we will use a subset to speed up the training process. Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/102flowers/102flowers-subdir.zip Once extracted, you can delete all sub-directories apart from: alpine_sea_holly anthurium artichoke Rename the subdir directory to 3flowers and move it into the data folder of our directory structure outlined.","title":"Data"},{"location":"image_classification/wai.tfimageclass/#training","text":"For training, we will use the following docker image: waikatodatamining/tf_image_classification:1.14 If you only have a CPU machine available, then use this one instead: waikatodatamining/tf_image_classification:1.14_cpu The training script is called tfic-retrain , for which we can invoke the help screen as follows: docker run -t waikatodatamining/tf_image_classification:1.14 tfic-retrain --help # GPU docker run -t waikatodatamining/tf_image_classification:1.14_cpu tfic-retrain --help # CPU It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: 3flowers-tf-default The following command will train an Inception v3 model for 500 steps: GPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules \\ -t waikatodatamining/tf_image_classification:1.14 \\ tfic-retrain \\ --image_dir /workspace/data/3flowers \\ --output_graph /workspace/output/3flowers-tf-default/graph.pb \\ --output_info /workspace/output/3flowers-tf-default/graph.json \\ --saved_model_dir /workspace/output/3flowers-tf-default/saved_model \\ --image_lists_dir /workspace/output/3flowers-tf-default \\ --tfhub_module https://tfhub.dev/google/imagenet/inception_v3/feature_vector/3 \\ --training_steps 500 CPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules \\ -t waikatodatamining/tf_image_classification:1.14_cpu \\ tfic-retrain \\ --image_dir /workspace/data/3flowers \\ --output_graph /workspace/output/3flowers-tf-default/graph.pb \\ --output_info /workspace/output/3flowers-tf-default/graph.json \\ --saved_model_dir /workspace/output/3flowers-tf-default/saved_model \\ --image_lists_dir /workspace/output/3flowers-tf-default \\ --tfhub_module https://tfhub.dev/google/imagenet/inception_v3/feature_vector/3 \\ --training_steps 500","title":"Training"},{"location":"image_classification/wai.tfimageclass/#exporting-model","text":"Before we can use the model, we need to export it to Tensorflow lite or tflite : GPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules \\ -t waikatodatamining/tf_image_classification:1.14 \\ tfic-export \\ --saved_model_dir /workspace/output/3flowers-tf-default/saved_model \\ --tflite_model /workspace/output/3flowers-tf-default/graph.tflite CPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules \\ -t waikatodatamining/tf_image_classification:1.14_cpu \\ tfic-export \\ --saved_model_dir /workspace/output/3flowers-tf-default/saved_model \\ --tflite_model /workspace/output/3flowers-tf-default/graph.tflite","title":"Exporting model"},{"location":"image_classification/wai.tfimageclass/#predicting","text":"For making predictions for a single image, you can use the script tfic-labelimage . Since we will want to batch predict multiple images, will use the script tfic-poll instead: GPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_image_classification:1.14 \\ tfic-poll \\ --graph /workspace/output/3flowers-tf-default/graph.tflite \\ --graph_type tflite \\ --info /workspace/output/3flowers-tf-default/graph.json \\ --in_dir /workspace/predictions/in \\ --out_dir /workspace/predictions/out CPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_image_classification:1.14_cpu \\ tfic-poll \\ --graph /workspace/output/3flowers-tf-default/graph.tflite \\ --graph_type tflite \\ --info /workspace/output/3flowers-tf-default/graph.json \\ --in_dir /workspace/predictions/in \\ --out_dir /workspace/predictions/out E.g., for the image_01965.jpg from the anthurium class, we will get a CSV file similar to this one : label probability anthurium 0.989429 artichoke 0.0060946 alpine_sea_holly 0.00447612","title":"Predicting"},{"location":"image_segmentation/","text":"Image segmentation classifies individual pixels within an image, assigning them a label (i.e., a color). Before you delve into training models, take a look at how you need to annotate your data . The following frameworks are available: Image Segmentation Keras MMSegmentation","title":"Introduction"},{"location":"image_segmentation/annotate/","text":"You can use the ADAMS framework for annotating your images. ADAMS comes pre-bundled in various setups and we need the adams-annotator bundle. You can get either: snapshot release (post 21.12.0 release) In the Flow editor (available from the Tools menu in ADAMS), you can load and execute the follow workflow (which is part of your ADAMS installation when downloading it as zip file): adams-imaging-image_segmentation_annotation.flow The following video takes you through the process:","title":"Annotate"},{"location":"image_segmentation/image-segmentation-keras/","text":"Image Segmenation Keras is a Keras/Tensorflow based image segmentation framework. Custom docker images with additional tools are available from here: https://github.com/waikato-datamining/tensorflow/tree/master/image-segmentation-keras Prerequisites Make sure you have the directory structure created as outlined in the Prerequisites . Data In this example, we will use the CamVid-12 dataset, which consists of still images from dashcam videos from a city environment (12 different labels). Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/camvid12/camvid12-grayscale.zip Once extracted, rename the grayscale directory to camvid-grayscale . Now we have to convert the format from grayscale into blue channel , which the framework uses. We can do this by using the wai.annotations library. At the same time, we can split the dataset into train , validation and test subsets. From within the applied_deep_learning directory, run the following command: docker run -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:latest \\ wai-annotations convert \\ from-grayscale-is \\ -i \"/workspace/data/camvid-grayscale/*.png\" \\ --labels sky building pole road pavement tree signsymbol fence car pedestrian bicyclist unlabelled \\ to-blue-channel-is \\ -o /workspace/data/camvid-bluechannel-split \\ --split-names train val test \\ --split-ratios 70 15 15 Training For training, we will use the following docker image: waikatodatamining/image-segmentation-keras:1.14.0_0.3.0 The training script is called keras_seg_train , for which we can invoke the help screen as follows: docker run -t waikatodatamining/image-segmentation-keras:1.14.0_0.3.0 keras_seg_train --help It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: camvid12-keras-unet50 Image Segmentation Keras does not use a config file, but can be configured via command-line parameters instead: the number of classes ( --n_classes ) is actual classes plus background the width/height of the inputs must be multiples of 32 ( --input_height , --input_width ) the output directory is specified via --checkpoints_path (make sure to have a trailing slash!) in our case, the images and the annotations reside in the same directories, so --train_images and --train_annotations point to the same directory (analog for --val_images and --val_annotations ) with --epochs you can specify for how long the model will get trained --model_name specifies the architecture and backend (see here for available options) Kick off the training of a U-Net with a ResNet50 backend using the following command: docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/.keras \\ -t waikatodatamining/image-segmentation-keras:1.14.0_0.3.0 \\ keras_seg_train \\ --checkpoints_path /workspace/output/camvid12-keras-unet50/ \\ --train_images /workspace/data/camvid-bluechannel-split/train \\ --train_annotations /workspace/data/camvid-bluechannel-split/train \\ --val_images /workspace/data/camvid-bluechannel-split/val \\ --val_annotations /workspace/data/camvid-bluechannel-split/val \\ --epochs 10 \\ --n_classes 13 \\ --input_height 384 \\ --input_width 480 \\ --model_name resnet50_unet Predicting Using the keras_seg_poll script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/.keras \\ -t waikatodatamining/image-segmentation-keras:1.14.0_0.3.0 \\ keras_seg_poll \\ --checkpoints_path /workspace/output/camvid12-keras-unet50/ \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Example prediction","title":"\u00bb Image Segmentation Keras"},{"location":"image_segmentation/image-segmentation-keras/#prerequisites","text":"Make sure you have the directory structure created as outlined in the Prerequisites .","title":"Prerequisites"},{"location":"image_segmentation/image-segmentation-keras/#data","text":"In this example, we will use the CamVid-12 dataset, which consists of still images from dashcam videos from a city environment (12 different labels). Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/camvid12/camvid12-grayscale.zip Once extracted, rename the grayscale directory to camvid-grayscale . Now we have to convert the format from grayscale into blue channel , which the framework uses. We can do this by using the wai.annotations library. At the same time, we can split the dataset into train , validation and test subsets. From within the applied_deep_learning directory, run the following command: docker run -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:latest \\ wai-annotations convert \\ from-grayscale-is \\ -i \"/workspace/data/camvid-grayscale/*.png\" \\ --labels sky building pole road pavement tree signsymbol fence car pedestrian bicyclist unlabelled \\ to-blue-channel-is \\ -o /workspace/data/camvid-bluechannel-split \\ --split-names train val test \\ --split-ratios 70 15 15","title":"Data"},{"location":"image_segmentation/image-segmentation-keras/#training","text":"For training, we will use the following docker image: waikatodatamining/image-segmentation-keras:1.14.0_0.3.0 The training script is called keras_seg_train , for which we can invoke the help screen as follows: docker run -t waikatodatamining/image-segmentation-keras:1.14.0_0.3.0 keras_seg_train --help It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: camvid12-keras-unet50 Image Segmentation Keras does not use a config file, but can be configured via command-line parameters instead: the number of classes ( --n_classes ) is actual classes plus background the width/height of the inputs must be multiples of 32 ( --input_height , --input_width ) the output directory is specified via --checkpoints_path (make sure to have a trailing slash!) in our case, the images and the annotations reside in the same directories, so --train_images and --train_annotations point to the same directory (analog for --val_images and --val_annotations ) with --epochs you can specify for how long the model will get trained --model_name specifies the architecture and backend (see here for available options) Kick off the training of a U-Net with a ResNet50 backend using the following command: docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/.keras \\ -t waikatodatamining/image-segmentation-keras:1.14.0_0.3.0 \\ keras_seg_train \\ --checkpoints_path /workspace/output/camvid12-keras-unet50/ \\ --train_images /workspace/data/camvid-bluechannel-split/train \\ --train_annotations /workspace/data/camvid-bluechannel-split/train \\ --val_images /workspace/data/camvid-bluechannel-split/val \\ --val_annotations /workspace/data/camvid-bluechannel-split/val \\ --epochs 10 \\ --n_classes 13 \\ --input_height 384 \\ --input_width 480 \\ --model_name resnet50_unet","title":"Training"},{"location":"image_segmentation/image-segmentation-keras/#predicting","text":"Using the keras_seg_poll script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/.keras \\ -t waikatodatamining/image-segmentation-keras:1.14.0_0.3.0 \\ keras_seg_poll \\ --checkpoints_path /workspace/output/camvid12-keras-unet50/ \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Example prediction","title":"Predicting"},{"location":"image_segmentation/mmsegmentation/","text":"MMSegmentation is a comprehensive and flexible framework for image segmentation that offers a wide variety of architectures. Custom docker images with additional tools are available from here: https://github.com/waikato-datamining/mmsegmentation Prerequisites Make sure you have the directory structure created as outlined in the Prerequisites . Data In this example, we will use the CamVid-12 dataset, which consists of still images from dashcam videos from a city environment (12 different labels). Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/camvid12/camvid12-grayscale.zip Once extracted, rename the grayscale directory to camvid-grayscale . Noe we have to convert the format from grayscale into indexed PNG , which MMSegmentation uses. We can do this by using the wai.annotations library. At the same time, we can split the dataset into train , validation and test subsets. From within the applied_deep_learning directory, run the following command: docker run -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:latest \\ wai-annotations convert \\ from-grayscale-is \\ -i \"/workspace/data/camvid-grayscale/*.png\" \\ --labels sky building pole road pavement tree signsymbol fence car pedestrian bicyclist unlabelled \\ to-indexed-png-is \\ -o /workspace/data/camvid-indexed-split \\ --split-names train val test \\ --split-ratios 70 15 15 Training For training, we will use the following docker image: waikatodatamining/mmsegmentation:0.25.0_cuda11.1 The training script is called mmseg_train , for which we can invoke the help screen as follows: docker run -t waikatodatamining/mmsegmentation:0.25.0_cuda11.1 mmseg_train --help It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: camvid12-mmseg-pspnet50 Before we can train, we will need to obtain and customize a config file. Within the container, you can find example configurations for various architectures in the following directory: /mmsegmentation/configs Using the mmseg_config command, we can expand and dump one of these configurations for our own purposes: docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -t waikatodatamining/mmsegmentation:0.25.0_cuda11.1 \\ mmseg_config \\ --config /mmsegmentation/configs/pspnet/pspnet_r50-d8_512x1024_40k_cityscapes.py \\ --output_config /workspace/output/camvid12-mmseg-pspnet50/pspnet_r50.py Open the pspnet_r50.py file in a text editor and perform the following operations: change num_classes to 12 (background not counted). change dataset_type to ExternalDataset and any occurrences of type in the train , test , val sections of the data dictionary. change data_root occurrences to /workspace/data/camvid-indexed-split (the directory above the train and val directories). change img_dir occurrences to img_dir=data_root+'/DIR', with DIR being the appropriate train , val or test do the same with ann_dir Kick off the training with the following command: docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMSEG_CLASSES=\"sky,building,pole,road,pavement,tree,signsymbol,fence,car,pedestrian,bicyclist,unlabelled\" \\ -t waikatodatamining/mmsegmentation:0.25.0_cuda11.1 \\ mmseg_train \\ /workspace/output/camvid12-mmseg-pspnet50/pspnet_r50.py \\ --work-dir /workspace/output/camvid12-mmseg-pspnet50 Predicting Using the mmseg_predict_poll script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMSEG_CLASSES=\"sky,building,pole,road,pavement,tree,signsymbol,fence,car,pedestrian,bicyclist,unlabelled\" \\ -t waikatodatamining/mmsegmentation:0.25.0_cuda11.1 \\ mmseg_predict_poll \\ --model /workspace/output/camvid12-mmseg-pspnet50/latest.pth \\ --config /workspace/output/camvid12-mmseg-pspnet50/pspnet_r50.py \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Notes The predictions can either be output in grayscale (default) or bluechannel format ( --prediction_format ). You can view the predictions with the ADAMS Preview browser and, e.g., the SimpleImageSegmentationAnnotationsHandler (which will overlay the PNG transparently on the JPG; may require some configuring via the ... button). Example prediction","title":"\u00bb MMSegmentation"},{"location":"image_segmentation/mmsegmentation/#prerequisites","text":"Make sure you have the directory structure created as outlined in the Prerequisites .","title":"Prerequisites"},{"location":"image_segmentation/mmsegmentation/#data","text":"In this example, we will use the CamVid-12 dataset, which consists of still images from dashcam videos from a city environment (12 different labels). Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/camvid12/camvid12-grayscale.zip Once extracted, rename the grayscale directory to camvid-grayscale . Noe we have to convert the format from grayscale into indexed PNG , which MMSegmentation uses. We can do this by using the wai.annotations library. At the same time, we can split the dataset into train , validation and test subsets. From within the applied_deep_learning directory, run the following command: docker run -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:latest \\ wai-annotations convert \\ from-grayscale-is \\ -i \"/workspace/data/camvid-grayscale/*.png\" \\ --labels sky building pole road pavement tree signsymbol fence car pedestrian bicyclist unlabelled \\ to-indexed-png-is \\ -o /workspace/data/camvid-indexed-split \\ --split-names train val test \\ --split-ratios 70 15 15","title":"Data"},{"location":"image_segmentation/mmsegmentation/#training","text":"For training, we will use the following docker image: waikatodatamining/mmsegmentation:0.25.0_cuda11.1 The training script is called mmseg_train , for which we can invoke the help screen as follows: docker run -t waikatodatamining/mmsegmentation:0.25.0_cuda11.1 mmseg_train --help It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: camvid12-mmseg-pspnet50 Before we can train, we will need to obtain and customize a config file. Within the container, you can find example configurations for various architectures in the following directory: /mmsegmentation/configs Using the mmseg_config command, we can expand and dump one of these configurations for our own purposes: docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -t waikatodatamining/mmsegmentation:0.25.0_cuda11.1 \\ mmseg_config \\ --config /mmsegmentation/configs/pspnet/pspnet_r50-d8_512x1024_40k_cityscapes.py \\ --output_config /workspace/output/camvid12-mmseg-pspnet50/pspnet_r50.py Open the pspnet_r50.py file in a text editor and perform the following operations: change num_classes to 12 (background not counted). change dataset_type to ExternalDataset and any occurrences of type in the train , test , val sections of the data dictionary. change data_root occurrences to /workspace/data/camvid-indexed-split (the directory above the train and val directories). change img_dir occurrences to img_dir=data_root+'/DIR', with DIR being the appropriate train , val or test do the same with ann_dir Kick off the training with the following command: docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMSEG_CLASSES=\"sky,building,pole,road,pavement,tree,signsymbol,fence,car,pedestrian,bicyclist,unlabelled\" \\ -t waikatodatamining/mmsegmentation:0.25.0_cuda11.1 \\ mmseg_train \\ /workspace/output/camvid12-mmseg-pspnet50/pspnet_r50.py \\ --work-dir /workspace/output/camvid12-mmseg-pspnet50","title":"Training"},{"location":"image_segmentation/mmsegmentation/#predicting","text":"Using the mmseg_predict_poll script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMSEG_CLASSES=\"sky,building,pole,road,pavement,tree,signsymbol,fence,car,pedestrian,bicyclist,unlabelled\" \\ -t waikatodatamining/mmsegmentation:0.25.0_cuda11.1 \\ mmseg_predict_poll \\ --model /workspace/output/camvid12-mmseg-pspnet50/latest.pth \\ --config /workspace/output/camvid12-mmseg-pspnet50/pspnet_r50.py \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Notes The predictions can either be output in grayscale (default) or bluechannel format ( --prediction_format ). You can view the predictions with the ADAMS Preview browser and, e.g., the SimpleImageSegmentationAnnotationsHandler (which will overlay the PNG transparently on the JPG; may require some configuring via the ... button). Example prediction","title":"Predicting"},{"location":"instance_segmentation/","text":"Instance segmentation not only finds objects within images, it also learns the shape of the objects rather than just a simple rectangle/bounding box. However, this makes the algorithms computationally more expensive and more memory hungry. Before you delve into training models, take a look at how you need to annotate your data . The following frameworks are available: Detectron2","title":"Introduction"},{"location":"instance_segmentation/annotate/","text":"You can use the ADAMS framework for annotating your images. ADAMS comes pre-bundled in various setups and we need the adams-annotator bundle. You can get either: snapshot release (21.12.0 or later) In the Flow editor (available from the Tools menu in ADAMS), you can load and execute the follow workflow (which is part of your ADAMS installation when downloading it as zip file): adams-imaging-annotate_objects.flow When prompted, use object_shape as Selection type . The following video takes you through the process:","title":"Annotate"},{"location":"instance_segmentation/detectron2/","text":"Detectron2 is a platform for object detection, segmentation and other visual recognition tasks. Custom docker images for instance segmentation with additional tools are available from here: https://github.com/waikato-datamining/pytorch/tree/master/detectron2 Prerequisites Make sure you have the directory structure created as outlined in the Prerequisites . Data In this example, we will use the Oxford Pets dataset, which consists of 37 different categories of cats and dogs. Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/oxford-pets/oxford-pets-adams.zip Rename the adams directory to pets-adams . To speed up training, we only use two labels: cat:abyssinian and dog:yorkshire_terrier . The label filtering and splitting it into train , validation and test subsets is done using wai.annotations : docker run -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:latest \\ wai-annotations convert \\ from-adams-od \\ -i \"/workspace/data/pets-adams/*.report\" \\ filter-labels \\ --labels cat:abyssinian dog:yorkshire_terrier \\ discard-negatives \\ coerce-mask \\ to-coco-od \\ -o /workspace/data/pets2-coco-split/annotations.json \\ --sort-categories \\ --category-output-file labels.txt \\ --split-names train val test \\ --split-ratios 70 15 15 Training For training, we will use the following docker image: waikatodatamining/detectron2:0.6 The training script is called d2_train_coco , for which we can invoke the help screen as follows: docker run -t waikatodatamining/detectron2:0.6 d2_train_coco --help It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: pets2-d2-maskrcnn Before we can train, we will need to obtain and customize a config file. Within the container, you can find example configurations for various architectures in the following directory: /opt/detectron2/configs/COCO-InstanceSegmentation Using the d2_dump_config command, we can expand and dump one of these configurations for our own purposes: docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -t waikatodatamining/detectron2:0.6 \\ d2_dump_config \\ --config_in /opt/detectron2/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml \\ --num_classes 2 \\ --output_dir /workspace/output/pets2-d2-maskrcnn \\ --config_out /workspace/output/pets2-d2-maskrcnn/mask_rcnn_R_50_FPN_1x.yaml Edit the mask_rcnn_R_50_FPN_1x.yaml file and change the following values ( IMS_PER_BATCH is reduced to fit in your GPU's memory, the others to limit training time): SOLVER: IMS_PER_BATCH: 4 MAX_ITER: 15000 STEPS: - 5000 - 10000 Kick off the training with the following command: docker run \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -v `pwd`/cache/iopath_cache:/tmp/iopath_cache \\ -t waikatodatamining/detectron2:0.6 \\ d2_train_coco \\ --config /workspace/output/pets2-d2-maskrcnn/mask_rcnn_R_50_FPN_1x.yaml \\ --train_annotations /workspace/data/pets2-coco-split/train/annotations.json \\ --train_images /workspace/data/pets2-coco-split/train/ \\ --test_annotations /workspace/data/pets2-coco-split/val/annotations.json \\ --test_images /workspace/data/pets2-coco-split/val/ \\ --labels /workspace/data/pets2-coco-split/train/labels.txt \\ --output_dir /workspace/output/pets2-d2-maskrcnn Predicting Using the d2_predict script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -v `pwd`/cache/iopath_cache:/tmp/iopath_cache \\ -t waikatodatamining/detectron2:0.6 \\ d2_predict \\ --model /workspace/output/pets2-d2-maskrcnn/model_final.pth \\ --config /workspace/output/pets2-d2-maskrcnn/mask_rcnn_R_50_FPN_1x.yaml \\ --labels /workspace/data/pets2-coco-split/train/labels.txt \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Notes The predictions are in ROI CSV format Example prediction","title":"\u00bb Detectron2"},{"location":"instance_segmentation/detectron2/#prerequisites","text":"Make sure you have the directory structure created as outlined in the Prerequisites .","title":"Prerequisites"},{"location":"instance_segmentation/detectron2/#data","text":"In this example, we will use the Oxford Pets dataset, which consists of 37 different categories of cats and dogs. Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/oxford-pets/oxford-pets-adams.zip Rename the adams directory to pets-adams . To speed up training, we only use two labels: cat:abyssinian and dog:yorkshire_terrier . The label filtering and splitting it into train , validation and test subsets is done using wai.annotations : docker run -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:latest \\ wai-annotations convert \\ from-adams-od \\ -i \"/workspace/data/pets-adams/*.report\" \\ filter-labels \\ --labels cat:abyssinian dog:yorkshire_terrier \\ discard-negatives \\ coerce-mask \\ to-coco-od \\ -o /workspace/data/pets2-coco-split/annotations.json \\ --sort-categories \\ --category-output-file labels.txt \\ --split-names train val test \\ --split-ratios 70 15 15","title":"Data"},{"location":"instance_segmentation/detectron2/#training","text":"For training, we will use the following docker image: waikatodatamining/detectron2:0.6 The training script is called d2_train_coco , for which we can invoke the help screen as follows: docker run -t waikatodatamining/detectron2:0.6 d2_train_coco --help It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: pets2-d2-maskrcnn Before we can train, we will need to obtain and customize a config file. Within the container, you can find example configurations for various architectures in the following directory: /opt/detectron2/configs/COCO-InstanceSegmentation Using the d2_dump_config command, we can expand and dump one of these configurations for our own purposes: docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -t waikatodatamining/detectron2:0.6 \\ d2_dump_config \\ --config_in /opt/detectron2/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml \\ --num_classes 2 \\ --output_dir /workspace/output/pets2-d2-maskrcnn \\ --config_out /workspace/output/pets2-d2-maskrcnn/mask_rcnn_R_50_FPN_1x.yaml Edit the mask_rcnn_R_50_FPN_1x.yaml file and change the following values ( IMS_PER_BATCH is reduced to fit in your GPU's memory, the others to limit training time): SOLVER: IMS_PER_BATCH: 4 MAX_ITER: 15000 STEPS: - 5000 - 10000 Kick off the training with the following command: docker run \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -v `pwd`/cache/iopath_cache:/tmp/iopath_cache \\ -t waikatodatamining/detectron2:0.6 \\ d2_train_coco \\ --config /workspace/output/pets2-d2-maskrcnn/mask_rcnn_R_50_FPN_1x.yaml \\ --train_annotations /workspace/data/pets2-coco-split/train/annotations.json \\ --train_images /workspace/data/pets2-coco-split/train/ \\ --test_annotations /workspace/data/pets2-coco-split/val/annotations.json \\ --test_images /workspace/data/pets2-coco-split/val/ \\ --labels /workspace/data/pets2-coco-split/train/labels.txt \\ --output_dir /workspace/output/pets2-d2-maskrcnn","title":"Training"},{"location":"instance_segmentation/detectron2/#predicting","text":"Using the d2_predict script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -v `pwd`/cache/iopath_cache:/tmp/iopath_cache \\ -t waikatodatamining/detectron2:0.6 \\ d2_predict \\ --model /workspace/output/pets2-d2-maskrcnn/model_final.pth \\ --config /workspace/output/pets2-d2-maskrcnn/mask_rcnn_R_50_FPN_1x.yaml \\ --labels /workspace/data/pets2-coco-split/train/labels.txt \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Notes The predictions are in ROI CSV format Example prediction","title":"Predicting"},{"location":"object_detection/","text":"Object detection detects one or more objects within an image and assigns each one of them a label, as opposed to image classification which assigns a lable to the whole image. The predicted locations are typically rectangles (aka bounding boxes). If you are looking for shapes or polygons, then have a look at instance segmentation . Before you delve into training models, take a look at how you need to annotate your data . The following frameworks are available: MMDetection Yolov5","title":"Introduction"},{"location":"object_detection/annotate/","text":"You can use the ADAMS framework for annotating your images. ADAMS comes pre-bundled in various setups and we need the adams-annotator bundle. You can get either: snapshot release (21.12.0 or later) In the Flow editor (available from the Tools menu in ADAMS), you can load and execute the follow workflow (which is part of your ADAMS installation when downloading it as zip file): adams-imaging-annotate_objects.flow When prompted, use bounding_box as Selection type . The following video takes you through the process:","title":"Annotate"},{"location":"object_detection/mmdetection/","text":"MMDetection is a comprehensive and flexible framework for object detection that offers a wide variety of architectures. Custom docker images with additional tools are available from here: https://github.com/waikato-datamining/mmdetection Prerequisites Make sure you have the directory structure created as outlined in the Prerequisites . Data In this example, we will use the American Sign Language Letters dataset, which consists of sets of images of hands, one per letter in the English alphabet (26 labels). Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/american-sign-language-letters/american-sign-language-letters-voc.zip Once extracted, rename the voc directory to sign-voc . Now we have to convert the format from VOC XML into MS COCO . We can do this by using the wai.annotations library. At the same time, we can split the dataset into train , validation and test subsets. From within the applied_deep_learning directory, run the following command: docker run -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:latest \\ wai-annotations convert \\ from-voc-od \\ -i \"/workspace/data/sign-voc/*.xml\" \\ to-coco-od \\ -o /workspace/data/sign-coco-split/annotations.json \\ --sort-categories \\ --category-output-file labels.txt \\ --split-names train val test \\ --split-ratios 70 15 15 Training For training, we will use the following docker image: waikatodatamining/mmdetection:2.24.1_cuda11.1 The training script is called mmdet_train , for which we can invoke the help screen as follows (unfortunately, we need to set the MMDET_CLASSES environment variable to avoid an exception): docker run \\ -e MMDET_CLASSES= \\ -t waikatodatamining/mmdetection:2.24.1_cuda11.1 \\ mmdet_train --help It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: sign-mmdet-fr50 Before we can train, we will need to obtain and customize a config file. Within the container, you can find example configurations for various architectures in the following directory: /mmdetection/configs Using the mmdet_config command, we can expand and dump one of these configurations for our own purposes: docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -t waikatodatamining/mmdetection:2.24.1_cuda11.1 \\ mmdet_config \\ /mmdetection/configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \\ > output/sign-mmdet-fr50/faster_rcnn_r50_fpn_1x_coco.py Open the faster_rcnn_r50_fpn_1x_coco.py file in a text editor and perform the following operations: remove any lines before model = dict( change num_classes to 26 change dataset_type to ExternalDataset and any occurrences of type in the train , test , val sections of the data dictionary change data_root occurrences to /workspace/data/sign-coco-split (the directory above the train and val directories) change img_prefix occurrences to img_prefix=data_root+'/DIR', with DIR being the appropriate train , val or test change ann_file occurrences to ann_file=data_root+'/DIR/annotations.json', with DIR being the appropriate train , val or test change max_epochs in runner to an appropriate value, e.g., 5 change interval in checkpoint_config to a higher value, e.g., 5 Kick off the training with the following command: docker run \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMDET_CLASSES=/workspace/data/sign-coco-split/train/labels.txt \\ -t waikatodatamining/mmdetection:2.24.1_cuda11.1 \\ mmdet_train \\ /workspace/output/sign-mmdet-fr50/faster_rcnn_r50_fpn_1x_coco.py \\ --work-dir /workspace/output/sign-mmdet-fr50 Predicting Using the mmdet_predict script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMDET_CLASSES=/workspace/data/sign-coco-split/train/labels.txt \\ -t waikatodatamining/mmdetection:2.24.1_cuda11.1 \\ mmdet_predict \\ --checkpoint /workspace/output/sign-mmdet-fr50/latest.pth \\ --config /workspace/output/sign-mmdet-fr50/faster_rcnn_r50_fpn_1x_coco.py \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Notes The predictions get output in ROI CSV format . You can view the predictions with the ADAMS Preview browser and, e.g., the ObjectLocationsFromSpreadSheet handler. You need to configure this generic handler via the ... button, entering the columns for the bounding box ( x0 , y0 , x1 , y1 ) and the label ( label_str ) of the reader ( ObjectLocationsSpreadSheetReader ). Example prediction","title":"\u00bb MMDetection"},{"location":"object_detection/mmdetection/#prerequisites","text":"Make sure you have the directory structure created as outlined in the Prerequisites .","title":"Prerequisites"},{"location":"object_detection/mmdetection/#data","text":"In this example, we will use the American Sign Language Letters dataset, which consists of sets of images of hands, one per letter in the English alphabet (26 labels). Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/american-sign-language-letters/american-sign-language-letters-voc.zip Once extracted, rename the voc directory to sign-voc . Now we have to convert the format from VOC XML into MS COCO . We can do this by using the wai.annotations library. At the same time, we can split the dataset into train , validation and test subsets. From within the applied_deep_learning directory, run the following command: docker run -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:latest \\ wai-annotations convert \\ from-voc-od \\ -i \"/workspace/data/sign-voc/*.xml\" \\ to-coco-od \\ -o /workspace/data/sign-coco-split/annotations.json \\ --sort-categories \\ --category-output-file labels.txt \\ --split-names train val test \\ --split-ratios 70 15 15","title":"Data"},{"location":"object_detection/mmdetection/#training","text":"For training, we will use the following docker image: waikatodatamining/mmdetection:2.24.1_cuda11.1 The training script is called mmdet_train , for which we can invoke the help screen as follows (unfortunately, we need to set the MMDET_CLASSES environment variable to avoid an exception): docker run \\ -e MMDET_CLASSES= \\ -t waikatodatamining/mmdetection:2.24.1_cuda11.1 \\ mmdet_train --help It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: sign-mmdet-fr50 Before we can train, we will need to obtain and customize a config file. Within the container, you can find example configurations for various architectures in the following directory: /mmdetection/configs Using the mmdet_config command, we can expand and dump one of these configurations for our own purposes: docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -t waikatodatamining/mmdetection:2.24.1_cuda11.1 \\ mmdet_config \\ /mmdetection/configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \\ > output/sign-mmdet-fr50/faster_rcnn_r50_fpn_1x_coco.py Open the faster_rcnn_r50_fpn_1x_coco.py file in a text editor and perform the following operations: remove any lines before model = dict( change num_classes to 26 change dataset_type to ExternalDataset and any occurrences of type in the train , test , val sections of the data dictionary change data_root occurrences to /workspace/data/sign-coco-split (the directory above the train and val directories) change img_prefix occurrences to img_prefix=data_root+'/DIR', with DIR being the appropriate train , val or test change ann_file occurrences to ann_file=data_root+'/DIR/annotations.json', with DIR being the appropriate train , val or test change max_epochs in runner to an appropriate value, e.g., 5 change interval in checkpoint_config to a higher value, e.g., 5 Kick off the training with the following command: docker run \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMDET_CLASSES=/workspace/data/sign-coco-split/train/labels.txt \\ -t waikatodatamining/mmdetection:2.24.1_cuda11.1 \\ mmdet_train \\ /workspace/output/sign-mmdet-fr50/faster_rcnn_r50_fpn_1x_coco.py \\ --work-dir /workspace/output/sign-mmdet-fr50","title":"Training"},{"location":"object_detection/mmdetection/#predicting","text":"Using the mmdet_predict script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMDET_CLASSES=/workspace/data/sign-coco-split/train/labels.txt \\ -t waikatodatamining/mmdetection:2.24.1_cuda11.1 \\ mmdet_predict \\ --checkpoint /workspace/output/sign-mmdet-fr50/latest.pth \\ --config /workspace/output/sign-mmdet-fr50/faster_rcnn_r50_fpn_1x_coco.py \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Notes The predictions get output in ROI CSV format . You can view the predictions with the ADAMS Preview browser and, e.g., the ObjectLocationsFromSpreadSheet handler. You need to configure this generic handler via the ... button, entering the columns for the bounding box ( x0 , y0 , x1 , y1 ) and the label ( label_str ) of the reader ( ObjectLocationsSpreadSheetReader ). Example prediction","title":"Predicting"},{"location":"object_detection/yolov5/","text":"Yolov5 is a family of object detection architectures and models pretrained on the COCO dataset, and represents Ultralytics open-source research into future vision AI methods. Custom docker images with additional tools are available from here: https://github.com/waikato-datamining/pytorch/tree/master/yolov5 Prerequisites Make sure you have the directory structure created as outlined in the Prerequisites . Data In this example, we will use the American Sign Language Letters dataset, which consists of sets of images of hands, one per letter in the English alphabet (26 labels). Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/american-sign-language-letters/american-sign-language-letters-voc.zip Once extracted, rename the voc directory to sign-voc . Now we have to convert the format from VOC XML into YOLO . We can do this by using the wai.annotations library. At the same time, we can split the dataset into train , validation and test subsets. From within the applied_deep_learning directory, run the following command: docker run -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:latest \\ wai-annotations convert \\ from-voc-od \\ -i \"/workspace/data/sign-voc/*.xml\" \\ to-yolo-od \\ -o /workspace/data/sign-yolo-split \\ --labels /workspace/data/sign-yolo-split/labels.txt \\ --labels-csv /workspace/data/sign-yolo-split/labels.csv \\ --split-names train val test \\ --split-ratios 70 15 15 NB: At the time of writing, the yolo plugin for wai.annotations still had a bug which creates empty top-level directories when splitting datasets via --split-names . In the sign-yolo-split directory, you can safely remove the train , test and val directories, since the actual splits are below the images and labels directories: rm -fR data/sign-yolo-split/train data/sign-yolo-split/test data/sign-yolo-split/val Finally, download the dataset.yaml file and place it in the sign-yolo-split directory. It contains information about the dataset directory, the splits and the class labels. Since the labels can come out in a random order, you need to update the labels in the yaml file with the ones that got output in the labels.txt file. You can automatically quote the comma-separated list using the following command: cat data/sign-yolo-split/labels.txt | sed s/,/\\',\\'/g | sed s/^/\\'/g | sed s/$/\\'/g Training For training, we will use the following docker image: waikatodatamining/pytorch-yolov5:2022-05-31_cuda11.1 If you do not have a GPU, you can use the CPU-only image: waikatodatamining/pytorch-yolov5:2022-01-21_cpu The training script is called yolov5_train , for which we can invoke the help screen as follows: docker run -t waikatodatamining/pytorch-yolov5:2022-05-31_cuda11.1 yolov5_train --help Instead of using config files, we can just tweak parameters via command-line options. However, we still need to download a base model to use for training. Yolov5 offers different models, which differ in speed and accuracy. We will use the medium one called yolov5m.pt from the v6.0 release: https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5m.pt Download it and place it in the models directory. It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: sign-yolov5 NB: Yolov5 will automatically append a number to the output directory if said directory already exists (in order not to overwrite any existing models/outputs). Since the image size should be a multiple of 32, we use 416 for this experiment. Kick off the training with the following command: docker run \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/pytorch-yolov5:2022-05-31_cuda11.1 \\ yolov5_train \\ --img 416 \\ --batch 16 \\ --epochs 20 \\ --data /workspace/data/sign-yolo-split/dataset.yaml \\ --weights /workspace/models/yolov5m.pt \\ --project /workspace/output \\ --name sign-yolov5 Exporting to ONNX Before we can use our trained model, we will need to export it to ONNX format using the yolov5_export script: docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/pytorch-yolov5:2022-05-31_cuda11.1 \\ yolov5_export \\ --weights /workspace/output/sign-yolov5/weights/best.pt \\ --img-size 416 416 \\ --include onnx This will create a file called best.onnx in the output directory. Predicting Using the yolov5_predict_poll script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/pytorch-yolov5:2022-05-31_cuda11.1 \\ yolov5_predict_poll \\ --model /workspace/output/sign-yolov5/weights/best.onnx \\ --data /workspace/data/sign-yolo-split/dataset.yaml \\ --image_size 416 \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Notes The predictions get output in ROI CSV format . You can view the predictions with the ADAMS Preview browser and, e.g., the ObjectLocationsFromSpreadSheet handler. You need to configure this generic handler via the ... button, entering the columns for the bounding box ( x0 , y0 , x1 , y1 ) and the label ( label_str ) of the reader ( ObjectLocationsSpreadSheetReader ). Example prediction","title":"\u00bb Yolov5"},{"location":"object_detection/yolov5/#prerequisites","text":"Make sure you have the directory structure created as outlined in the Prerequisites .","title":"Prerequisites"},{"location":"object_detection/yolov5/#data","text":"In this example, we will use the American Sign Language Letters dataset, which consists of sets of images of hands, one per letter in the English alphabet (26 labels). Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/american-sign-language-letters/american-sign-language-letters-voc.zip Once extracted, rename the voc directory to sign-voc . Now we have to convert the format from VOC XML into YOLO . We can do this by using the wai.annotations library. At the same time, we can split the dataset into train , validation and test subsets. From within the applied_deep_learning directory, run the following command: docker run -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:latest \\ wai-annotations convert \\ from-voc-od \\ -i \"/workspace/data/sign-voc/*.xml\" \\ to-yolo-od \\ -o /workspace/data/sign-yolo-split \\ --labels /workspace/data/sign-yolo-split/labels.txt \\ --labels-csv /workspace/data/sign-yolo-split/labels.csv \\ --split-names train val test \\ --split-ratios 70 15 15 NB: At the time of writing, the yolo plugin for wai.annotations still had a bug which creates empty top-level directories when splitting datasets via --split-names . In the sign-yolo-split directory, you can safely remove the train , test and val directories, since the actual splits are below the images and labels directories: rm -fR data/sign-yolo-split/train data/sign-yolo-split/test data/sign-yolo-split/val Finally, download the dataset.yaml file and place it in the sign-yolo-split directory. It contains information about the dataset directory, the splits and the class labels. Since the labels can come out in a random order, you need to update the labels in the yaml file with the ones that got output in the labels.txt file. You can automatically quote the comma-separated list using the following command: cat data/sign-yolo-split/labels.txt | sed s/,/\\',\\'/g | sed s/^/\\'/g | sed s/$/\\'/g","title":"Data"},{"location":"object_detection/yolov5/#training","text":"For training, we will use the following docker image: waikatodatamining/pytorch-yolov5:2022-05-31_cuda11.1 If you do not have a GPU, you can use the CPU-only image: waikatodatamining/pytorch-yolov5:2022-01-21_cpu The training script is called yolov5_train , for which we can invoke the help screen as follows: docker run -t waikatodatamining/pytorch-yolov5:2022-05-31_cuda11.1 yolov5_train --help Instead of using config files, we can just tweak parameters via command-line options. However, we still need to download a base model to use for training. Yolov5 offers different models, which differ in speed and accuracy. We will use the medium one called yolov5m.pt from the v6.0 release: https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5m.pt Download it and place it in the models directory. It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: sign-yolov5 NB: Yolov5 will automatically append a number to the output directory if said directory already exists (in order not to overwrite any existing models/outputs). Since the image size should be a multiple of 32, we use 416 for this experiment. Kick off the training with the following command: docker run \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/pytorch-yolov5:2022-05-31_cuda11.1 \\ yolov5_train \\ --img 416 \\ --batch 16 \\ --epochs 20 \\ --data /workspace/data/sign-yolo-split/dataset.yaml \\ --weights /workspace/models/yolov5m.pt \\ --project /workspace/output \\ --name sign-yolov5","title":"Training"},{"location":"object_detection/yolov5/#exporting-to-onnx","text":"Before we can use our trained model, we will need to export it to ONNX format using the yolov5_export script: docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/pytorch-yolov5:2022-05-31_cuda11.1 \\ yolov5_export \\ --weights /workspace/output/sign-yolov5/weights/best.pt \\ --img-size 416 416 \\ --include onnx This will create a file called best.onnx in the output directory.","title":"Exporting to ONNX"},{"location":"object_detection/yolov5/#predicting","text":"Using the yolov5_predict_poll script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/pytorch-yolov5:2022-05-31_cuda11.1 \\ yolov5_predict_poll \\ --model /workspace/output/sign-yolov5/weights/best.onnx \\ --data /workspace/data/sign-yolo-split/dataset.yaml \\ --image_size 416 \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Notes The predictions get output in ROI CSV format . You can view the predictions with the ADAMS Preview browser and, e.g., the ObjectLocationsFromSpreadSheet handler. You need to configure this generic handler via the ... button, entering the columns for the bounding box ( x0 , y0 , x1 , y1 ) and the label ( label_str ) of the reader ( ObjectLocationsSpreadSheetReader ). Example prediction","title":"Predicting"}]}