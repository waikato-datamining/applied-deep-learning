{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Over the years, the Applied Machine Learning Group here at the University of Waikato has been working on a number of projects that involved applying deep learning algorithms to image problems. Setting up deep learning frameworks is always a slow and painstaking process, getting all the library dependencies right (CUDA, cuDNN, numpy, etc). To speed things up, we have developed (and maintain) a number of Python libraries and Docker images that can be used for various deep learning tasks. Here we are showing examples on how you can use these ready-to-use images on datasets to train your own models and how to make predictions with them. The use of Docker made it a lot easier and faster to apply algorithms to new datasets. If you have not used Docker before, we recommend you to have a look at our introduction called Docker for Data Scientists . The following domains are covered by the examples: Image classification Object detection Instance segmentation Image segmentation Speech-to-text These tutorials will use shorter training times (i.e., lower epochs/iterations/steps) to arrive faster at a model. However, this also means that the quality of the model will be lower. You will need to experiment with the training duration (and probably other hyper parameters) to achieve a good performance. The point of these tutorials is to get you going with deep learning. Note on I/O # To keep things simple, these examples use file-polling when making predictions, i.e., looking for images in an input directory and outputting predictions (and original input images in another directory). However, this may not be optimal when using SSDs, as they can wear out quickly when processing large amounts of files on a 24/7 basis. Having model pipelines can acerbate things even further. To alleviate wear and tear on the hardware, these frameworks also allow processing images via a Redis in-memory database backend. In this case, the models listen for images being broadcast on a Redis channel, pick them up, make predictions and then broadcast the predictions on another Redis channel. This allows for the construction of efficient, low-latency processing pipelines. When using this approach, the predictions are commonly broadcast in a JSON format, which can be easily processed in most programming languages. Redis itself has clients available in a wide range of programming languages as well. Note on Windows # The instructions in these tutorials were written for and executed on a Linux machine. However, you should be able to replicate these on Windows as well. Check out these instructions to get you set up.","title":"Home"},{"location":"#note-on-io","text":"To keep things simple, these examples use file-polling when making predictions, i.e., looking for images in an input directory and outputting predictions (and original input images in another directory). However, this may not be optimal when using SSDs, as they can wear out quickly when processing large amounts of files on a 24/7 basis. Having model pipelines can acerbate things even further. To alleviate wear and tear on the hardware, these frameworks also allow processing images via a Redis in-memory database backend. In this case, the models listen for images being broadcast on a Redis channel, pick them up, make predictions and then broadcast the predictions on another Redis channel. This allows for the construction of efficient, low-latency processing pipelines. When using this approach, the predictions are commonly broadcast in a JSON format, which can be easily processed in most programming languages. Redis itself has clients available in a wide range of programming languages as well.","title":"Note on I/O"},{"location":"#note-on-windows","text":"The instructions in these tutorials were written for and executed on a Linux machine. However, you should be able to replicate these on Windows as well. Check out these instructions to get you set up.","title":"Note on Windows"},{"location":"prerequisites/","text":"The following prerequisites apply to most data domains: Annotations # In order to annotate data (e.g., for object detection, instance segmentation or image segmentation), you should download the ADAMS Annotator application (you only need Java 11 or 17 installed ): Snapshot Format conversions # For turning annotations from one format into another, we will utilize the wai.annotations Python library: wai.annotations project wai.annotations manual Since we will use this library through Docker images, there is no need to install anything. If you should decide to install it locally, it is recommended to do so in a virtual environment Hardware # For building models, a computer with NVIDIA GPU (8GB+) and Linux operating system is recommended. Only image classification models can be built in a reasonable amount of time on CPU-only machines. Directory structure # Create the following directory structure for the examples of this tutorial: | +-- applied_deep_learning | +-- cache | | | +-- torch | | | +-- iopath_cache | +-- data | +-- models | +-- output | +-- predictions | +-- in | +-- out Use this command-line to create the directories: mkdir -p \\ applied_deep_learning/cache/torch \\ applied_deep_learning/cache/iopath_cache \\ applied_deep_learning/data \\ applied_deep_learning/models \\ applied_deep_learning/output \\ applied_deep_learning/predictions/in \\ applied_deep_learning/predictions/out Docker notes # To make the Docker commands as easy as possible, they will all get issued from the within the applied_deep_learning directory. In order to get access to the data , output and predictions directories, we use the following mapping: -v `pwd`:/workspace This will map the applied_deep_learning directory onto the /workspace directory within the container. If you have spaces in any of the parent directories, you need to use double quotes around the part before the : (otherwise you will get an error like docker: invalid reference format. ): -v \"`pwd`\":/workspace Since Docker usually runs as root within the container, we want to make sure that the user group of any files that get generated are being owned by the current user. This can be achieved by adding the following to the Docker command: -u $(id -u):$(id -g) -e USER=$USER","title":"Prerequisites"},{"location":"prerequisites/#annotations","text":"In order to annotate data (e.g., for object detection, instance segmentation or image segmentation), you should download the ADAMS Annotator application (you only need Java 11 or 17 installed ): Snapshot","title":"Annotations"},{"location":"prerequisites/#format-conversions","text":"For turning annotations from one format into another, we will utilize the wai.annotations Python library: wai.annotations project wai.annotations manual Since we will use this library through Docker images, there is no need to install anything. If you should decide to install it locally, it is recommended to do so in a virtual environment","title":"Format conversions"},{"location":"prerequisites/#hardware","text":"For building models, a computer with NVIDIA GPU (8GB+) and Linux operating system is recommended. Only image classification models can be built in a reasonable amount of time on CPU-only machines.","title":"Hardware"},{"location":"prerequisites/#directory-structure","text":"Create the following directory structure for the examples of this tutorial: | +-- applied_deep_learning | +-- cache | | | +-- torch | | | +-- iopath_cache | +-- data | +-- models | +-- output | +-- predictions | +-- in | +-- out Use this command-line to create the directories: mkdir -p \\ applied_deep_learning/cache/torch \\ applied_deep_learning/cache/iopath_cache \\ applied_deep_learning/data \\ applied_deep_learning/models \\ applied_deep_learning/output \\ applied_deep_learning/predictions/in \\ applied_deep_learning/predictions/out","title":"Directory structure"},{"location":"prerequisites/#docker-notes","text":"To make the Docker commands as easy as possible, they will all get issued from the within the applied_deep_learning directory. In order to get access to the data , output and predictions directories, we use the following mapping: -v `pwd`:/workspace This will map the applied_deep_learning directory onto the /workspace directory within the container. If you have spaces in any of the parent directories, you need to use double quotes around the part before the : (otherwise you will get an error like docker: invalid reference format. ): -v \"`pwd`\":/workspace Since Docker usually runs as root within the container, we want to make sure that the user group of any files that get generated are being owned by the current user. This can be achieved by adding the following to the Docker command: -u $(id -u):$(id -g) -e USER=$USER","title":"Docker notes"},{"location":"previewing_predictions/","text":"Below you can find ways of previewing predictions generated by the various frameworks used through this tutorial. For most of them, you can use ADAMS' Preview browser and a specific content handler. Configurations for these handlers can be applied as follows: Copy a configuration from below to the clipboard. Open the Preview browser in ADAMS (under the Visualization menu). Click on the ... button at the bottom to bring up the options dialog. Click on the \u22c1 button at the top right of the dialog and select Paste setup . Accept the setup by clicking on the OK button. Domains # Image classification # CSV # adams.gui.tools.previewbrowser.ImageClassificationHandler -image-reader adams.data.io.input.JAIImageReader -reader \"adams.data.io.input.ImageClassificationSpreadSheetReportReader -input ${HOME}/temp/preview/image_01965.csv -reader \\\"adams.data.io.input.CsvSpreadSheetReader -data-row-type adams.data.spreadsheet.DenseDataRow -spreadsheet-type adams.data.spreadsheet.DefaultSpreadSheet\\\"\" -format \"$ ({score|.2})\" Example: JSON # The following content handler overlays the label and score on the image: adams.gui.tools.previewbrowser.ImageClassificationHandler -image-reader adams.data.io.input.JAIImageReader -reader adams.data.io.input.ImageClassificationJsonReportReader -format \"$ ({score|.2})\" Example: Object detection # ROIs CSV format # adams.gui.tools.previewbrowser.ObjectAnnotationsHandler -image-reader adams.data.io.input.JAIImageReader -file-suffix -rois.csv -reader \"adams.data.io.input.ObjectLocationsSpreadSheetReader -input ${HOME}/temp/preview/A11_jpg.rf.ff2610c21c7f6d0a793cb58efc3bd96e-rois.csv -reader \\\"adams.data.io.input.CsvSpreadSheetReader -data-row-type adams.data.spreadsheet.DenseDataRow -spreadsheet-type adams.data.spreadsheet.DefaultSpreadSheet\\\" -row-finder adams.data.spreadsheet.rowfinder.AllFinder -col-left x0 -col-top y0 -col-right x1 -col-bottom y1 -col-polygon-x poly_x -col-polygon-y poly_y -col-type label_str -range-meta-data score\" -cleaner \"adams.gui.visualization.object.objectannotations.cleaning.OverlapRemoval -finder adams.data.objectfinder.AllFinder -overlap-detection adams.data.objectoverlap.AreaRatio -overlap-removal \\\"adams.data.overlappingobjectremoval.KeepHighestMetaDataValue -score-key score\\\"\" -shape-plotter adams.gui.visualization.object.objectannotations.shape.NoShape -shape-color \"adams.gui.visualization.object.objectannotations.colors.PerType -color-provider adams.gui.visualization.core.DefaultColorProvider\" -outline-plotter adams.gui.visualization.object.objectannotations.outline.PolygonOutline -outline-color \"adams.gui.visualization.object.objectannotations.colors.PerType -color-provider adams.gui.visualization.core.DefaultColorProvider\" -label-plotter \"adams.gui.visualization.object.objectannotations.label.Anchored -format \\\"$ ({score|.2})\\\"\" -label-color \"adams.gui.visualization.object.objectannotations.colors.PerType -color-provider adams.gui.visualization.core.DefaultColorProvider\" -show-object-panel true Example: OPEX format # adams.gui.tools.previewbrowser.ObjectAnnotationsHandler -image-reader adams.data.io.input.JAIImageReader -file-suffix .json -reader \"adams.data.io.input.OpexObjectLocationsReader -input ${HOME}/temp/preview/A11_jpg.rf.ff2610c21c7f6d0a793cb58efc3bd96e.json\" -cleaner \"adams.gui.visualization.object.objectannotations.cleaning.OverlapRemoval -finder adams.data.objectfinder.AllFinder -overlap-detection adams.data.objectoverlap.AreaRatio -overlap-removal \\\"adams.data.overlappingobjectremoval.KeepHighestMetaDataValue -score-key score\\\"\" -shape-plotter adams.gui.visualization.object.objectannotations.shape.NoShape -shape-color \"adams.gui.visualization.object.objectannotations.colors.PerType -color-provider adams.gui.visualization.core.DefaultColorProvider\" -outline-plotter adams.gui.visualization.object.objectannotations.outline.PolygonOutline -outline-color \"adams.gui.visualization.object.objectannotations.colors.PerType -color-provider adams.gui.visualization.core.DefaultColorProvider\" -label-plotter \"adams.gui.visualization.object.objectannotations.label.Anchored -format \\\"$ ({score|.2})\\\"\" -label-color \"adams.gui.visualization.object.objectannotations.colors.PerType -color-provider adams.gui.visualization.core.DefaultColorProvider\" -show-object-panel true Example: Instance segmentation # ROIs CSV format # adams.gui.tools.previewbrowser.ObjectAnnotationsHandler -image-reader adams.data.io.input.JAIImageReader -file-suffix -rois.csv -reader \"adams.data.io.input.ObjectLocationsSpreadSheetReader -input ${HOME}/temp/preview/Abyssinian_10-rois.csv -reader \\\"adams.data.io.input.CsvSpreadSheetReader -data-row-type adams.data.spreadsheet.DenseDataRow -spreadsheet-type adams.data.spreadsheet.DefaultSpreadSheet\\\" -row-finder adams.data.spreadsheet.rowfinder.AllFinder -col-left x0 -col-top y0 -col-right x1 -col-bottom y1 -col-polygon-x poly_x -col-polygon-y poly_y -col-type label_str -range-meta-data score\" -cleaner \"adams.gui.visualization.object.objectannotations.cleaning.OverlapRemoval -finder adams.data.objectfinder.AllFinder -overlap-detection adams.data.objectoverlap.AreaRatio -overlap-removal \\\"adams.data.overlappingobjectremoval.KeepHighestMetaDataValue -score-key score\\\"\" -shape-plotter \"adams.gui.visualization.object.objectannotations.shape.FilledPolygon -bounding-box-fallback-ratio 0.1\" -shape-color \"adams.gui.visualization.object.objectannotations.colors.PerType -color-provider \\\"adams.gui.visualization.core.TranslucentColorProvider -alpha 64 -provider adams.gui.visualization.core.DefaultColorProvider\\\"\" -outline-plotter adams.gui.visualization.object.objectannotations.outline.PolygonOutline -outline-color \"adams.gui.visualization.object.objectannotations.colors.PerType -color-provider adams.gui.visualization.core.DefaultColorProvider\" -label-plotter \"adams.gui.visualization.object.objectannotations.label.Anchored -format \\\"$ ({score|.2})\\\"\" -label-color \"adams.gui.visualization.object.objectannotations.colors.PerType -color-provider adams.gui.visualization.core.DefaultColorProvider\" -show-object-panel true Example: Image segmentation # Indexed PNG format # The following handler works for indexed PNGs and for grayscale ones: adams.gui.tools.previewbrowser.SimpleImageSegmentationAnnotationsHandler -reader adams.data.io.input.JAIImageReader -overlay-reader \"adams.data.io.input.PNGImageReader -color-provider adams.gui.visualization.core.DefaultColorProvider\" -colorizer \"adams.data.image.transformer.GrayOrIndexedColorizer -color-provider adams.gui.visualization.core.DefaultColorProvider\" Example: Speech-to-text # The following preview handler allows the playback of the WAV/MP3 file while displaying the content of the associated .txt file: adams.gui.tools.previewbrowser.SpeechPlaybackHandler Generic # Plain text # Any text file can be viewed with this handler: adams.gui.tools.previewbrowser.PlainTextHandler JSON # This handler displays JSON in a pretty printed way: adams.gui.tools.previewbrowser.JsonPrettyPrintHandler Example:","title":"Previewing predictions"},{"location":"previewing_predictions/#domains","text":"","title":"Domains"},{"location":"previewing_predictions/#image-classification","text":"","title":"Image classification"},{"location":"previewing_predictions/#imgcls_csv","text":"adams.gui.tools.previewbrowser.ImageClassificationHandler -image-reader adams.data.io.input.JAIImageReader -reader \"adams.data.io.input.ImageClassificationSpreadSheetReportReader -input ${HOME}/temp/preview/image_01965.csv -reader \\\"adams.data.io.input.CsvSpreadSheetReader -data-row-type adams.data.spreadsheet.DenseDataRow -spreadsheet-type adams.data.spreadsheet.DefaultSpreadSheet\\\"\" -format \"$ ({score|.2})\" Example:","title":"CSV"},{"location":"previewing_predictions/#imgcls_json","text":"The following content handler overlays the label and score on the image: adams.gui.tools.previewbrowser.ImageClassificationHandler -image-reader adams.data.io.input.JAIImageReader -reader adams.data.io.input.ImageClassificationJsonReportReader -format \"$ ({score|.2})\" Example:","title":"JSON"},{"location":"previewing_predictions/#object-detection","text":"","title":"Object detection"},{"location":"previewing_predictions/#objdet_rois","text":"adams.gui.tools.previewbrowser.ObjectAnnotationsHandler -image-reader adams.data.io.input.JAIImageReader -file-suffix -rois.csv -reader \"adams.data.io.input.ObjectLocationsSpreadSheetReader -input ${HOME}/temp/preview/A11_jpg.rf.ff2610c21c7f6d0a793cb58efc3bd96e-rois.csv -reader \\\"adams.data.io.input.CsvSpreadSheetReader -data-row-type adams.data.spreadsheet.DenseDataRow -spreadsheet-type adams.data.spreadsheet.DefaultSpreadSheet\\\" -row-finder adams.data.spreadsheet.rowfinder.AllFinder -col-left x0 -col-top y0 -col-right x1 -col-bottom y1 -col-polygon-x poly_x -col-polygon-y poly_y -col-type label_str -range-meta-data score\" -cleaner \"adams.gui.visualization.object.objectannotations.cleaning.OverlapRemoval -finder adams.data.objectfinder.AllFinder -overlap-detection adams.data.objectoverlap.AreaRatio -overlap-removal \\\"adams.data.overlappingobjectremoval.KeepHighestMetaDataValue -score-key score\\\"\" -shape-plotter adams.gui.visualization.object.objectannotations.shape.NoShape -shape-color \"adams.gui.visualization.object.objectannotations.colors.PerType -color-provider adams.gui.visualization.core.DefaultColorProvider\" -outline-plotter adams.gui.visualization.object.objectannotations.outline.PolygonOutline -outline-color \"adams.gui.visualization.object.objectannotations.colors.PerType -color-provider adams.gui.visualization.core.DefaultColorProvider\" -label-plotter \"adams.gui.visualization.object.objectannotations.label.Anchored -format \\\"$ ({score|.2})\\\"\" -label-color \"adams.gui.visualization.object.objectannotations.colors.PerType -color-provider adams.gui.visualization.core.DefaultColorProvider\" -show-object-panel true Example:","title":"ROIs CSV format"},{"location":"previewing_predictions/#objdet_opex","text":"adams.gui.tools.previewbrowser.ObjectAnnotationsHandler -image-reader adams.data.io.input.JAIImageReader -file-suffix .json -reader \"adams.data.io.input.OpexObjectLocationsReader -input ${HOME}/temp/preview/A11_jpg.rf.ff2610c21c7f6d0a793cb58efc3bd96e.json\" -cleaner \"adams.gui.visualization.object.objectannotations.cleaning.OverlapRemoval -finder adams.data.objectfinder.AllFinder -overlap-detection adams.data.objectoverlap.AreaRatio -overlap-removal \\\"adams.data.overlappingobjectremoval.KeepHighestMetaDataValue -score-key score\\\"\" -shape-plotter adams.gui.visualization.object.objectannotations.shape.NoShape -shape-color \"adams.gui.visualization.object.objectannotations.colors.PerType -color-provider adams.gui.visualization.core.DefaultColorProvider\" -outline-plotter adams.gui.visualization.object.objectannotations.outline.PolygonOutline -outline-color \"adams.gui.visualization.object.objectannotations.colors.PerType -color-provider adams.gui.visualization.core.DefaultColorProvider\" -label-plotter \"adams.gui.visualization.object.objectannotations.label.Anchored -format \\\"$ ({score|.2})\\\"\" -label-color \"adams.gui.visualization.object.objectannotations.colors.PerType -color-provider adams.gui.visualization.core.DefaultColorProvider\" -show-object-panel true Example:","title":"OPEX format"},{"location":"previewing_predictions/#instance-segmentation","text":"","title":"Instance segmentation"},{"location":"previewing_predictions/#insseg_rois","text":"adams.gui.tools.previewbrowser.ObjectAnnotationsHandler -image-reader adams.data.io.input.JAIImageReader -file-suffix -rois.csv -reader \"adams.data.io.input.ObjectLocationsSpreadSheetReader -input ${HOME}/temp/preview/Abyssinian_10-rois.csv -reader \\\"adams.data.io.input.CsvSpreadSheetReader -data-row-type adams.data.spreadsheet.DenseDataRow -spreadsheet-type adams.data.spreadsheet.DefaultSpreadSheet\\\" -row-finder adams.data.spreadsheet.rowfinder.AllFinder -col-left x0 -col-top y0 -col-right x1 -col-bottom y1 -col-polygon-x poly_x -col-polygon-y poly_y -col-type label_str -range-meta-data score\" -cleaner \"adams.gui.visualization.object.objectannotations.cleaning.OverlapRemoval -finder adams.data.objectfinder.AllFinder -overlap-detection adams.data.objectoverlap.AreaRatio -overlap-removal \\\"adams.data.overlappingobjectremoval.KeepHighestMetaDataValue -score-key score\\\"\" -shape-plotter \"adams.gui.visualization.object.objectannotations.shape.FilledPolygon -bounding-box-fallback-ratio 0.1\" -shape-color \"adams.gui.visualization.object.objectannotations.colors.PerType -color-provider \\\"adams.gui.visualization.core.TranslucentColorProvider -alpha 64 -provider adams.gui.visualization.core.DefaultColorProvider\\\"\" -outline-plotter adams.gui.visualization.object.objectannotations.outline.PolygonOutline -outline-color \"adams.gui.visualization.object.objectannotations.colors.PerType -color-provider adams.gui.visualization.core.DefaultColorProvider\" -label-plotter \"adams.gui.visualization.object.objectannotations.label.Anchored -format \\\"$ ({score|.2})\\\"\" -label-color \"adams.gui.visualization.object.objectannotations.colors.PerType -color-provider adams.gui.visualization.core.DefaultColorProvider\" -show-object-panel true Example:","title":"ROIs CSV format"},{"location":"previewing_predictions/#image-segmentation","text":"","title":"Image segmentation"},{"location":"previewing_predictions/#imgseg_indexed","text":"The following handler works for indexed PNGs and for grayscale ones: adams.gui.tools.previewbrowser.SimpleImageSegmentationAnnotationsHandler -reader adams.data.io.input.JAIImageReader -overlay-reader \"adams.data.io.input.PNGImageReader -color-provider adams.gui.visualization.core.DefaultColorProvider\" -colorizer \"adams.data.image.transformer.GrayOrIndexedColorizer -color-provider adams.gui.visualization.core.DefaultColorProvider\" Example:","title":"Indexed PNG format"},{"location":"previewing_predictions/#stt","text":"The following preview handler allows the playback of the WAV/MP3 file while displaying the content of the associated .txt file: adams.gui.tools.previewbrowser.SpeechPlaybackHandler","title":"Speech-to-text"},{"location":"previewing_predictions/#generic","text":"","title":"Generic"},{"location":"previewing_predictions/#generic_plaintext","text":"Any text file can be viewed with this handler: adams.gui.tools.previewbrowser.PlainTextHandler","title":"Plain text"},{"location":"previewing_predictions/#generic_json","text":"This handler displays JSON in a pretty printed way: adams.gui.tools.previewbrowser.JsonPrettyPrintHandler Example:","title":"JSON"},{"location":"windows/","text":"Installing Docker # One way of installing Docker on Windows is using Docker Desktop . However, the terms for using Docker Desktop changed on August 31st, 2022 and it may no longer be free for you (e.g., when working at a larger research organization or company). However, if you are not afraid of running a few commands in the terminal and having a terminal open while working with Docker, then you can follow these instructions of getting Docker working under WSL2 . Get Ubuntu 20.04.5 from the Windows store Configure the default user and password when asked for during the installation Get your system ready sudo apt update && sudo apt upgrade` sudo apt install --no-install-recommends apt-transport-https ca-certificates curl gnupg2 Change iptables to legacy : sudo update-alternatives --config iptables Install Docker . /etc/os-release curl -fsSL https://download.docker.com/linux/${ID}/gpg | sudo tee /etc/apt/trusted.gpg.d/docker.asc echo \"deb [arch=amd64] https://download.docker.com/linux/${ID} ${VERSION_CODENAME} stable\" | sudo tee /etc/apt/sources.list.d/docker.list sudo apt update sudo apt install docker-ce docker-ce-cli containerd.io sudo usermod -aG docker $USER Close the WSL2 window and open a new one for the changes to take effect Test your Docker installation by running: sudo dockerd After a lot of output of Docker starting up, you should see the following: API listen on /var/run/docker.sock (The above instructions were taken from this post by Jonathan Bowman) Installing NVIDIA Docker # Since our CUDA dependencies are packaged within the Docker images, we only need to worry about installing NVIDIA Docker to get access to the GPU: distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit Due to installing a new runtime, we need to restart our dockerd daemon, of course. (The above instructions were taken from this post by Frank Chung) Testing the GPU # The following commands test the inference of a Yolov7 model on the GPU: Create a test directory: mkdir gpu-test cd gpu-test Download the pre-trained model and a test image: wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt wget \"https://raw.githubusercontent.com/waikato-datamining/adams-addons/master/adams-docker/src/main/flows/data/2021_Toyota_GR_Yaris_Circuit_4WD_1.6_(1).jpg\" Perform inference: docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -w /workspace \\ --gpus=all \\ -t waikatodatamining/pytorch-yolov7:2022-10-08_cuda11.1 \\ yolov7_detect \\ --weights ./yolov7_training.pt \\ --source ./'2021_Toyota_GR_Yaris_Circuit_4WD_1.6_(1).jpg' \\ --no-trace \\ --conf-thres 0.8 \\ --device 0 In directory gpu-test/runs/detect/exp you will find the image with the detected objects overlaid.","title":"Docker on Windows"},{"location":"windows/#installing-docker","text":"One way of installing Docker on Windows is using Docker Desktop . However, the terms for using Docker Desktop changed on August 31st, 2022 and it may no longer be free for you (e.g., when working at a larger research organization or company). However, if you are not afraid of running a few commands in the terminal and having a terminal open while working with Docker, then you can follow these instructions of getting Docker working under WSL2 . Get Ubuntu 20.04.5 from the Windows store Configure the default user and password when asked for during the installation Get your system ready sudo apt update && sudo apt upgrade` sudo apt install --no-install-recommends apt-transport-https ca-certificates curl gnupg2 Change iptables to legacy : sudo update-alternatives --config iptables Install Docker . /etc/os-release curl -fsSL https://download.docker.com/linux/${ID}/gpg | sudo tee /etc/apt/trusted.gpg.d/docker.asc echo \"deb [arch=amd64] https://download.docker.com/linux/${ID} ${VERSION_CODENAME} stable\" | sudo tee /etc/apt/sources.list.d/docker.list sudo apt update sudo apt install docker-ce docker-ce-cli containerd.io sudo usermod -aG docker $USER Close the WSL2 window and open a new one for the changes to take effect Test your Docker installation by running: sudo dockerd After a lot of output of Docker starting up, you should see the following: API listen on /var/run/docker.sock (The above instructions were taken from this post by Jonathan Bowman)","title":"Installing Docker"},{"location":"windows/#installing-nvidia-docker","text":"Since our CUDA dependencies are packaged within the Docker images, we only need to worry about installing NVIDIA Docker to get access to the GPU: distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit Due to installing a new runtime, we need to restart our dockerd daemon, of course. (The above instructions were taken from this post by Frank Chung)","title":"Installing NVIDIA Docker"},{"location":"windows/#testing-the-gpu","text":"The following commands test the inference of a Yolov7 model on the GPU: Create a test directory: mkdir gpu-test cd gpu-test Download the pre-trained model and a test image: wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt wget \"https://raw.githubusercontent.com/waikato-datamining/adams-addons/master/adams-docker/src/main/flows/data/2021_Toyota_GR_Yaris_Circuit_4WD_1.6_(1).jpg\" Perform inference: docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -w /workspace \\ --gpus=all \\ -t waikatodatamining/pytorch-yolov7:2022-10-08_cuda11.1 \\ yolov7_detect \\ --weights ./yolov7_training.pt \\ --source ./'2021_Toyota_GR_Yaris_Circuit_4WD_1.6_(1).jpg' \\ --no-trace \\ --conf-thres 0.8 \\ --device 0 In directory gpu-test/runs/detect/exp you will find the image with the detected objects overlaid.","title":"Testing the GPU"},{"location":"image_classification/","text":"Image classification is the simplest and least computational expensive task, as it classifies whole images, assigning them a category. What to do next: Annotate your data Choose a framework","title":"Introduction"},{"location":"image_classification/annotate/","text":"Annotating data for image classification is straight forward. Since we are classifying whole images, we can simply use the directory layout for annotating images. In the example below, we have a dataset called flowers . The sub-directories below the flowers directory represent the labels for the images containing within the sub-directories. | +- flowers | +- daisy | +- dandelion | +- roses | +- sunflowers | +- tulip NB: To avoid any issues, the labels should be lowercase and underscores should be used instead of blanks/spaces.","title":"Annotate"},{"location":"image_classification/frameworks/","text":"The following frameworks are available for image classification : MMClassification tf2_make_image_classifier wai.tfimageclass","title":"Frameworks"},{"location":"image_classification/mmclassification/","text":"MMClassification is a comprehensive and flexible framework for image segmentation that offers a wide variety of models. Custom docker images with additional tools are available from here: https://github.com/waikato-datamining/mmclassification Prerequisites # Make sure you have the directory structure created as outlined in the Prerequisites . Data # In this example, we will use the 102 flowers dataset, which consists of 102 different categories (~ species) of flowers. More precisely, we will download the dataset with the flowers already split into categories from which we will use a subset to speed up the training process. Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/102flowers/102flowers-subdir.zip Once extracted, you can delete all sub-directories apart from: alpine_sea_holly anthurium artichoke azalea ball_moss Rename the subdir directory to 5flowers and move it into the data folder of our directory structure outlined. Split the data into train , validation and test subsets using wai.annotations : docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:0.8.0 \\ wai-annotations convert \\ from-subdir-ic \\ -i \"/workspace/data/5flowers/**/*.jpg\" \\ to-subdir-ic \\ -o /workspace/data/5flowers-split \\ --split-names train val test \\ --split-ratios 70 15 15 Training # For training, we will use the following docker image: waikatodatamining/mmclassification:0.25.0_cuda11.1 If you only have a CPU machine available, then use this one instead: waikatodatamining/mmclassification:0.25.0_cpu The training script is called mmcls_train , for which we can invoke the help screen as follows: docker run --rm -t waikatodatamining/mmclassification:0.25.0_cuda11.1 mmcls_train --help # GPU docker run --rm -t waikatodatamining/mmclassification:0.25.0_cpu mmcls_train --help # CPU It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: 5flowers-mmcl-r18 Before we can train, we will need to obtain and customize a config file. Within the container, you can find example configurations for various architectures in the following directory: /mmclassification/configs Using the mmseg_config command, we can expand and dump one of these configurations for our own purposes: CPU: docker run --rm \\ -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -t waikatodatamining/mmclassification:0.25.0_cpu \\ mmcls_config \\ /mmclassification/configs/resnet/resnet18_b32x8_imagenet.py \\ > `pwd`/output/5flowers-mmcl-r18/resnet18_b32x8_imagenet.py GPU: docker run --rm \\ -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -t waikatodatamining/mmclassification:0.25.0_cuda11.1 \\ mmcls_config \\ /mmclassification/configs/resnet/resnet18_b32x8_imagenet.py \\ > `pwd`/output/5flowers-mmcl-r18/resnet18_b32x8_imagenet.py Open the resnet18_b32x8_imagenet.py file in a text editor and perform the following operations: remove the lines before model = dict( change num_classes to 5 change dataset_type to ExternalDataset and any occurrences of type in the train , test , val sections of the data dictionary change data_prefix to /workspace/data/5flowers-split/train , ../test and ../val in the relevant sections change ann_file occurrences to None change max_epochs in the runner to a suitable value, e.g., 10 change the interval of the checkpoint_config to a value that makes sense with max_epochs , e.g., 5 if we want to perform transfer learning, we can re-use a pretrained model from the model zoo and just need to specify it in the backbone dictionary, by inserting the following before the style='pytorch' setting (see also documentation ): init_cfg=dict( type='Pretrained', checkpoint='https://download.openmmlab.com/mmclassification/v0/resnet/resnet18_batch256_imagenet_20200708-34ab8f90.pth', prefix='backbone', ), GPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMCLS_CLASSES=alpine_sea_holly,anthurium,artichoke,azalea,ball_moss \\ -t waikatodatamining/mmclassification:0.25.0_cuda11.1 \\ mmcls_train \\ /workspace/output/5flowers-mmcl-r18/resnet18_b32x8_imagenet.py \\ --work-dir /workspace/output/5flowers-mmcl-r18 CPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ --shm-size 8G \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMCLS_CLASSES=alpine_sea_holly,anthurium,artichoke,azalea,ball_moss \\ -t waikatodatamining/mmclassification:0.25.0_cpu \\ mmcls_train \\ /workspace/output/5flowers-mmcl-r18/resnet18_b32x8_imagenet.py \\ --work-dir /workspace/output/5flowers-mmcl-r18 Predicting # Using the mmcls_predict_poll script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): GPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -e MMCLS_CLASSES=alpine_sea_holly,anthurium,artichoke,azalea,ball_moss \\ -t waikatodatamining/mmclassification:0.25.0_cuda11.1 \\ mmcls_predict_poll \\ --model /workspace/output/5flowers-mmcl-r18/latest.pth \\ --config /workspace/output/5flowers-mmcl-r18/resnet18_b32x8_imagenet.py \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out CPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ --shm-size 8G \\ -v `pwd`:/workspace \\ -e MMCLS_CLASSES=alpine_sea_holly,anthurium,artichoke,azalea,ball_moss \\ -t waikatodatamining/mmclassification:0.25.0_cpu \\ mmcls_predict_poll \\ --model /workspace/output/5flowers-mmcl-r18/latest.pth \\ --config /workspace/output/5flowers-mmcl-r18/resnet18_b32x8_imagenet.py \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out E.g., for the image_08085.jpg from the alpine_sea_holly class, we will get a JSON file similar to this one : { \"alpine_sea_holly\": 0.9981447458267212, \"anthurium\": 1.6010968465707265e-05, \"artichoke\": 0.0015235934406518936, \"azalea\": 2.2820820504421135e-06, \"ball_moss\": 0.00031330727506428957 } Notes You can view the predictions with the ADAMS Preview browser : Image classification (JSON) Troubleshooting # RuntimeError: selected index k out of range This can occur if you have less than 5 class labels. You need to update the config file as follows ( source ): set model/head/topk to (1, ) rather than (1, 5) add metric_options={'topk': (1, ) to the evaluation dictionary","title":"MMClassification"},{"location":"image_classification/mmclassification/#prerequisites","text":"Make sure you have the directory structure created as outlined in the Prerequisites .","title":"Prerequisites"},{"location":"image_classification/mmclassification/#data","text":"In this example, we will use the 102 flowers dataset, which consists of 102 different categories (~ species) of flowers. More precisely, we will download the dataset with the flowers already split into categories from which we will use a subset to speed up the training process. Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/102flowers/102flowers-subdir.zip Once extracted, you can delete all sub-directories apart from: alpine_sea_holly anthurium artichoke azalea ball_moss Rename the subdir directory to 5flowers and move it into the data folder of our directory structure outlined. Split the data into train , validation and test subsets using wai.annotations : docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:0.8.0 \\ wai-annotations convert \\ from-subdir-ic \\ -i \"/workspace/data/5flowers/**/*.jpg\" \\ to-subdir-ic \\ -o /workspace/data/5flowers-split \\ --split-names train val test \\ --split-ratios 70 15 15","title":"Data"},{"location":"image_classification/mmclassification/#training","text":"For training, we will use the following docker image: waikatodatamining/mmclassification:0.25.0_cuda11.1 If you only have a CPU machine available, then use this one instead: waikatodatamining/mmclassification:0.25.0_cpu The training script is called mmcls_train , for which we can invoke the help screen as follows: docker run --rm -t waikatodatamining/mmclassification:0.25.0_cuda11.1 mmcls_train --help # GPU docker run --rm -t waikatodatamining/mmclassification:0.25.0_cpu mmcls_train --help # CPU It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: 5flowers-mmcl-r18 Before we can train, we will need to obtain and customize a config file. Within the container, you can find example configurations for various architectures in the following directory: /mmclassification/configs Using the mmseg_config command, we can expand and dump one of these configurations for our own purposes: CPU: docker run --rm \\ -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -t waikatodatamining/mmclassification:0.25.0_cpu \\ mmcls_config \\ /mmclassification/configs/resnet/resnet18_b32x8_imagenet.py \\ > `pwd`/output/5flowers-mmcl-r18/resnet18_b32x8_imagenet.py GPU: docker run --rm \\ -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -t waikatodatamining/mmclassification:0.25.0_cuda11.1 \\ mmcls_config \\ /mmclassification/configs/resnet/resnet18_b32x8_imagenet.py \\ > `pwd`/output/5flowers-mmcl-r18/resnet18_b32x8_imagenet.py Open the resnet18_b32x8_imagenet.py file in a text editor and perform the following operations: remove the lines before model = dict( change num_classes to 5 change dataset_type to ExternalDataset and any occurrences of type in the train , test , val sections of the data dictionary change data_prefix to /workspace/data/5flowers-split/train , ../test and ../val in the relevant sections change ann_file occurrences to None change max_epochs in the runner to a suitable value, e.g., 10 change the interval of the checkpoint_config to a value that makes sense with max_epochs , e.g., 5 if we want to perform transfer learning, we can re-use a pretrained model from the model zoo and just need to specify it in the backbone dictionary, by inserting the following before the style='pytorch' setting (see also documentation ): init_cfg=dict( type='Pretrained', checkpoint='https://download.openmmlab.com/mmclassification/v0/resnet/resnet18_batch256_imagenet_20200708-34ab8f90.pth', prefix='backbone', ), GPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMCLS_CLASSES=alpine_sea_holly,anthurium,artichoke,azalea,ball_moss \\ -t waikatodatamining/mmclassification:0.25.0_cuda11.1 \\ mmcls_train \\ /workspace/output/5flowers-mmcl-r18/resnet18_b32x8_imagenet.py \\ --work-dir /workspace/output/5flowers-mmcl-r18 CPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ --shm-size 8G \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMCLS_CLASSES=alpine_sea_holly,anthurium,artichoke,azalea,ball_moss \\ -t waikatodatamining/mmclassification:0.25.0_cpu \\ mmcls_train \\ /workspace/output/5flowers-mmcl-r18/resnet18_b32x8_imagenet.py \\ --work-dir /workspace/output/5flowers-mmcl-r18","title":"Training"},{"location":"image_classification/mmclassification/#predicting","text":"Using the mmcls_predict_poll script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): GPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -e MMCLS_CLASSES=alpine_sea_holly,anthurium,artichoke,azalea,ball_moss \\ -t waikatodatamining/mmclassification:0.25.0_cuda11.1 \\ mmcls_predict_poll \\ --model /workspace/output/5flowers-mmcl-r18/latest.pth \\ --config /workspace/output/5flowers-mmcl-r18/resnet18_b32x8_imagenet.py \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out CPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ --shm-size 8G \\ -v `pwd`:/workspace \\ -e MMCLS_CLASSES=alpine_sea_holly,anthurium,artichoke,azalea,ball_moss \\ -t waikatodatamining/mmclassification:0.25.0_cpu \\ mmcls_predict_poll \\ --model /workspace/output/5flowers-mmcl-r18/latest.pth \\ --config /workspace/output/5flowers-mmcl-r18/resnet18_b32x8_imagenet.py \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out E.g., for the image_08085.jpg from the alpine_sea_holly class, we will get a JSON file similar to this one : { \"alpine_sea_holly\": 0.9981447458267212, \"anthurium\": 1.6010968465707265e-05, \"artichoke\": 0.0015235934406518936, \"azalea\": 2.2820820504421135e-06, \"ball_moss\": 0.00031330727506428957 } Notes You can view the predictions with the ADAMS Preview browser : Image classification (JSON)","title":"Predicting"},{"location":"image_classification/mmclassification/#troubleshooting","text":"RuntimeError: selected index k out of range This can occur if you have less than 5 class labels. You need to update the config file as follows ( source ): set model/head/topk to (1, ) rather than (1, 5) add metric_options={'topk': (1, ) to the evaluation dictionary","title":"Troubleshooting"},{"location":"image_classification/tf2_make_image_classifier/","text":"The make_image_classifier Python library can be used for training various TensorFlow 2 image classification models that are available from Tensorflow Hub . Code for the Docker images and additional Python code is available from here: https://github.com/waikato-datamining/tensorflow/tree/master/image_classification2 Prerequisites # Make sure you have the directory structure created as outlined in the Prerequisites . Data # In this example, we will use the 102 flowers dataset, which consists of 102 different categories (~ species) of flowers. More precisely, we will download the dataset with the flowers already split into categories from which we will use a subset to speed up the training process. Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/102flowers/102flowers-subdir.zip Once extracted, you can delete all sub-directories apart from: alpine_sea_holly anthurium artichoke Rename the subdir directory to 3flowers and move it into the data folder of our directory structure outlined. Training # For training, we will use the following docker image: waikatodatamining/tf_image_classification2:2.9.1_cuda11.1 If you only have a CPU machine available, then use this one instead: waikatodatamining/tf_image_classification2:2.9.1_cpu The training script is called make_image_classifier , for which we can invoke the help screen as follows: docker run --rm -t waikatodatamining/tf_image_classification2:2.9.1_cuda11.1 make_image_classifier --helpfull # GPU docker run --rm -t waikatodatamining/tf_image_classification2:2.9.1_cpu make_image_classifier --helpfull # CPU It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: 3flowers-tf2-default The following command will train an EfficientNet b0 model for 10 epochs: GPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules \\ -t waikatodatamining/tf_image_classification2:2.9.1_cuda11.1 \\ make_image_classifier \\ --image_dir /workspace/data/3flowers \\ --image_size 224 \\ --saved_model_dir /workspace/output/3flowers-tf2-default \\ --labels_output_file /workspace/output/3flowers-tf2-default/labels.txt \\ --tflite_output_file /workspace/output/3flowers-tf2-default/model.tflite \\ --tfhub_module https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/feature_vector/2 \\ --train_epochs 10 CPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules \\ -t waikatodatamining/tf_image_classification2:2.9.1_cpu \\ make_image_classifier \\ --image_dir /workspace/data/3flowers \\ --image_size 224 \\ --saved_model_dir /workspace/output/3flowers-tf2-default \\ --labels_output_file /workspace/output/3flowers-tf2-default/labels.txt \\ --tflite_output_file /workspace/output/3flowers-tf2-default/model.tflite \\ --tfhub_module https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/feature_vector/2 \\ --train_epochs 10 Predicting # For making predictions for a single image, you can use the script label_image . Since we will want to batch predict multiple images, will use the script predict_poll instead: GPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_image_classification2:2.9.1_cuda11.1 \\ predict_poll \\ --model /workspace/output/3flowers-tf2-default/model.tflite \\ --labels /workspace/output/3flowers-tf2-default/labels.txt \\ --input_mean 0 \\ --input_std 255 \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out CPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_image_classification2:2.9.1_cpu \\ predict_poll \\ --model /workspace/output/3flowers-tf2-default/model.tflite \\ --labels /workspace/output/3flowers-tf2-default/labels.txt \\ --input_mean 0 \\ --input_std 255 \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out E.g., for the image_02048.jpg from the anthurium class, we will get a JSON file similar to this one : { \"anthurium\": 0.8697358965873718, \"alpine_sea_holly\": 0.06686040759086609, \"artichoke\": 0.0634036660194397 } Notes You can view the predictions with the ADAMS Preview browser : Image classification (JSON)","title":"tf2_make_image_classifier"},{"location":"image_classification/tf2_make_image_classifier/#prerequisites","text":"Make sure you have the directory structure created as outlined in the Prerequisites .","title":"Prerequisites"},{"location":"image_classification/tf2_make_image_classifier/#data","text":"In this example, we will use the 102 flowers dataset, which consists of 102 different categories (~ species) of flowers. More precisely, we will download the dataset with the flowers already split into categories from which we will use a subset to speed up the training process. Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/102flowers/102flowers-subdir.zip Once extracted, you can delete all sub-directories apart from: alpine_sea_holly anthurium artichoke Rename the subdir directory to 3flowers and move it into the data folder of our directory structure outlined.","title":"Data"},{"location":"image_classification/tf2_make_image_classifier/#training","text":"For training, we will use the following docker image: waikatodatamining/tf_image_classification2:2.9.1_cuda11.1 If you only have a CPU machine available, then use this one instead: waikatodatamining/tf_image_classification2:2.9.1_cpu The training script is called make_image_classifier , for which we can invoke the help screen as follows: docker run --rm -t waikatodatamining/tf_image_classification2:2.9.1_cuda11.1 make_image_classifier --helpfull # GPU docker run --rm -t waikatodatamining/tf_image_classification2:2.9.1_cpu make_image_classifier --helpfull # CPU It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: 3flowers-tf2-default The following command will train an EfficientNet b0 model for 10 epochs: GPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules \\ -t waikatodatamining/tf_image_classification2:2.9.1_cuda11.1 \\ make_image_classifier \\ --image_dir /workspace/data/3flowers \\ --image_size 224 \\ --saved_model_dir /workspace/output/3flowers-tf2-default \\ --labels_output_file /workspace/output/3flowers-tf2-default/labels.txt \\ --tflite_output_file /workspace/output/3flowers-tf2-default/model.tflite \\ --tfhub_module https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/feature_vector/2 \\ --train_epochs 10 CPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules \\ -t waikatodatamining/tf_image_classification2:2.9.1_cpu \\ make_image_classifier \\ --image_dir /workspace/data/3flowers \\ --image_size 224 \\ --saved_model_dir /workspace/output/3flowers-tf2-default \\ --labels_output_file /workspace/output/3flowers-tf2-default/labels.txt \\ --tflite_output_file /workspace/output/3flowers-tf2-default/model.tflite \\ --tfhub_module https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/feature_vector/2 \\ --train_epochs 10","title":"Training"},{"location":"image_classification/tf2_make_image_classifier/#predicting","text":"For making predictions for a single image, you can use the script label_image . Since we will want to batch predict multiple images, will use the script predict_poll instead: GPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_image_classification2:2.9.1_cuda11.1 \\ predict_poll \\ --model /workspace/output/3flowers-tf2-default/model.tflite \\ --labels /workspace/output/3flowers-tf2-default/labels.txt \\ --input_mean 0 \\ --input_std 255 \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out CPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_image_classification2:2.9.1_cpu \\ predict_poll \\ --model /workspace/output/3flowers-tf2-default/model.tflite \\ --labels /workspace/output/3flowers-tf2-default/labels.txt \\ --input_mean 0 \\ --input_std 255 \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out E.g., for the image_02048.jpg from the anthurium class, we will get a JSON file similar to this one : { \"anthurium\": 0.8697358965873718, \"alpine_sea_holly\": 0.06686040759086609, \"artichoke\": 0.0634036660194397 } Notes You can view the predictions with the ADAMS Preview browser : Image classification (JSON)","title":"Predicting"},{"location":"image_classification/wai.tfimageclass/","text":"The wai.tfimageclass Python library ( PyPI ) can be used for training various models that are available from Tensorflow Hub . Prerequisites # Make sure you have the directory structure created as outlined in the Prerequisites . Data # In this example, we will use the 102 flowers dataset, which consists of 102 different categories (~ species) of flowers. More precisely, we will download the dataset with the flowers already split into categories from which we will use a subset to speed up the training process. Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/102flowers/102flowers-subdir.zip Once extracted, you can delete all sub-directories apart from: alpine_sea_holly anthurium artichoke Rename the subdir directory to 3flowers and move it into the data folder of our directory structure outlined. Training # For training, we will use the following docker image: waikatodatamining/tf_image_classification:1.14 If you only have a CPU machine available, then use this one instead: waikatodatamining/tf_image_classification:1.14_cpu The training script is called tfic-retrain , for which we can invoke the help screen as follows: docker run --rm -t waikatodatamining/tf_image_classification:1.14 tfic-retrain --help # GPU docker run --rm -t waikatodatamining/tf_image_classification:1.14_cpu tfic-retrain --help # CPU It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: 3flowers-tf-default The following command will train an Inception v3 model for 500 steps: GPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules \\ -t waikatodatamining/tf_image_classification:1.14 \\ tfic-retrain \\ --image_dir /workspace/data/3flowers \\ --output_graph /workspace/output/3flowers-tf-default/graph.pb \\ --output_info /workspace/output/3flowers-tf-default/graph.json \\ --saved_model_dir /workspace/output/3flowers-tf-default/saved_model \\ --image_lists_dir /workspace/output/3flowers-tf-default \\ --tfhub_module https://tfhub.dev/google/imagenet/inception_v3/feature_vector/3 \\ --training_steps 500 CPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules \\ -t waikatodatamining/tf_image_classification:1.14_cpu \\ tfic-retrain \\ --image_dir /workspace/data/3flowers \\ --output_graph /workspace/output/3flowers-tf-default/graph.pb \\ --output_info /workspace/output/3flowers-tf-default/graph.json \\ --saved_model_dir /workspace/output/3flowers-tf-default/saved_model \\ --image_lists_dir /workspace/output/3flowers-tf-default \\ --tfhub_module https://tfhub.dev/google/imagenet/inception_v3/feature_vector/3 \\ --training_steps 500 Exporting model # Before we can use the model, we need to export it to Tensorflow lite or tflite : GPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules \\ -t waikatodatamining/tf_image_classification:1.14 \\ tfic-export \\ --saved_model_dir /workspace/output/3flowers-tf-default/saved_model \\ --tflite_model /workspace/output/3flowers-tf-default/graph.tflite CPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules \\ -t waikatodatamining/tf_image_classification:1.14_cpu \\ tfic-export \\ --saved_model_dir /workspace/output/3flowers-tf-default/saved_model \\ --tflite_model /workspace/output/3flowers-tf-default/graph.tflite Predicting # For making predictions for a single image, you can use the script tfic-labelimage . Since we will want to batch predict multiple images, will use the script tfic-poll instead: GPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_image_classification:1.14 \\ tfic-poll \\ --graph /workspace/output/3flowers-tf-default/graph.tflite \\ --graph_type tflite \\ --info /workspace/output/3flowers-tf-default/graph.json \\ --in_dir /workspace/predictions/in \\ --out_dir /workspace/predictions/out CPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_image_classification:1.14_cpu \\ tfic-poll \\ --graph /workspace/output/3flowers-tf-default/graph.tflite \\ --graph_type tflite \\ --info /workspace/output/3flowers-tf-default/graph.json \\ --in_dir /workspace/predictions/in \\ --out_dir /workspace/predictions/out E.g., for the image_01965.jpg from the anthurium class, we will get a CSV file similar to this one : label probability anthurium 0.989429 artichoke 0.0060946 alpine_sea_holly 0.00447612 Notes You can view the predictions with the ADAMS Preview browser : Image classification (CSV)","title":"wai.tfimageclass"},{"location":"image_classification/wai.tfimageclass/#prerequisites","text":"Make sure you have the directory structure created as outlined in the Prerequisites .","title":"Prerequisites"},{"location":"image_classification/wai.tfimageclass/#data","text":"In this example, we will use the 102 flowers dataset, which consists of 102 different categories (~ species) of flowers. More precisely, we will download the dataset with the flowers already split into categories from which we will use a subset to speed up the training process. Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/102flowers/102flowers-subdir.zip Once extracted, you can delete all sub-directories apart from: alpine_sea_holly anthurium artichoke Rename the subdir directory to 3flowers and move it into the data folder of our directory structure outlined.","title":"Data"},{"location":"image_classification/wai.tfimageclass/#training","text":"For training, we will use the following docker image: waikatodatamining/tf_image_classification:1.14 If you only have a CPU machine available, then use this one instead: waikatodatamining/tf_image_classification:1.14_cpu The training script is called tfic-retrain , for which we can invoke the help screen as follows: docker run --rm -t waikatodatamining/tf_image_classification:1.14 tfic-retrain --help # GPU docker run --rm -t waikatodatamining/tf_image_classification:1.14_cpu tfic-retrain --help # CPU It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: 3flowers-tf-default The following command will train an Inception v3 model for 500 steps: GPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules \\ -t waikatodatamining/tf_image_classification:1.14 \\ tfic-retrain \\ --image_dir /workspace/data/3flowers \\ --output_graph /workspace/output/3flowers-tf-default/graph.pb \\ --output_info /workspace/output/3flowers-tf-default/graph.json \\ --saved_model_dir /workspace/output/3flowers-tf-default/saved_model \\ --image_lists_dir /workspace/output/3flowers-tf-default \\ --tfhub_module https://tfhub.dev/google/imagenet/inception_v3/feature_vector/3 \\ --training_steps 500 CPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules \\ -t waikatodatamining/tf_image_classification:1.14_cpu \\ tfic-retrain \\ --image_dir /workspace/data/3flowers \\ --output_graph /workspace/output/3flowers-tf-default/graph.pb \\ --output_info /workspace/output/3flowers-tf-default/graph.json \\ --saved_model_dir /workspace/output/3flowers-tf-default/saved_model \\ --image_lists_dir /workspace/output/3flowers-tf-default \\ --tfhub_module https://tfhub.dev/google/imagenet/inception_v3/feature_vector/3 \\ --training_steps 500","title":"Training"},{"location":"image_classification/wai.tfimageclass/#exporting-model","text":"Before we can use the model, we need to export it to Tensorflow lite or tflite : GPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules \\ -t waikatodatamining/tf_image_classification:1.14 \\ tfic-export \\ --saved_model_dir /workspace/output/3flowers-tf-default/saved_model \\ --tflite_model /workspace/output/3flowers-tf-default/graph.tflite CPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules \\ -t waikatodatamining/tf_image_classification:1.14_cpu \\ tfic-export \\ --saved_model_dir /workspace/output/3flowers-tf-default/saved_model \\ --tflite_model /workspace/output/3flowers-tf-default/graph.tflite","title":"Exporting model"},{"location":"image_classification/wai.tfimageclass/#predicting","text":"For making predictions for a single image, you can use the script tfic-labelimage . Since we will want to batch predict multiple images, will use the script tfic-poll instead: GPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_image_classification:1.14 \\ tfic-poll \\ --graph /workspace/output/3flowers-tf-default/graph.tflite \\ --graph_type tflite \\ --info /workspace/output/3flowers-tf-default/graph.json \\ --in_dir /workspace/predictions/in \\ --out_dir /workspace/predictions/out CPU: docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_image_classification:1.14_cpu \\ tfic-poll \\ --graph /workspace/output/3flowers-tf-default/graph.tflite \\ --graph_type tflite \\ --info /workspace/output/3flowers-tf-default/graph.json \\ --in_dir /workspace/predictions/in \\ --out_dir /workspace/predictions/out E.g., for the image_01965.jpg from the anthurium class, we will get a CSV file similar to this one : label probability anthurium 0.989429 artichoke 0.0060946 alpine_sea_holly 0.00447612 Notes You can view the predictions with the ADAMS Preview browser : Image classification (CSV)","title":"Predicting"},{"location":"image_segmentation/","text":"Image segmentation classifies individual pixels within an image, assigning them a label (i.e., a color). What to do next: Annotate your data Choose a framework","title":"Introduction"},{"location":"image_segmentation/annotate/","text":"You can use the ADAMS framework for annotating your images. ADAMS comes pre-bundled in various setups and we need the adams-annotator bundle. You can get either: snapshot release (post 21.12.0 release) In the Flow editor (available from the Tools menu in ADAMS), you can load and execute the follow workflow (which is part of your ADAMS installation when downloading it as zip file): adams-imaging-image_segmentation_annotation.flow The following video takes you through the process:","title":"Annotate"},{"location":"image_segmentation/frameworks/","text":"The following frameworks are available for image segmentation : Image Segmentation Keras MMSegmentation","title":"Frameworks"},{"location":"image_segmentation/image-segmentation-keras/","text":"Image Segmentation Keras is a Keras/Tensorflow based image segmentation framework. Custom docker images with additional tools are available from here: https://github.com/waikato-datamining/tensorflow/tree/master/image-segmentation-keras Prerequisites # Make sure you have the directory structure created as outlined in the Prerequisites . Data # In this example, we will use the CamVid-12 dataset, which consists of still images from dashcam videos from a city environment (12 different labels). Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/camvid12/camvid12-grayscale.zip Once extracted, rename the grayscale directory to camvid-grayscale . Now we have to convert the format from grayscale into blue channel , which the framework uses. We can do this by using the wai.annotations library. At the same time, we can split the dataset into train , validation and test subsets. From within the applied_deep_learning directory, run the following command: docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:0.8.0 \\ wai-annotations convert \\ from-grayscale-is \\ -i \"/workspace/data/camvid-grayscale/*.png\" \\ --labels sky building pole road pavement tree signsymbol fence car pedestrian bicyclist unlabelled \\ to-blue-channel-is \\ -o /workspace/data/camvid-bluechannel-split \\ --split-names train val test \\ --split-ratios 70 15 15 Training # For training, we will use the following docker image: waikatodatamining/image-segmentation-keras:1.14.0_0.3.0 The training script is called keras_seg_train , for which we can invoke the help screen as follows: docker run --rm -t waikatodatamining/image-segmentation-keras:1.14.0_0.3.0 keras_seg_train --help It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: camvid12-keras-unet50 Image Segmentation Keras does not use a config file, but can be configured via command-line parameters instead: the number of classes ( --n_classes ) is actual classes plus background the width/height of the inputs must be multiples of 32 ( --input_height , --input_width ) the output directory is specified via --checkpoints_path (make sure to have a trailing slash!) in our case, the images and the annotations reside in the same directories, so --train_images and --train_annotations point to the same directory (analog for --val_images and --val_annotations ) with --epochs you can specify for how long the model will get trained --model_name specifies the architecture and backend (see here for available options) Kick off the training of a U-Net with a ResNet50 backend using the following command: docker run --rm \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/.keras \\ -t waikatodatamining/image-segmentation-keras:1.14.0_0.3.0 \\ keras_seg_train \\ --checkpoints_path /workspace/output/camvid12-keras-unet50/ \\ --train_images /workspace/data/camvid-bluechannel-split/train \\ --train_annotations /workspace/data/camvid-bluechannel-split/train \\ --val_images /workspace/data/camvid-bluechannel-split/val \\ --val_annotations /workspace/data/camvid-bluechannel-split/val \\ --epochs 10 \\ --n_classes 13 \\ --input_height 384 \\ --input_width 480 \\ --model_name resnet50_unet Predicting # Using the keras_seg_poll script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run --rm \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/.keras \\ -t waikatodatamining/image-segmentation-keras:1.14.0_0.3.0 \\ keras_seg_poll \\ --checkpoints_path /workspace/output/camvid12-keras-unet50/ \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Notes The predictions are output in Indexed PNG format. You can view the predictions with the ADAMS Preview browser : Indexed PNG Example prediction","title":"Image Segmentation Keras"},{"location":"image_segmentation/image-segmentation-keras/#prerequisites","text":"Make sure you have the directory structure created as outlined in the Prerequisites .","title":"Prerequisites"},{"location":"image_segmentation/image-segmentation-keras/#data","text":"In this example, we will use the CamVid-12 dataset, which consists of still images from dashcam videos from a city environment (12 different labels). Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/camvid12/camvid12-grayscale.zip Once extracted, rename the grayscale directory to camvid-grayscale . Now we have to convert the format from grayscale into blue channel , which the framework uses. We can do this by using the wai.annotations library. At the same time, we can split the dataset into train , validation and test subsets. From within the applied_deep_learning directory, run the following command: docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:0.8.0 \\ wai-annotations convert \\ from-grayscale-is \\ -i \"/workspace/data/camvid-grayscale/*.png\" \\ --labels sky building pole road pavement tree signsymbol fence car pedestrian bicyclist unlabelled \\ to-blue-channel-is \\ -o /workspace/data/camvid-bluechannel-split \\ --split-names train val test \\ --split-ratios 70 15 15","title":"Data"},{"location":"image_segmentation/image-segmentation-keras/#training","text":"For training, we will use the following docker image: waikatodatamining/image-segmentation-keras:1.14.0_0.3.0 The training script is called keras_seg_train , for which we can invoke the help screen as follows: docker run --rm -t waikatodatamining/image-segmentation-keras:1.14.0_0.3.0 keras_seg_train --help It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: camvid12-keras-unet50 Image Segmentation Keras does not use a config file, but can be configured via command-line parameters instead: the number of classes ( --n_classes ) is actual classes plus background the width/height of the inputs must be multiples of 32 ( --input_height , --input_width ) the output directory is specified via --checkpoints_path (make sure to have a trailing slash!) in our case, the images and the annotations reside in the same directories, so --train_images and --train_annotations point to the same directory (analog for --val_images and --val_annotations ) with --epochs you can specify for how long the model will get trained --model_name specifies the architecture and backend (see here for available options) Kick off the training of a U-Net with a ResNet50 backend using the following command: docker run --rm \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/.keras \\ -t waikatodatamining/image-segmentation-keras:1.14.0_0.3.0 \\ keras_seg_train \\ --checkpoints_path /workspace/output/camvid12-keras-unet50/ \\ --train_images /workspace/data/camvid-bluechannel-split/train \\ --train_annotations /workspace/data/camvid-bluechannel-split/train \\ --val_images /workspace/data/camvid-bluechannel-split/val \\ --val_annotations /workspace/data/camvid-bluechannel-split/val \\ --epochs 10 \\ --n_classes 13 \\ --input_height 384 \\ --input_width 480 \\ --model_name resnet50_unet","title":"Training"},{"location":"image_segmentation/image-segmentation-keras/#predicting","text":"Using the keras_seg_poll script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run --rm \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/.keras \\ -t waikatodatamining/image-segmentation-keras:1.14.0_0.3.0 \\ keras_seg_poll \\ --checkpoints_path /workspace/output/camvid12-keras-unet50/ \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Notes The predictions are output in Indexed PNG format. You can view the predictions with the ADAMS Preview browser : Indexed PNG Example prediction","title":"Predicting"},{"location":"image_segmentation/mmsegmentation/","text":"MMSegmentation is a comprehensive and flexible framework for image segmentation that offers a wide variety of architectures. Custom docker images with additional tools are available from here: https://github.com/waikato-datamining/mmsegmentation Prerequisites # Make sure you have the directory structure created as outlined in the Prerequisites . Data # In this example, we will use the CamVid-12 dataset, which consists of still images from dashcam videos from a city environment (12 different labels). Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/camvid12/camvid12-grayscale.zip Once extracted, rename the grayscale directory to camvid-grayscale . Noe we have to convert the format from grayscale into indexed PNG , which MMSegmentation uses. We can do this by using the wai.annotations library. At the same time, we can split the dataset into train , validation and test subsets. From within the applied_deep_learning directory, run the following command: docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:0.8.0 \\ wai-annotations convert \\ from-grayscale-is \\ -i \"/workspace/data/camvid-grayscale/*.png\" \\ --labels sky building pole road pavement tree signsymbol fence car pedestrian bicyclist unlabelled \\ to-indexed-png-is \\ -o /workspace/data/camvid-indexed-split \\ --split-names train val test \\ --split-ratios 70 15 15 Training # For training, we will use the following docker image: waikatodatamining/mmsegmentation:0.30.0_cuda11.1 Training and inference are possible on just a CPU as well (though much, much slower). For utilizing a CPU you can use the following docker image: waikatodatamining/mmsegmentation:0.30.0_cpu The training script is called mmseg_train , for which we can invoke the help screen as follows: docker run --rm -t waikatodatamining/mmsegmentation:0.30.0_cuda11.1 mmseg_train --help It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: camvid12-mmseg-pspnet50 Before we can train, we will need to obtain and customize a config file. Within the container, you can find example configurations for various architectures in the following directory: /mmsegmentation/configs Using the mmseg_config command, we can expand and dump one of these configurations for our own purposes: docker run --rm \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -t waikatodatamining/mmsegmentation:0.30.0_cuda11.1 \\ mmseg_config \\ --config /mmsegmentation/configs/pspnet/pspnet_r50-d8_512x1024_40k_cityscapes.py \\ --output_config /workspace/output/camvid12-mmseg-pspnet50/pspnet_r50.py Open the pspnet_r50.py file in a text editor and perform the following operations: change num_classes to 12 (background not counted). change dataset_type to ExternalDataset and any occurrences of type in the train , test , val sections of the data dictionary. change data_root occurrences to /workspace/data/camvid-indexed-split (the directory above the train and val directories). change img_dir occurrences to img_dir=data_root+'/DIR', with DIR being the appropriate train , val or test do the same with ann_dir Kick off the training with the following command: docker run --rm \\ -u $(id -u):$(id -g) \\ --shm-size=8g \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMSEG_CLASSES=\"sky,building,pole,road,pavement,tree,signsymbol,fence,car,pedestrian,bicyclist,unlabelled\" \\ -t waikatodatamining/mmsegmentation:0.30.0_cuda11.1 \\ mmseg_train \\ /workspace/output/camvid12-mmseg-pspnet50/pspnet_r50.py \\ --work-dir /workspace/output/camvid12-mmseg-pspnet50 Predicting # Using the mmseg_predict_poll script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run --rm \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMSEG_CLASSES=\"sky,building,pole,road,pavement,tree,signsymbol,fence,car,pedestrian,bicyclist,unlabelled\" \\ -t waikatodatamining/mmsegmentation:0.30.0_cuda11.1 \\ mmseg_predict_poll \\ --model /workspace/output/camvid12-mmseg-pspnet50/latest.pth \\ --config /workspace/output/camvid12-mmseg-pspnet50/pspnet_r50.py \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Notes The predictions can either be output in grayscale (default) or bluechannel format ( --prediction_format ). You can view the predictions with the ADAMS Preview browser : Indexed PNG Example prediction","title":"MMSegmentation"},{"location":"image_segmentation/mmsegmentation/#prerequisites","text":"Make sure you have the directory structure created as outlined in the Prerequisites .","title":"Prerequisites"},{"location":"image_segmentation/mmsegmentation/#data","text":"In this example, we will use the CamVid-12 dataset, which consists of still images from dashcam videos from a city environment (12 different labels). Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/camvid12/camvid12-grayscale.zip Once extracted, rename the grayscale directory to camvid-grayscale . Noe we have to convert the format from grayscale into indexed PNG , which MMSegmentation uses. We can do this by using the wai.annotations library. At the same time, we can split the dataset into train , validation and test subsets. From within the applied_deep_learning directory, run the following command: docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:0.8.0 \\ wai-annotations convert \\ from-grayscale-is \\ -i \"/workspace/data/camvid-grayscale/*.png\" \\ --labels sky building pole road pavement tree signsymbol fence car pedestrian bicyclist unlabelled \\ to-indexed-png-is \\ -o /workspace/data/camvid-indexed-split \\ --split-names train val test \\ --split-ratios 70 15 15","title":"Data"},{"location":"image_segmentation/mmsegmentation/#training","text":"For training, we will use the following docker image: waikatodatamining/mmsegmentation:0.30.0_cuda11.1 Training and inference are possible on just a CPU as well (though much, much slower). For utilizing a CPU you can use the following docker image: waikatodatamining/mmsegmentation:0.30.0_cpu The training script is called mmseg_train , for which we can invoke the help screen as follows: docker run --rm -t waikatodatamining/mmsegmentation:0.30.0_cuda11.1 mmseg_train --help It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: camvid12-mmseg-pspnet50 Before we can train, we will need to obtain and customize a config file. Within the container, you can find example configurations for various architectures in the following directory: /mmsegmentation/configs Using the mmseg_config command, we can expand and dump one of these configurations for our own purposes: docker run --rm \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -t waikatodatamining/mmsegmentation:0.30.0_cuda11.1 \\ mmseg_config \\ --config /mmsegmentation/configs/pspnet/pspnet_r50-d8_512x1024_40k_cityscapes.py \\ --output_config /workspace/output/camvid12-mmseg-pspnet50/pspnet_r50.py Open the pspnet_r50.py file in a text editor and perform the following operations: change num_classes to 12 (background not counted). change dataset_type to ExternalDataset and any occurrences of type in the train , test , val sections of the data dictionary. change data_root occurrences to /workspace/data/camvid-indexed-split (the directory above the train and val directories). change img_dir occurrences to img_dir=data_root+'/DIR', with DIR being the appropriate train , val or test do the same with ann_dir Kick off the training with the following command: docker run --rm \\ -u $(id -u):$(id -g) \\ --shm-size=8g \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMSEG_CLASSES=\"sky,building,pole,road,pavement,tree,signsymbol,fence,car,pedestrian,bicyclist,unlabelled\" \\ -t waikatodatamining/mmsegmentation:0.30.0_cuda11.1 \\ mmseg_train \\ /workspace/output/camvid12-mmseg-pspnet50/pspnet_r50.py \\ --work-dir /workspace/output/camvid12-mmseg-pspnet50","title":"Training"},{"location":"image_segmentation/mmsegmentation/#predicting","text":"Using the mmseg_predict_poll script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run --rm \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMSEG_CLASSES=\"sky,building,pole,road,pavement,tree,signsymbol,fence,car,pedestrian,bicyclist,unlabelled\" \\ -t waikatodatamining/mmsegmentation:0.30.0_cuda11.1 \\ mmseg_predict_poll \\ --model /workspace/output/camvid12-mmseg-pspnet50/latest.pth \\ --config /workspace/output/camvid12-mmseg-pspnet50/pspnet_r50.py \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Notes The predictions can either be output in grayscale (default) or bluechannel format ( --prediction_format ). You can view the predictions with the ADAMS Preview browser : Indexed PNG Example prediction","title":"Predicting"},{"location":"instance_segmentation/","text":"Instance segmentation not only finds objects within images, it also learns the shape of the objects rather than just a simple rectangle/bounding box. However, this makes the algorithms computationally more expensive and more memory hungry. What to do next: Annotate your data Choose a framework","title":"Introduction"},{"location":"instance_segmentation/annotate/","text":"You can use the ADAMS framework for annotating your images. ADAMS comes pre-bundled in various setups and we need the adams-annotator bundle. You can get either: snapshot release (21.12.0 or later) In the Flow editor (available from the Tools menu in ADAMS), you can load and execute the follow workflow (which is part of your ADAMS installation when downloading it as zip file): adams-imaging-annotate_objects.flow When prompted, use object_shape as Selection type . The following video takes you through the process:","title":"Annotate"},{"location":"instance_segmentation/detectron2/","text":"Detectron2 is a platform for object detection, segmentation and other visual recognition tasks. Custom docker images for instance segmentation with additional tools are available from here: https://github.com/waikato-datamining/pytorch/tree/master/detectron2 Prerequisites # Make sure you have the directory structure created as outlined in the Prerequisites . Data # In this example, we will use the Oxford Pets dataset, which consists of 37 different categories of cats and dogs. Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/oxford-pets/oxford-pets-adams.zip Rename the adams directory to pets-adams . To speed up training, we only use two labels: cat:abyssinian and dog:yorkshire_terrier . The label filtering and splitting it into train , validation and test subsets is done using wai.annotations : docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:0.8.0 \\ wai-annotations convert \\ from-adams-od \\ -i \"/workspace/data/pets-adams/*.report\" \\ filter-labels \\ --labels cat:abyssinian dog:yorkshire_terrier \\ discard-negatives \\ coerce-mask \\ to-coco-od \\ -o /workspace/data/pets2-coco-split/annotations.json \\ --sort-categories \\ --category-output-file labels.txt \\ --split-names train val test \\ --split-ratios 70 15 15 Training # For training, we will use the following docker image: waikatodatamining/detectron2:0.6 The training script is called d2_train_coco , for which we can invoke the help screen as follows: docker run --rm -t waikatodatamining/detectron2:0.6 d2_train_coco --help It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: pets2-d2-maskrcnn Before we can train, we will need to obtain and customize a config file. Within the container, you can find example configurations for various architectures in the following directory: /opt/detectron2/configs/COCO-InstanceSegmentation Using the d2_dump_config command, we can expand and dump one of these configurations for our own purposes: docker run --rm \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -t waikatodatamining/detectron2:0.6 \\ d2_dump_config \\ --config_in /opt/detectron2/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml \\ --num_classes 2 \\ --output_dir /workspace/output/pets2-d2-maskrcnn \\ --config_out /workspace/output/pets2-d2-maskrcnn/mask_rcnn_R_50_FPN_1x.yaml Edit the mask_rcnn_R_50_FPN_1x.yaml file and change the following values ( IMS_PER_BATCH is reduced to fit in your GPU's memory, the others to limit training time): SOLVER: IMS_PER_BATCH: 4 MAX_ITER: 15000 STEPS: - 5000 - 10000 Kick off the training with the following command: docker run --rm \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -v `pwd`/cache/iopath_cache:/tmp/iopath_cache \\ -t waikatodatamining/detectron2:0.6 \\ d2_train_coco \\ --config /workspace/output/pets2-d2-maskrcnn/mask_rcnn_R_50_FPN_1x.yaml \\ --train_annotations /workspace/data/pets2-coco-split/train/annotations.json \\ --train_images /workspace/data/pets2-coco-split/train/ \\ --test_annotations /workspace/data/pets2-coco-split/val/annotations.json \\ --test_images /workspace/data/pets2-coco-split/val/ \\ --labels /workspace/data/pets2-coco-split/train/labels.txt \\ --output_dir /workspace/output/pets2-d2-maskrcnn Predicting # Using the d2_predict script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run --rm \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -v `pwd`/cache/iopath_cache:/tmp/iopath_cache \\ -t waikatodatamining/detectron2:0.6 \\ d2_predict \\ --model /workspace/output/pets2-d2-maskrcnn/model_final.pth \\ --config /workspace/output/pets2-d2-maskrcnn/mask_rcnn_R_50_FPN_1x.yaml \\ --labels /workspace/data/pets2-coco-split/train/labels.txt \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Notes By default, the predictions get output in ROI CSV format . But you can also output them in the OPEX JSON format by adding --prediction_format opex --prediction_suffix .json to the command. You can view the predictions with the ADAMS Preview browser : ROIS CSV OPEX Example prediction","title":"Detectron2"},{"location":"instance_segmentation/detectron2/#prerequisites","text":"Make sure you have the directory structure created as outlined in the Prerequisites .","title":"Prerequisites"},{"location":"instance_segmentation/detectron2/#data","text":"In this example, we will use the Oxford Pets dataset, which consists of 37 different categories of cats and dogs. Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/oxford-pets/oxford-pets-adams.zip Rename the adams directory to pets-adams . To speed up training, we only use two labels: cat:abyssinian and dog:yorkshire_terrier . The label filtering and splitting it into train , validation and test subsets is done using wai.annotations : docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:0.8.0 \\ wai-annotations convert \\ from-adams-od \\ -i \"/workspace/data/pets-adams/*.report\" \\ filter-labels \\ --labels cat:abyssinian dog:yorkshire_terrier \\ discard-negatives \\ coerce-mask \\ to-coco-od \\ -o /workspace/data/pets2-coco-split/annotations.json \\ --sort-categories \\ --category-output-file labels.txt \\ --split-names train val test \\ --split-ratios 70 15 15","title":"Data"},{"location":"instance_segmentation/detectron2/#training","text":"For training, we will use the following docker image: waikatodatamining/detectron2:0.6 The training script is called d2_train_coco , for which we can invoke the help screen as follows: docker run --rm -t waikatodatamining/detectron2:0.6 d2_train_coco --help It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: pets2-d2-maskrcnn Before we can train, we will need to obtain and customize a config file. Within the container, you can find example configurations for various architectures in the following directory: /opt/detectron2/configs/COCO-InstanceSegmentation Using the d2_dump_config command, we can expand and dump one of these configurations for our own purposes: docker run --rm \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -t waikatodatamining/detectron2:0.6 \\ d2_dump_config \\ --config_in /opt/detectron2/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml \\ --num_classes 2 \\ --output_dir /workspace/output/pets2-d2-maskrcnn \\ --config_out /workspace/output/pets2-d2-maskrcnn/mask_rcnn_R_50_FPN_1x.yaml Edit the mask_rcnn_R_50_FPN_1x.yaml file and change the following values ( IMS_PER_BATCH is reduced to fit in your GPU's memory, the others to limit training time): SOLVER: IMS_PER_BATCH: 4 MAX_ITER: 15000 STEPS: - 5000 - 10000 Kick off the training with the following command: docker run --rm \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -v `pwd`/cache/iopath_cache:/tmp/iopath_cache \\ -t waikatodatamining/detectron2:0.6 \\ d2_train_coco \\ --config /workspace/output/pets2-d2-maskrcnn/mask_rcnn_R_50_FPN_1x.yaml \\ --train_annotations /workspace/data/pets2-coco-split/train/annotations.json \\ --train_images /workspace/data/pets2-coco-split/train/ \\ --test_annotations /workspace/data/pets2-coco-split/val/annotations.json \\ --test_images /workspace/data/pets2-coco-split/val/ \\ --labels /workspace/data/pets2-coco-split/train/labels.txt \\ --output_dir /workspace/output/pets2-d2-maskrcnn","title":"Training"},{"location":"instance_segmentation/detectron2/#predicting","text":"Using the d2_predict script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run --rm \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -v `pwd`/cache/iopath_cache:/tmp/iopath_cache \\ -t waikatodatamining/detectron2:0.6 \\ d2_predict \\ --model /workspace/output/pets2-d2-maskrcnn/model_final.pth \\ --config /workspace/output/pets2-d2-maskrcnn/mask_rcnn_R_50_FPN_1x.yaml \\ --labels /workspace/data/pets2-coco-split/train/labels.txt \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Notes By default, the predictions get output in ROI CSV format . But you can also output them in the OPEX JSON format by adding --prediction_format opex --prediction_suffix .json to the command. You can view the predictions with the ADAMS Preview browser : ROIS CSV OPEX Example prediction","title":"Predicting"},{"location":"instance_segmentation/frameworks/","text":"The following frameworks are available for instance segmentation : Detectron2 MMDetection Yolov5","title":"Frameworks"},{"location":"instance_segmentation/mmdetection/","text":"MMDetection is a comprehensive and flexible framework not only for object detection, but also for instance segmentation. Custom docker images with additional tools are available from here: https://github.com/waikato-datamining/mmdetection Prerequisites # Make sure you have the directory structure created as outlined in the Prerequisites . Data # In this example, we will use the Oxford Pets dataset, which consists of 37 different categories of cats and dogs. Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/oxford-pets/oxford-pets-adams.zip Rename the adams directory to pets-adams . To speed up training, we only use two labels: cat:abyssinian and dog:yorkshire_terrier . The label filtering and splitting it into train , validation and test subsets is done using wai.annotations : docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:0.8.0 \\ wai-annotations convert \\ from-adams-od \\ -i \"/workspace/data/pets-adams/*.report\" \\ filter-labels \\ --labels cat:abyssinian dog:yorkshire_terrier \\ discard-negatives \\ coerce-mask \\ to-coco-od \\ -o /workspace/data/pets2-coco-split/annotations.json \\ --sort-categories \\ --category-output-file labels.txt \\ --split-names train val test \\ --split-ratios 70 15 15 Training # For training, we will use the following docker image: waikatodatamining/mmdetection:2.27.0_cuda11.1 The training script is called mmdet_train , for which we can invoke the help screen as follows (unfortunately, we need to set the MMDET_CLASSES environment variable to avoid an exception): docker run --rm \\ -e MMDET_CLASSES= \\ -t waikatodatamining/mmdetection:2.27.0_cuda11.1 \\ mmdet_train --help It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: pets2-mmdet-maskrcnn Before we can train, we will need to obtain and customize a config file. Within the container, you can find example configurations for various architectures in the following directory: /mmdetection/configs Using the mmdet_config command, we can expand and dump one of these configurations for our own purposes: docker run --rm \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -t waikatodatamining/mmdetection:2.27.0_cuda11.1 \\ mmdet_config \\ /mmdetection/configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py \\ > output/pets2-mmdet-maskrcnn/mask_rcnn_r50_fpn_1x_coco.py Open the mask_rcnn_r50_fpn_1x_coco.py file in a text editor and perform the following operations: remove any lines before model = dict( change all occurrences of num_classes to 2 change dataset_type to Dataset and any occurrences of type in the train , test , val sections of the data dictionary change data_root occurrences to /workspace/data/pets2-coco-split (the directory above the train and val directories) change img_prefix occurrences to img_prefix=data_root+'/DIR', with DIR being the appropriate train , val or test change ann_file occurrences to ann_file=data_root+'/DIR/annotations.json', with DIR being the appropriate train , val or test change max_epochs in runner to an appropriate value, e.g., 50 change interval in checkpoint_config to a higher value, e.g., 5 change lr (learning rate) in optimizer to 0.002 to avoid NaNs with a learning rate that is too high Kick off the training with the following command: docker run --rm \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMDET_CLASSES=/workspace/data/pets2-coco-split/train/labels.txt \\ -t waikatodatamining/mmdetection:2.27.0_cuda11.1 \\ mmdet_train \\ /workspace/output/pets2-mmdet-maskrcnn/mask_rcnn_r50_fpn_1x_coco.py \\ --work-dir /workspace/output/pets2-mmdet-maskrcnn Predicting # Using the mmdet_predict script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run --rm \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMDET_CLASSES=/workspace/data/pets2-coco-split/train/labels.txt \\ -t waikatodatamining/mmdetection:2.27.0_cuda11.1 \\ mmdet_predict \\ --checkpoint /workspace/output/pets2-mmdet-maskrcnn/latest.pth \\ --config /workspace/output/pets2-mmdet-maskrcnn/mask_rcnn_r50_fpn_1x_coco.py \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Notes By default, the predictions get output in ROI CSV format . But you can also output them in the OPEX JSON format by adding --prediction_format opex --prediction_suffix .json to the command. You can view the predictions with the ADAMS Preview browser : ROIS CSV OPEX Example prediction","title":"MMDetection (instance segmentation)"},{"location":"instance_segmentation/mmdetection/#prerequisites","text":"Make sure you have the directory structure created as outlined in the Prerequisites .","title":"Prerequisites"},{"location":"instance_segmentation/mmdetection/#data","text":"In this example, we will use the Oxford Pets dataset, which consists of 37 different categories of cats and dogs. Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/oxford-pets/oxford-pets-adams.zip Rename the adams directory to pets-adams . To speed up training, we only use two labels: cat:abyssinian and dog:yorkshire_terrier . The label filtering and splitting it into train , validation and test subsets is done using wai.annotations : docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:0.8.0 \\ wai-annotations convert \\ from-adams-od \\ -i \"/workspace/data/pets-adams/*.report\" \\ filter-labels \\ --labels cat:abyssinian dog:yorkshire_terrier \\ discard-negatives \\ coerce-mask \\ to-coco-od \\ -o /workspace/data/pets2-coco-split/annotations.json \\ --sort-categories \\ --category-output-file labels.txt \\ --split-names train val test \\ --split-ratios 70 15 15","title":"Data"},{"location":"instance_segmentation/mmdetection/#training","text":"For training, we will use the following docker image: waikatodatamining/mmdetection:2.27.0_cuda11.1 The training script is called mmdet_train , for which we can invoke the help screen as follows (unfortunately, we need to set the MMDET_CLASSES environment variable to avoid an exception): docker run --rm \\ -e MMDET_CLASSES= \\ -t waikatodatamining/mmdetection:2.27.0_cuda11.1 \\ mmdet_train --help It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: pets2-mmdet-maskrcnn Before we can train, we will need to obtain and customize a config file. Within the container, you can find example configurations for various architectures in the following directory: /mmdetection/configs Using the mmdet_config command, we can expand and dump one of these configurations for our own purposes: docker run --rm \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -t waikatodatamining/mmdetection:2.27.0_cuda11.1 \\ mmdet_config \\ /mmdetection/configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py \\ > output/pets2-mmdet-maskrcnn/mask_rcnn_r50_fpn_1x_coco.py Open the mask_rcnn_r50_fpn_1x_coco.py file in a text editor and perform the following operations: remove any lines before model = dict( change all occurrences of num_classes to 2 change dataset_type to Dataset and any occurrences of type in the train , test , val sections of the data dictionary change data_root occurrences to /workspace/data/pets2-coco-split (the directory above the train and val directories) change img_prefix occurrences to img_prefix=data_root+'/DIR', with DIR being the appropriate train , val or test change ann_file occurrences to ann_file=data_root+'/DIR/annotations.json', with DIR being the appropriate train , val or test change max_epochs in runner to an appropriate value, e.g., 50 change interval in checkpoint_config to a higher value, e.g., 5 change lr (learning rate) in optimizer to 0.002 to avoid NaNs with a learning rate that is too high Kick off the training with the following command: docker run --rm \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMDET_CLASSES=/workspace/data/pets2-coco-split/train/labels.txt \\ -t waikatodatamining/mmdetection:2.27.0_cuda11.1 \\ mmdet_train \\ /workspace/output/pets2-mmdet-maskrcnn/mask_rcnn_r50_fpn_1x_coco.py \\ --work-dir /workspace/output/pets2-mmdet-maskrcnn","title":"Training"},{"location":"instance_segmentation/mmdetection/#predicting","text":"Using the mmdet_predict script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run --rm \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMDET_CLASSES=/workspace/data/pets2-coco-split/train/labels.txt \\ -t waikatodatamining/mmdetection:2.27.0_cuda11.1 \\ mmdet_predict \\ --checkpoint /workspace/output/pets2-mmdet-maskrcnn/latest.pth \\ --config /workspace/output/pets2-mmdet-maskrcnn/mask_rcnn_r50_fpn_1x_coco.py \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Notes By default, the predictions get output in ROI CSV format . But you can also output them in the OPEX JSON format by adding --prediction_format opex --prediction_suffix .json to the command. You can view the predictions with the ADAMS Preview browser : ROIS CSV OPEX Example prediction","title":"Predicting"},{"location":"instance_segmentation/yolov5/","text":"Yolov5 is a family of object detection architectures and models pretrained on the COCO dataset, and represents Ultralytics open-source research into future vision AI methods. Later releases also added support for instance segmentation. Custom docker images with additional tools are available from here: https://github.com/waikato-datamining/pytorch/tree/master/yolov5 Prerequisites # Make sure you have the directory structure created as outlined in the Prerequisites . Data # In this example, we will use the Oxford Pets dataset, which consists of 37 different categories of cats and dogs. Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/oxford-pets/oxford-pets-adams.zip Rename the adams directory to pets-adams . To speed up training, we only use two labels: cat:abyssinian and dog:yorkshire_terrier . The label filtering and splitting it into train , validation and test subsets is done using wai.annotations : docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:0.8.0 \\ wai-annotations convert \\ from-adams-od \\ -i \"/workspace/data/pets-adams/*.report\" \\ filter-labels \\ --labels cat:abyssinian dog:yorkshire_terrier \\ discard-negatives \\ coerce-mask \\ to-yolo-od \\ -o /workspace/data/pets-yolo-split \\ --labels /workspace/data/pets-yolo-split/labels.txt \\ --labels-csv /workspace/data/pets-yolo-split/labels.csv \\ --use-polygon-format \\ --split-names train val test \\ --split-ratios 70 15 15 NB: At the time of writing, the yolo plugin for wai.annotations still had a bug which creates empty top-level directories when splitting datasets via --split-names . In the pets-yolo-split directory, you can safely remove the train , test and val directories, since the actual splits are below the images and labels directories: rm -fR data/pets-yolo-split/train data/pets-yolo-split/test data/pets-yolo-split/val Finally, download the dataset.yaml file and place it in the pets-yolo-split directory. It contains information about the dataset directory, the splits and the class labels. Since the labels can come out in a random order, you need to update the labels in the yaml file with the ones that got output in the labels.txt file. You can automatically quote the comma-separated list using the following command: cat data/pets-yolo-split/labels.txt | sed s/,/\\',\\'/g | sed s/^/\\'/g | sed s/$/\\'/g Training # For training, we will use the following docker image: waikatodatamining/pytorch-yolov5:2022-11-05_cuda11.1 If you do not have a GPU, you can use the CPU-only image: waikatodatamining/pytorch-yolov5:2022-11-05_cpu The training script is called yolov5_train , for which we can invoke the help screen as follows: docker run --rm -t waikatodatamining/pytorch-yolov5:2022-11-05_cuda11.1 yolov5_train --help Instead of using config files, we can just tweak parameters via command-line options. However, we still need to download a base model to use for training. Yolov5 offers different models, which differ in speed and accuracy. We will use the medium one called yolov5m-seg.pt from the v6.2 release: https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5m-seg.pt Download it and place it in the models directory. It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: pets-yolov5 Since the image size should be a multiple of 32, we use 416 for this experiment. Download the yolov5m-seg.yaml parameter file into the output/pets-yolov5 folder: https://github.com/ultralytics/yolov5/blob/master/models/segment/yolov5m-seg.yaml Once downloaded, change the number of classes ( nc ) to 2. Kick off the training with the following command: docker run --rm \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/pytorch-yolov5:2022-11-05_cuda11.1 \\ yolov5seg_train \\ --img 640 \\ --workers 1 \\ --device 0 \\ --batch 16 \\ --epochs 10 \\ --cfg /workspace/output/pets-yolov5/yolov5m-seg.yaml \\ --data /workspace/data/pets-yolo-split/dataset.yaml \\ --weights /workspace/models/yolov5m-seg.pt \\ --project /workspace/output/ \\ --name pets-yolov5 \\ --exist-ok Predicting # Using the yolov5_predict_poll script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run --rm \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/pytorch-yolov5:2022-11-05_cuda11.1 \\ yolov5seg_predict_poll \\ --model /workspace/output/pets-yolov5/weights/best.pt \\ --data /workspace/data/pets-yolo-split/dataset.yaml \\ --image_size 416 \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Notes By default, the predictions get output in ROI CSV format . But you can also output them in the OPEX JSON format by adding --prediction_format opex --prediction_suffix .json to the command. You can view the predictions with the ADAMS Preview browser : ROIS CSV OPEX Example prediction Troubleshooting # If you are re-using a dataset that was used by another YolovX framework, you may get strange error messages when reading the data. This can be due to incompatible cache files that get generated to speed up loading the data. Make sure to remove all files in the labels directory that have a .cache extension.","title":"Yolov5"},{"location":"instance_segmentation/yolov5/#prerequisites","text":"Make sure you have the directory structure created as outlined in the Prerequisites .","title":"Prerequisites"},{"location":"instance_segmentation/yolov5/#data","text":"In this example, we will use the Oxford Pets dataset, which consists of 37 different categories of cats and dogs. Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/oxford-pets/oxford-pets-adams.zip Rename the adams directory to pets-adams . To speed up training, we only use two labels: cat:abyssinian and dog:yorkshire_terrier . The label filtering and splitting it into train , validation and test subsets is done using wai.annotations : docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:0.8.0 \\ wai-annotations convert \\ from-adams-od \\ -i \"/workspace/data/pets-adams/*.report\" \\ filter-labels \\ --labels cat:abyssinian dog:yorkshire_terrier \\ discard-negatives \\ coerce-mask \\ to-yolo-od \\ -o /workspace/data/pets-yolo-split \\ --labels /workspace/data/pets-yolo-split/labels.txt \\ --labels-csv /workspace/data/pets-yolo-split/labels.csv \\ --use-polygon-format \\ --split-names train val test \\ --split-ratios 70 15 15 NB: At the time of writing, the yolo plugin for wai.annotations still had a bug which creates empty top-level directories when splitting datasets via --split-names . In the pets-yolo-split directory, you can safely remove the train , test and val directories, since the actual splits are below the images and labels directories: rm -fR data/pets-yolo-split/train data/pets-yolo-split/test data/pets-yolo-split/val Finally, download the dataset.yaml file and place it in the pets-yolo-split directory. It contains information about the dataset directory, the splits and the class labels. Since the labels can come out in a random order, you need to update the labels in the yaml file with the ones that got output in the labels.txt file. You can automatically quote the comma-separated list using the following command: cat data/pets-yolo-split/labels.txt | sed s/,/\\',\\'/g | sed s/^/\\'/g | sed s/$/\\'/g","title":"Data"},{"location":"instance_segmentation/yolov5/#training","text":"For training, we will use the following docker image: waikatodatamining/pytorch-yolov5:2022-11-05_cuda11.1 If you do not have a GPU, you can use the CPU-only image: waikatodatamining/pytorch-yolov5:2022-11-05_cpu The training script is called yolov5_train , for which we can invoke the help screen as follows: docker run --rm -t waikatodatamining/pytorch-yolov5:2022-11-05_cuda11.1 yolov5_train --help Instead of using config files, we can just tweak parameters via command-line options. However, we still need to download a base model to use for training. Yolov5 offers different models, which differ in speed and accuracy. We will use the medium one called yolov5m-seg.pt from the v6.2 release: https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5m-seg.pt Download it and place it in the models directory. It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: pets-yolov5 Since the image size should be a multiple of 32, we use 416 for this experiment. Download the yolov5m-seg.yaml parameter file into the output/pets-yolov5 folder: https://github.com/ultralytics/yolov5/blob/master/models/segment/yolov5m-seg.yaml Once downloaded, change the number of classes ( nc ) to 2. Kick off the training with the following command: docker run --rm \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/pytorch-yolov5:2022-11-05_cuda11.1 \\ yolov5seg_train \\ --img 640 \\ --workers 1 \\ --device 0 \\ --batch 16 \\ --epochs 10 \\ --cfg /workspace/output/pets-yolov5/yolov5m-seg.yaml \\ --data /workspace/data/pets-yolo-split/dataset.yaml \\ --weights /workspace/models/yolov5m-seg.pt \\ --project /workspace/output/ \\ --name pets-yolov5 \\ --exist-ok","title":"Training"},{"location":"instance_segmentation/yolov5/#predicting","text":"Using the yolov5_predict_poll script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run --rm \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/pytorch-yolov5:2022-11-05_cuda11.1 \\ yolov5seg_predict_poll \\ --model /workspace/output/pets-yolov5/weights/best.pt \\ --data /workspace/data/pets-yolo-split/dataset.yaml \\ --image_size 416 \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Notes By default, the predictions get output in ROI CSV format . But you can also output them in the OPEX JSON format by adding --prediction_format opex --prediction_suffix .json to the command. You can view the predictions with the ADAMS Preview browser : ROIS CSV OPEX Example prediction","title":"Predicting"},{"location":"instance_segmentation/yolov5/#troubleshooting","text":"If you are re-using a dataset that was used by another YolovX framework, you may get strange error messages when reading the data. This can be due to incompatible cache files that get generated to speed up loading the data. Make sure to remove all files in the labels directory that have a .cache extension.","title":"Troubleshooting"},{"location":"object_detection/","text":"Object detection detects one or more objects within an image and assigns each one of them a label, as opposed to image classification which assigns a lable to the whole image. The predicted locations are typically rectangles (aka bounding boxes). If you are looking for shapes or polygons, then have a look at instance segmentation . What to do next: Annotate your data Choose a framework","title":"Introduction"},{"location":"object_detection/annotate/","text":"You can use the ADAMS framework for annotating your images. ADAMS comes pre-bundled in various setups and we need the adams-annotator bundle. You can get either: snapshot release (21.12.0 or later) In the Flow editor (available from the Tools menu in ADAMS), you can load and execute the follow workflow (which is part of your ADAMS installation when downloading it as zip file): adams-imaging-annotate_objects.flow When prompted, use bounding_box as Selection type . The following video takes you through the process:","title":"Annotate"},{"location":"object_detection/frameworks/","text":"The following frameworks are available for object detection : MMDetection Yolov5 Yolov7","title":"Frameworks"},{"location":"object_detection/mmdetection/","text":"MMDetection is a comprehensive and flexible framework for object detection that offers a wide variety of architectures. Custom docker images with additional tools are available from here: https://github.com/waikato-datamining/mmdetection Prerequisites # Make sure you have the directory structure created as outlined in the Prerequisites . Data # In this example, we will use the American Sign Language Letters dataset, which consists of sets of images of hands, one per letter in the English alphabet (26 labels). Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/american-sign-language-letters/american-sign-language-letters-voc.zip Once extracted, rename the voc directory to sign-voc . Now we have to convert the format from VOC XML into MS COCO . We can do this by using the wai.annotations library. At the same time, we can split the dataset into train , validation and test subsets. From within the applied_deep_learning directory, run the following command: docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:0.8.0 \\ wai-annotations convert \\ from-voc-od \\ -i \"/workspace/data/sign-voc/*.xml\" \\ to-coco-od \\ -o /workspace/data/sign-coco-split/annotations.json \\ --sort-categories \\ --category-output-file labels.txt \\ --split-names train val test \\ --split-ratios 70 15 15 Training # For training, we will use the following docker image: waikatodatamining/mmdetection:2.27.0_cuda11.1 Inference is possible without a GPU as well (though much, much slower). For utilizing a CPU you can use the following docker image: waikatodatamining/mmdetection:2.27.0_cpu The training script is called mmdet_train , for which we can invoke the help screen as follows (unfortunately, we need to set the MMDET_CLASSES environment variable to avoid an exception): docker run --rm \\ -e MMDET_CLASSES= \\ -t waikatodatamining/mmdetection:2.27.0_cuda11.1 \\ mmdet_train --help It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: sign-mmdet-fr50 Before we can train, we will need to obtain and customize a config file. Within the container, you can find example configurations for various architectures in the following directory: /mmdetection/configs Using the mmdet_config command, we can expand and dump one of these configurations for our own purposes: docker run --rm \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -t waikatodatamining/mmdetection:2.27.0_cuda11.1 \\ mmdet_config \\ /mmdetection/configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \\ > output/sign-mmdet-fr50/faster_rcnn_r50_fpn_1x_coco.py Open the faster_rcnn_r50_fpn_1x_coco.py file in a text editor and perform the following operations: remove any lines before model = dict( change num_classes to 26 change dataset_type to Dataset and any occurrences of type in the train , test , val sections of the data dictionary change data_root occurrences to /workspace/data/sign-coco-split (the directory above the train and val directories) change img_prefix occurrences to img_prefix=data_root+'/DIR', with DIR being the appropriate train , val or test change ann_file occurrences to ann_file=data_root+'/DIR/annotations.json', with DIR being the appropriate train , val or test change max_epochs in runner to an appropriate value, e.g., 5 change interval in checkpoint_config to a higher value, e.g., 5 Kick off the training with the following command: docker run --rm \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMDET_CLASSES=/workspace/data/sign-coco-split/train/labels.txt \\ -t waikatodatamining/mmdetection:2.27.0_cuda11.1 \\ mmdet_train \\ /workspace/output/sign-mmdet-fr50/faster_rcnn_r50_fpn_1x_coco.py \\ --work-dir /workspace/output/sign-mmdet-fr50 Predicting # Using the mmdet_predict script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run --rm \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMDET_CLASSES=/workspace/data/sign-coco-split/train/labels.txt \\ -t waikatodatamining/mmdetection:2.27.0_cuda11.1 \\ mmdet_predict \\ --checkpoint /workspace/output/sign-mmdet-fr50/latest.pth \\ --config /workspace/output/sign-mmdet-fr50/faster_rcnn_r50_fpn_1x_coco.py \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Notes By default, the predictions get output in ROI CSV format . But you can also output them in the OPEX JSON format by adding --prediction_format opex --prediction_suffix .json to the command. You can view the predictions with the ADAMS Preview browser : ROIS CSV OPEX Example prediction","title":"MMDetection (object detection)"},{"location":"object_detection/mmdetection/#prerequisites","text":"Make sure you have the directory structure created as outlined in the Prerequisites .","title":"Prerequisites"},{"location":"object_detection/mmdetection/#data","text":"In this example, we will use the American Sign Language Letters dataset, which consists of sets of images of hands, one per letter in the English alphabet (26 labels). Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/american-sign-language-letters/american-sign-language-letters-voc.zip Once extracted, rename the voc directory to sign-voc . Now we have to convert the format from VOC XML into MS COCO . We can do this by using the wai.annotations library. At the same time, we can split the dataset into train , validation and test subsets. From within the applied_deep_learning directory, run the following command: docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:0.8.0 \\ wai-annotations convert \\ from-voc-od \\ -i \"/workspace/data/sign-voc/*.xml\" \\ to-coco-od \\ -o /workspace/data/sign-coco-split/annotations.json \\ --sort-categories \\ --category-output-file labels.txt \\ --split-names train val test \\ --split-ratios 70 15 15","title":"Data"},{"location":"object_detection/mmdetection/#training","text":"For training, we will use the following docker image: waikatodatamining/mmdetection:2.27.0_cuda11.1 Inference is possible without a GPU as well (though much, much slower). For utilizing a CPU you can use the following docker image: waikatodatamining/mmdetection:2.27.0_cpu The training script is called mmdet_train , for which we can invoke the help screen as follows (unfortunately, we need to set the MMDET_CLASSES environment variable to avoid an exception): docker run --rm \\ -e MMDET_CLASSES= \\ -t waikatodatamining/mmdetection:2.27.0_cuda11.1 \\ mmdet_train --help It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: sign-mmdet-fr50 Before we can train, we will need to obtain and customize a config file. Within the container, you can find example configurations for various architectures in the following directory: /mmdetection/configs Using the mmdet_config command, we can expand and dump one of these configurations for our own purposes: docker run --rm \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -t waikatodatamining/mmdetection:2.27.0_cuda11.1 \\ mmdet_config \\ /mmdetection/configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \\ > output/sign-mmdet-fr50/faster_rcnn_r50_fpn_1x_coco.py Open the faster_rcnn_r50_fpn_1x_coco.py file in a text editor and perform the following operations: remove any lines before model = dict( change num_classes to 26 change dataset_type to Dataset and any occurrences of type in the train , test , val sections of the data dictionary change data_root occurrences to /workspace/data/sign-coco-split (the directory above the train and val directories) change img_prefix occurrences to img_prefix=data_root+'/DIR', with DIR being the appropriate train , val or test change ann_file occurrences to ann_file=data_root+'/DIR/annotations.json', with DIR being the appropriate train , val or test change max_epochs in runner to an appropriate value, e.g., 5 change interval in checkpoint_config to a higher value, e.g., 5 Kick off the training with the following command: docker run --rm \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMDET_CLASSES=/workspace/data/sign-coco-split/train/labels.txt \\ -t waikatodatamining/mmdetection:2.27.0_cuda11.1 \\ mmdet_train \\ /workspace/output/sign-mmdet-fr50/faster_rcnn_r50_fpn_1x_coco.py \\ --work-dir /workspace/output/sign-mmdet-fr50","title":"Training"},{"location":"object_detection/mmdetection/#predicting","text":"Using the mmdet_predict script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run --rm \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMDET_CLASSES=/workspace/data/sign-coco-split/train/labels.txt \\ -t waikatodatamining/mmdetection:2.27.0_cuda11.1 \\ mmdet_predict \\ --checkpoint /workspace/output/sign-mmdet-fr50/latest.pth \\ --config /workspace/output/sign-mmdet-fr50/faster_rcnn_r50_fpn_1x_coco.py \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Notes By default, the predictions get output in ROI CSV format . But you can also output them in the OPEX JSON format by adding --prediction_format opex --prediction_suffix .json to the command. You can view the predictions with the ADAMS Preview browser : ROIS CSV OPEX Example prediction","title":"Predicting"},{"location":"object_detection/yolov5/","text":"Yolov5 is a family of object detection architectures and models pretrained on the COCO dataset, and represents Ultralytics open-source research into future vision AI methods. Custom docker images with additional tools are available from here: https://github.com/waikato-datamining/pytorch/tree/master/yolov5 Prerequisites # Make sure you have the directory structure created as outlined in the Prerequisites . Data # In this example, we will use the American Sign Language Letters dataset, which consists of sets of images of hands, one per letter in the English alphabet (26 labels). Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/american-sign-language-letters/american-sign-language-letters-voc.zip Once extracted, rename the voc directory to sign-voc . Now we have to convert the format from VOC XML into YOLO . We can do this by using the wai.annotations library. At the same time, we can split the dataset into train , validation and test subsets. From within the applied_deep_learning directory, run the following command: docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:0.8.0 \\ wai-annotations convert \\ from-voc-od \\ -i \"/workspace/data/sign-voc/*.xml\" \\ to-yolo-od \\ -o /workspace/data/sign-yolo-split \\ --labels /workspace/data/sign-yolo-split/labels.txt \\ --labels-csv /workspace/data/sign-yolo-split/labels.csv \\ --split-names train val test \\ --split-ratios 70 15 15 NB: At the time of writing, the yolo plugin for wai.annotations still had a bug which creates empty top-level directories when splitting datasets via --split-names . In the sign-yolo-split directory, you can safely remove the train , test and val directories, since the actual splits are below the images and labels directories: rm -fR data/sign-yolo-split/train data/sign-yolo-split/test data/sign-yolo-split/val Finally, download the dataset.yaml file and place it in the sign-yolo-split directory. It contains information about the dataset directory, the splits and the class labels. Since the labels can come out in a random order, you need to update the labels in the yaml file with the ones that got output in the labels.txt file. You can automatically quote the comma-separated list using the following command: cat data/sign-yolo-split/labels.txt | sed s/,/\\',\\'/g | sed s/^/\\'/g | sed s/$/\\'/g Training # For training, we will use the following docker image: waikatodatamining/pytorch-yolov5:2022-11-05_cuda11.1 If you do not have a GPU, you can use the CPU-only image: waikatodatamining/pytorch-yolov5:2022-11-05_cpu The training script is called yolov5_train , for which we can invoke the help screen as follows: docker run --rm -t waikatodatamining/pytorch-yolov5:2022-11-05_cuda11.1 yolov5_train --help Instead of using config files, we can just tweak parameters via command-line options. However, we still need to download a base model to use for training. Yolov5 offers different models, which differ in speed and accuracy. We will use the medium one called yolov5m.pt from the v6.2 release: https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5m.pt Download it and place it in the models directory. It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: sign-yolov5 Since the image size should be a multiple of 32, we use 416 for this experiment. Kick off the training with the following command: docker run --rm \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/pytorch-yolov5:2022-11-05_cuda11.1 \\ yolov5_train \\ --img 416 \\ --batch 16 \\ --epochs 20 \\ --data /workspace/data/sign-yolo-split/dataset.yaml \\ --weights /workspace/models/yolov5m.pt \\ --project /workspace/output \\ --name sign-yolov5 \\ --exist-ok Exporting to ONNX # Before we can use our trained model, we will need to export it to ONNX format using the yolov5_export script: docker run --rm \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/pytorch-yolov5:2022-11-05_cuda11.1 \\ yolov5_export \\ --weights /workspace/output/sign-yolov5/weights/best.pt \\ --img-size 416 416 \\ --include onnx This will create a file called best.onnx in the output directory. Predicting # Using the yolov5_predict_poll script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run --rm \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/pytorch-yolov5:2022-11-05_cuda11.1 \\ yolov5_predict_poll \\ --model /workspace/output/sign-yolov5/weights/best.onnx \\ --data /workspace/data/sign-yolo-split/dataset.yaml \\ --image_size 416 \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Notes By default, the predictions get output in ROI CSV format . But you can also output them in the OPEX JSON format by adding --prediction_format opex --prediction_suffix .json to the command. You can view the predictions with the ADAMS Preview browser : ROIS CSV OPEX Example prediction Troubleshooting # If you are re-using a dataset that was used by another YolovX framework, you may get strange error messages when reading the data. This can be due to incompatible cache files that get generated to speed up loading the data. Make sure to remove all files in the labels directory that have a .cache extension.","title":"Yolov5"},{"location":"object_detection/yolov5/#prerequisites","text":"Make sure you have the directory structure created as outlined in the Prerequisites .","title":"Prerequisites"},{"location":"object_detection/yolov5/#data","text":"In this example, we will use the American Sign Language Letters dataset, which consists of sets of images of hands, one per letter in the English alphabet (26 labels). Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/american-sign-language-letters/american-sign-language-letters-voc.zip Once extracted, rename the voc directory to sign-voc . Now we have to convert the format from VOC XML into YOLO . We can do this by using the wai.annotations library. At the same time, we can split the dataset into train , validation and test subsets. From within the applied_deep_learning directory, run the following command: docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:0.8.0 \\ wai-annotations convert \\ from-voc-od \\ -i \"/workspace/data/sign-voc/*.xml\" \\ to-yolo-od \\ -o /workspace/data/sign-yolo-split \\ --labels /workspace/data/sign-yolo-split/labels.txt \\ --labels-csv /workspace/data/sign-yolo-split/labels.csv \\ --split-names train val test \\ --split-ratios 70 15 15 NB: At the time of writing, the yolo plugin for wai.annotations still had a bug which creates empty top-level directories when splitting datasets via --split-names . In the sign-yolo-split directory, you can safely remove the train , test and val directories, since the actual splits are below the images and labels directories: rm -fR data/sign-yolo-split/train data/sign-yolo-split/test data/sign-yolo-split/val Finally, download the dataset.yaml file and place it in the sign-yolo-split directory. It contains information about the dataset directory, the splits and the class labels. Since the labels can come out in a random order, you need to update the labels in the yaml file with the ones that got output in the labels.txt file. You can automatically quote the comma-separated list using the following command: cat data/sign-yolo-split/labels.txt | sed s/,/\\',\\'/g | sed s/^/\\'/g | sed s/$/\\'/g","title":"Data"},{"location":"object_detection/yolov5/#training","text":"For training, we will use the following docker image: waikatodatamining/pytorch-yolov5:2022-11-05_cuda11.1 If you do not have a GPU, you can use the CPU-only image: waikatodatamining/pytorch-yolov5:2022-11-05_cpu The training script is called yolov5_train , for which we can invoke the help screen as follows: docker run --rm -t waikatodatamining/pytorch-yolov5:2022-11-05_cuda11.1 yolov5_train --help Instead of using config files, we can just tweak parameters via command-line options. However, we still need to download a base model to use for training. Yolov5 offers different models, which differ in speed and accuracy. We will use the medium one called yolov5m.pt from the v6.2 release: https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5m.pt Download it and place it in the models directory. It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: sign-yolov5 Since the image size should be a multiple of 32, we use 416 for this experiment. Kick off the training with the following command: docker run --rm \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/pytorch-yolov5:2022-11-05_cuda11.1 \\ yolov5_train \\ --img 416 \\ --batch 16 \\ --epochs 20 \\ --data /workspace/data/sign-yolo-split/dataset.yaml \\ --weights /workspace/models/yolov5m.pt \\ --project /workspace/output \\ --name sign-yolov5 \\ --exist-ok","title":"Training"},{"location":"object_detection/yolov5/#exporting-to-onnx","text":"Before we can use our trained model, we will need to export it to ONNX format using the yolov5_export script: docker run --rm \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/pytorch-yolov5:2022-11-05_cuda11.1 \\ yolov5_export \\ --weights /workspace/output/sign-yolov5/weights/best.pt \\ --img-size 416 416 \\ --include onnx This will create a file called best.onnx in the output directory.","title":"Exporting to ONNX"},{"location":"object_detection/yolov5/#predicting","text":"Using the yolov5_predict_poll script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run --rm \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/pytorch-yolov5:2022-11-05_cuda11.1 \\ yolov5_predict_poll \\ --model /workspace/output/sign-yolov5/weights/best.onnx \\ --data /workspace/data/sign-yolo-split/dataset.yaml \\ --image_size 416 \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Notes By default, the predictions get output in ROI CSV format . But you can also output them in the OPEX JSON format by adding --prediction_format opex --prediction_suffix .json to the command. You can view the predictions with the ADAMS Preview browser : ROIS CSV OPEX Example prediction","title":"Predicting"},{"location":"object_detection/yolov5/#troubleshooting","text":"If you are re-using a dataset that was used by another YolovX framework, you may get strange error messages when reading the data. This can be due to incompatible cache files that get generated to speed up loading the data. Make sure to remove all files in the labels directory that have a .cache extension.","title":"Troubleshooting"},{"location":"object_detection/yolov7/","text":"Yolov7 represents the implementation of YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors . Custom docker images with additional tools are available from here: https://github.com/waikato-datamining/pytorch/tree/master/yolov7 Prerequisites # Make sure you have the directory structure created as outlined in the Prerequisites . Data # In this example, we will use the American Sign Language Letters dataset, which consists of sets of images of hands, one per letter in the English alphabet (26 labels). Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/american-sign-language-letters/american-sign-language-letters-voc.zip Once extracted, rename the voc directory to sign-voc . Now we have to convert the format from VOC XML into YOLO . We can do this by using the wai.annotations library. At the same time, we can split the dataset into train , validation and test subsets. From within the applied_deep_learning directory, run the following command: docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:0.8.0 \\ wai-annotations convert \\ from-voc-od \\ -i \"/workspace/data/sign-voc/*.xml\" \\ to-yolo-od \\ -o /workspace/data/sign-yolo-split \\ --labels /workspace/data/sign-yolo-split/labels.txt \\ --labels-csv /workspace/data/sign-yolo-split/labels.csv \\ --split-names train val test \\ --split-ratios 70 15 15 NB: At the time of writing, the yolo plugin for wai.annotations still had a bug which creates empty top-level directories when splitting datasets via --split-names . In the sign-yolo-split directory, you can safely remove the train , test and val directories, since the actual splits are below the images and labels directories: rm -fR data/sign-yolo-split/train data/sign-yolo-split/test data/sign-yolo-split/val Finally, download the dataset7.yaml file, place it in the sign-yolo-split directory. It contains information about the dataset directory, the splits and the class labels. Since the labels can come out in a random order, you need to update the labels in the yaml file with the ones that got output in the labels.txt file. You can automatically quote the comma-separated list using the following command: cat data/sign-yolo-split/labels.txt | sed s/,/\\',\\'/g | sed s/^/\\'/g | sed s/$/\\'/g Training # For training, we will use the following docker image: waikatodatamining/pytorch-yolov7:2022-10-08_cuda11.1 If you do not have a GPU, you can use the CPU-only image: waikatodatamining/pytorch-yolov7:2022-10-08_cpu The training script is called yolov7_train , for which we can invoke the help screen as follows: docker run --rm -t waikatodatamining/pytorch-yolov7:2022-10-08_cuda11.1 yolov7_train --help Since we will be performing transfer larning, we need to download a base model to use for training. Yolov7 offers different models, which differ in speed and accuracy. We will use the fastest one called yolov7_training.pt from the v0.1 release: https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt Download it and place it in the models directory. It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: sign-yolov7 Next, we need to download two more configuration files into our output directory: yolov7.yaml (architecture) adjust the nc parameter and just 26 instead of 80 hyp.scratch.custom.yaml here you can adjust training parameters and also tweak basic image augmentation methods Since the image size should be a multiple of 32, we use 416 for this experiment. Kick off the training with the following command: docker run --rm \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/pytorch-yolov7:2022-10-08_cuda11.1 \\ yolov7_train \\ --workers 1 \\ --device 0 \\ --batch-size 8 \\ --epochs 50 \\ --data /workspace/data/sign-yolo-split/dataset7.yaml \\ --img 416 416 \\ --weights /workspace/models/yolov7_training.pt \\ --project /workspace/output \\ --name sign-yolov7 \\ --exist-ok \\ --hyp /workspace/output/sign-yolov7/hyp.scratch.custom.yaml \\ --cfg /workspace/output/sign-yolov7/yolov7.yaml Predicting # Using the yolov7_predict_poll script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run --rm \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/pytorch-yolov7:2022-10-08_cuda11.1 \\ yolov7_predict_poll \\ --model /workspace/output/sign-yolov7/weights/best.pt \\ --no_trace \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Notes By default, the predictions get output in ROI CSV format . But you can also output them in the OPEX JSON format by adding --prediction_format opex --prediction_suffix .json to the command. You can view the predictions with the ADAMS Preview browser : ROIS CSV OPEX Example prediction Exporting to ONNX (optional) # Before we can use our trained model, we will need to export it to ONNX format using the yolov7_export script: docker run --rm \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/pytorch-yolov7:2022-10-08_cuda11.1 \\ yolov7_export \\ --weights /workspace/output/sign-yolov7/weights/best.pt \\ --grid \\ --end2end \\ --simplify \\ --topk-all 26 \\ --iou-thres 0.65 \\ --conf-thres 0.35 \\ --img-size 416 416 \\ --max-wh 416 This will create a file called best.onnx in the output directory. Troubleshooting # If you are re-using a dataset that was used by another YolovX framework, you may get strange error messages when reading the data. This can be due to incompatible cache files that get generated to speed up loading the data. Make sure to remove all files in the labels directory that have a .cache extension.","title":"Yolov7"},{"location":"object_detection/yolov7/#prerequisites","text":"Make sure you have the directory structure created as outlined in the Prerequisites .","title":"Prerequisites"},{"location":"object_detection/yolov7/#data","text":"In this example, we will use the American Sign Language Letters dataset, which consists of sets of images of hands, one per letter in the English alphabet (26 labels). Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/american-sign-language-letters/american-sign-language-letters-voc.zip Once extracted, rename the voc directory to sign-voc . Now we have to convert the format from VOC XML into YOLO . We can do this by using the wai.annotations library. At the same time, we can split the dataset into train , validation and test subsets. From within the applied_deep_learning directory, run the following command: docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:0.8.0 \\ wai-annotations convert \\ from-voc-od \\ -i \"/workspace/data/sign-voc/*.xml\" \\ to-yolo-od \\ -o /workspace/data/sign-yolo-split \\ --labels /workspace/data/sign-yolo-split/labels.txt \\ --labels-csv /workspace/data/sign-yolo-split/labels.csv \\ --split-names train val test \\ --split-ratios 70 15 15 NB: At the time of writing, the yolo plugin for wai.annotations still had a bug which creates empty top-level directories when splitting datasets via --split-names . In the sign-yolo-split directory, you can safely remove the train , test and val directories, since the actual splits are below the images and labels directories: rm -fR data/sign-yolo-split/train data/sign-yolo-split/test data/sign-yolo-split/val Finally, download the dataset7.yaml file, place it in the sign-yolo-split directory. It contains information about the dataset directory, the splits and the class labels. Since the labels can come out in a random order, you need to update the labels in the yaml file with the ones that got output in the labels.txt file. You can automatically quote the comma-separated list using the following command: cat data/sign-yolo-split/labels.txt | sed s/,/\\',\\'/g | sed s/^/\\'/g | sed s/$/\\'/g","title":"Data"},{"location":"object_detection/yolov7/#training","text":"For training, we will use the following docker image: waikatodatamining/pytorch-yolov7:2022-10-08_cuda11.1 If you do not have a GPU, you can use the CPU-only image: waikatodatamining/pytorch-yolov7:2022-10-08_cpu The training script is called yolov7_train , for which we can invoke the help screen as follows: docker run --rm -t waikatodatamining/pytorch-yolov7:2022-10-08_cuda11.1 yolov7_train --help Since we will be performing transfer larning, we need to download a base model to use for training. Yolov7 offers different models, which differ in speed and accuracy. We will use the fastest one called yolov7_training.pt from the v0.1 release: https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt Download it and place it in the models directory. It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: sign-yolov7 Next, we need to download two more configuration files into our output directory: yolov7.yaml (architecture) adjust the nc parameter and just 26 instead of 80 hyp.scratch.custom.yaml here you can adjust training parameters and also tweak basic image augmentation methods Since the image size should be a multiple of 32, we use 416 for this experiment. Kick off the training with the following command: docker run --rm \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/pytorch-yolov7:2022-10-08_cuda11.1 \\ yolov7_train \\ --workers 1 \\ --device 0 \\ --batch-size 8 \\ --epochs 50 \\ --data /workspace/data/sign-yolo-split/dataset7.yaml \\ --img 416 416 \\ --weights /workspace/models/yolov7_training.pt \\ --project /workspace/output \\ --name sign-yolov7 \\ --exist-ok \\ --hyp /workspace/output/sign-yolov7/hyp.scratch.custom.yaml \\ --cfg /workspace/output/sign-yolov7/yolov7.yaml","title":"Training"},{"location":"object_detection/yolov7/#predicting","text":"Using the yolov7_predict_poll script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run --rm \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/pytorch-yolov7:2022-10-08_cuda11.1 \\ yolov7_predict_poll \\ --model /workspace/output/sign-yolov7/weights/best.pt \\ --no_trace \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Notes By default, the predictions get output in ROI CSV format . But you can also output them in the OPEX JSON format by adding --prediction_format opex --prediction_suffix .json to the command. You can view the predictions with the ADAMS Preview browser : ROIS CSV OPEX Example prediction","title":"Predicting"},{"location":"object_detection/yolov7/#exporting-to-onnx-optional","text":"Before we can use our trained model, we will need to export it to ONNX format using the yolov7_export script: docker run --rm \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/pytorch-yolov7:2022-10-08_cuda11.1 \\ yolov7_export \\ --weights /workspace/output/sign-yolov7/weights/best.pt \\ --grid \\ --end2end \\ --simplify \\ --topk-all 26 \\ --iou-thres 0.65 \\ --conf-thres 0.35 \\ --img-size 416 416 \\ --max-wh 416 This will create a file called best.onnx in the output directory.","title":"Exporting to ONNX (optional)"},{"location":"object_detection/yolov7/#troubleshooting","text":"If you are re-using a dataset that was used by another YolovX framework, you may get strange error messages when reading the data. This can be due to incompatible cache files that get generated to speed up loading the data. Make sure to remove all files in the labels directory that have a .cache extension.","title":"Troubleshooting"},{"location":"speech_to_text/","text":"Speech-to-text ( STT; also ASR ) transcribes audio files, usually short phrases, into text. These models can be used, e.g., for voice assistants that react to spoken commands. What to do next: Annotate your data Choose a framework","title":"Introduction"},{"location":"speech_to_text/annotate/","text":"Audio # The audio recordings should be sampled with 16kHz, in mono and stored in WAV files. You can do this using wai.annotations . E.g., the following command converts all WAV files in INPUT_DIR and stores them in OUTPUT_DIR : docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:0.8.0 \\ wai-annotations convert \\ from-audio-files-ac \\ -i \"INPUT_DIR/*.wav\" \\ convert-to-mono \\ resample-audio \\ -s 16000 \\ to-audio-files-ac \\ -o OUTPUT_DIR Annotations # The simplest file formats for speech annotations are probably: Festvox Coqui TTS Festvox # The annotations are stored in a text file, each row for a separated recording, using the following format: ( FILENAME \"TRANSCRIPT\" ) FILENAME - the name of the file without extension TRANSCRIPT - the text associated with the recording It is best to store the annotations file alongside the WAV files to avoid paths. Coqui TTS # In this format , the annotations are stored in a CSV file with the following columns: wav_filename - the name of the file with extension wav_filesize - the size of the WAV file in bytes transcript - the text associated with the recording It is best to store the annotations file alongside the WAV files to avoid paths.","title":"Annotate"},{"location":"speech_to_text/annotate/#audio","text":"The audio recordings should be sampled with 16kHz, in mono and stored in WAV files. You can do this using wai.annotations . E.g., the following command converts all WAV files in INPUT_DIR and stores them in OUTPUT_DIR : docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:0.8.0 \\ wai-annotations convert \\ from-audio-files-ac \\ -i \"INPUT_DIR/*.wav\" \\ convert-to-mono \\ resample-audio \\ -s 16000 \\ to-audio-files-ac \\ -o OUTPUT_DIR","title":"Audio"},{"location":"speech_to_text/annotate/#annotations","text":"The simplest file formats for speech annotations are probably: Festvox Coqui TTS","title":"Annotations"},{"location":"speech_to_text/annotate/#festvox","text":"The annotations are stored in a text file, each row for a separated recording, using the following format: ( FILENAME \"TRANSCRIPT\" ) FILENAME - the name of the file without extension TRANSCRIPT - the text associated with the recording It is best to store the annotations file alongside the WAV files to avoid paths.","title":"Festvox"},{"location":"speech_to_text/annotate/#coqui-tts","text":"In this format , the annotations are stored in a CSV file with the following columns: wav_filename - the name of the file with extension wav_filesize - the size of the WAV file in bytes transcript - the text associated with the recording It is best to store the annotations file alongside the WAV files to avoid paths.","title":"Coqui TTS"},{"location":"speech_to_text/coqui_stt/","text":"Coqui STT is a deep learning toolkit for Speech-to-Text. Custom docker images with additional tools are available from here: https://github.com/waikato-datamining/tensorflow/tree/master/coqui/stt Prerequisites # Make sure you have the directory structure created as outlined in the Prerequisites . Data # In this example, we will use the Irish dataset of the Living Audio Datasets collection. Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/living-audio-datasets/irish-coqui-stt.zip Rename the coqui-stt directory to lad-irish . Training # For training, we will use the following docker image: waikatodatamining/tf_coqui_stt:1.4.0_cuda11.6 At prediction time, we no longer need a GPU and can use this image: waikatodatamining/tf_coqui_stt:1.15.2_0.10.0a10_cpu The training script is called stt_train , for which we can invoke the help screen as follows: docker run --rm -t waikatodatamining/tf_coqui_stt:1.4.0_cuda11.6 stt_train --help Instead of using config files, we can just tweak parameters via command-line options . However, we still need to download a base model to use for transfer learning: https://github.com/coqui-ai/STT/releases/download/v1.3.0/coqui-stt-1.3.0-checkpoint.tar.gz (648MB) Download it into the models directory and decompress it ( tar -xzf coqui-stt-1.3.0-checkpoint.tar.gz ). It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: lad-irish-coqui For generating the alphabet specific to this dataset, use the following command (if necessary, you can supply multiple CSV files to the stt_alphabet script): docker run --rm \\ -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_coqui_stt:1.4.0_cuda11.6 \\ stt_alphabet \\ -i /workspace/data/lad-irish/samples.csv \\ -o /workspace/data/lad-irish/alphabet.txt Kick off transfer learning with the following command: docker run --rm \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_coqui_stt:1.4.0_cuda11.6 \\ stt_train \\ --alphabet_config_path /workspace/data/lad-irish/alphabet.txt \\ --auto_input_dataset /workspace/data/lad-irish/samples.csv \\ --drop_source_layers 2 \\ --n_hidden 2048 \\ --use_allow_growth true \\ --train_cudnn true \\ --train_batch_size 16 \\ --dev_batch_size 16 \\ --export_batch_size 16 \\ --epochs 75 \\ --skip_batch_test true \\ --load_checkpoint_dir /workspace/models/coqui-stt-1.3.0-checkpoint \\ --save_checkpoint_dir /workspace/output/lad-irish-coqui Notes: How many layers you drop when training with a new dataset ( --drop_source_layers X ) requires some experimentation, dropping just one does not always work. --auto_input_dataset will split the single dataset into train.csv , dev.csv and test.csv . If you already have these datasets (e.g., obtained from Common Voice), then you can use the --train_files , --dev_files and --test_files options to supply them explicitly. The checkpoint available for release 1.4.0 did not work well, hence we are still using the one for 1.3.0. Evaluating the model # Once the model has been built, we can evaluate it using stt_eval . This script will use the test set and output the best and worst transcripts (using CER and WER as metrics). You can run it like this: docker run --rm \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_coqui_stt:1.4.0_cuda11.6 \\ stt_eval \\ --alphabet_config_path /workspace/data/lad-irish/alphabet.txt \\ --train_files /workspace/data/lad-irish/train.csv \\ --dev_files /workspace/data/lad-irish/dev.csv \\ --test_files /workspace/data/lad-irish/test.csv \\ --test_batch_size 16 \\ --export_batch_size 16 \\ --checkpoint_dir /workspace/output/lad-irish-coqui Exporting to tflite # Before we can use our trained model, we will need to export it to TensorFlow lite (aka tflite) using the stt_export script: docker run --rm \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_coqui_stt:1.4.0_cuda11.6 \\ stt_export \\ --export_quantize false \\ --checkpoint_dir /workspace/output/lad-irish-coqui \\ --export_dir /workspace/output/lad-irish-coqui/export This will create a file called output_graph.tflite in the export sub-directory of the output directory. Notes --export_quantize false - will create a more accurate but slower model --export_quantize true - will create a model optimized for speed/memory but less accurate Predicting # Using the stt_transcribe_poll script, we can batch-process audio files placed in the predictions/in directory as follows (e.g., from our test subset): docker run --rm \\ -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_coqui_stt:1.4.0_cuda11.6 \\ stt_transcribe_poll \\ --model /workspace/output/lad-irish-coqui/export/output_graph.tflite \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out If you just want to test with a single audio file, then use stt_transcribe_single instead (point to the audio file with --audio ): docker run --rm \\ -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_coqui_stt:1.4.0_cuda11.6 \\ stt_transcribe_single \\ --model /workspace/output/lad-irish-coqui/export/output_graph.tflite \\ --audio /workspace/data/lad-irish/cll_z0001_018.wav This will output something like this: Loading model from file /workspace/output/lad-irish-coqui/export/output_graph.tflite TensorFlow: v2.9.1-11-gf8242ebc005 Coqui STT: v1.4.0-0-gfcec06bd INFO: Created TensorFlow Lite XNNPACK delegate for CPU. Loaded model in 0.00207s. Running inference. Iir scol\u00e1ira b\u00e9leidis \u00e9 eidrsan. Inference took 2.371s for 2.464s audio file. The ground truth for cll_z0001_018.wav is as follows: N\u00edor scol\u00e1ire b\u00e9aloidis \u00e9 Pedersen.","title":"Coqui STT"},{"location":"speech_to_text/coqui_stt/#prerequisites","text":"Make sure you have the directory structure created as outlined in the Prerequisites .","title":"Prerequisites"},{"location":"speech_to_text/coqui_stt/#data","text":"In this example, we will use the Irish dataset of the Living Audio Datasets collection. Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/living-audio-datasets/irish-coqui-stt.zip Rename the coqui-stt directory to lad-irish .","title":"Data"},{"location":"speech_to_text/coqui_stt/#training","text":"For training, we will use the following docker image: waikatodatamining/tf_coqui_stt:1.4.0_cuda11.6 At prediction time, we no longer need a GPU and can use this image: waikatodatamining/tf_coqui_stt:1.15.2_0.10.0a10_cpu The training script is called stt_train , for which we can invoke the help screen as follows: docker run --rm -t waikatodatamining/tf_coqui_stt:1.4.0_cuda11.6 stt_train --help Instead of using config files, we can just tweak parameters via command-line options . However, we still need to download a base model to use for transfer learning: https://github.com/coqui-ai/STT/releases/download/v1.3.0/coqui-stt-1.3.0-checkpoint.tar.gz (648MB) Download it into the models directory and decompress it ( tar -xzf coqui-stt-1.3.0-checkpoint.tar.gz ). It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: lad-irish-coqui For generating the alphabet specific to this dataset, use the following command (if necessary, you can supply multiple CSV files to the stt_alphabet script): docker run --rm \\ -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_coqui_stt:1.4.0_cuda11.6 \\ stt_alphabet \\ -i /workspace/data/lad-irish/samples.csv \\ -o /workspace/data/lad-irish/alphabet.txt Kick off transfer learning with the following command: docker run --rm \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_coqui_stt:1.4.0_cuda11.6 \\ stt_train \\ --alphabet_config_path /workspace/data/lad-irish/alphabet.txt \\ --auto_input_dataset /workspace/data/lad-irish/samples.csv \\ --drop_source_layers 2 \\ --n_hidden 2048 \\ --use_allow_growth true \\ --train_cudnn true \\ --train_batch_size 16 \\ --dev_batch_size 16 \\ --export_batch_size 16 \\ --epochs 75 \\ --skip_batch_test true \\ --load_checkpoint_dir /workspace/models/coqui-stt-1.3.0-checkpoint \\ --save_checkpoint_dir /workspace/output/lad-irish-coqui Notes: How many layers you drop when training with a new dataset ( --drop_source_layers X ) requires some experimentation, dropping just one does not always work. --auto_input_dataset will split the single dataset into train.csv , dev.csv and test.csv . If you already have these datasets (e.g., obtained from Common Voice), then you can use the --train_files , --dev_files and --test_files options to supply them explicitly. The checkpoint available for release 1.4.0 did not work well, hence we are still using the one for 1.3.0.","title":"Training"},{"location":"speech_to_text/coqui_stt/#evaluating-the-model","text":"Once the model has been built, we can evaluate it using stt_eval . This script will use the test set and output the best and worst transcripts (using CER and WER as metrics). You can run it like this: docker run --rm \\ -u $(id -u):$(id -g) \\ --shm-size 8G \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_coqui_stt:1.4.0_cuda11.6 \\ stt_eval \\ --alphabet_config_path /workspace/data/lad-irish/alphabet.txt \\ --train_files /workspace/data/lad-irish/train.csv \\ --dev_files /workspace/data/lad-irish/dev.csv \\ --test_files /workspace/data/lad-irish/test.csv \\ --test_batch_size 16 \\ --export_batch_size 16 \\ --checkpoint_dir /workspace/output/lad-irish-coqui","title":"Evaluating the model"},{"location":"speech_to_text/coqui_stt/#exporting-to-tflite","text":"Before we can use our trained model, we will need to export it to TensorFlow lite (aka tflite) using the stt_export script: docker run --rm \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_coqui_stt:1.4.0_cuda11.6 \\ stt_export \\ --export_quantize false \\ --checkpoint_dir /workspace/output/lad-irish-coqui \\ --export_dir /workspace/output/lad-irish-coqui/export This will create a file called output_graph.tflite in the export sub-directory of the output directory. Notes --export_quantize false - will create a more accurate but slower model --export_quantize true - will create a model optimized for speed/memory but less accurate","title":"Exporting to tflite"},{"location":"speech_to_text/coqui_stt/#predicting","text":"Using the stt_transcribe_poll script, we can batch-process audio files placed in the predictions/in directory as follows (e.g., from our test subset): docker run --rm \\ -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_coqui_stt:1.4.0_cuda11.6 \\ stt_transcribe_poll \\ --model /workspace/output/lad-irish-coqui/export/output_graph.tflite \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out If you just want to test with a single audio file, then use stt_transcribe_single instead (point to the audio file with --audio ): docker run --rm \\ -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_coqui_stt:1.4.0_cuda11.6 \\ stt_transcribe_single \\ --model /workspace/output/lad-irish-coqui/export/output_graph.tflite \\ --audio /workspace/data/lad-irish/cll_z0001_018.wav This will output something like this: Loading model from file /workspace/output/lad-irish-coqui/export/output_graph.tflite TensorFlow: v2.9.1-11-gf8242ebc005 Coqui STT: v1.4.0-0-gfcec06bd INFO: Created TensorFlow Lite XNNPACK delegate for CPU. Loaded model in 0.00207s. Running inference. Iir scol\u00e1ira b\u00e9leidis \u00e9 eidrsan. Inference took 2.371s for 2.464s audio file. The ground truth for cll_z0001_018.wav is as follows: N\u00edor scol\u00e1ire b\u00e9aloidis \u00e9 Pedersen.","title":"Predicting"},{"location":"speech_to_text/frameworks/","text":"The following frameworks are available for speech-to-text : Coqui STT","title":"Frameworks"}]}