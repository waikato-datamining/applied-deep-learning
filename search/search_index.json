{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"While working on a number of projects over the years, we have developed (and maintain) a number of Python libraries and Docker images that can be used for various deep learning tasks. Especially the use of Docker made it a lot easier and faster to apply algorithms to new datasets. If you have not used Docker before, we recommend you to have a look at our introduction called Docker for Data Scientists . The following domains are covered: Image classification Object detection Instance segmentation Image segmentation","title":"Home"},{"location":"prerequisites/","text":"The following prerequisites apply to most data domains: Annotations In order to annotate data (e.g., for object detection, instance segmentation or image segmentation), you should download the ADAMS Annotator application (you only need Java 11 installed ): Snapshot Release 2021.12.0 or later Format conversions For turning annotations from one format into another, you need to install the wai.annotations Python library: wai.annotations project wai.annotations manual NB: it is recommended to install it in a virtual environment to avoid any conflicts with the host system. Hardware For building models, a computer with NVIDIA GPU (8GB+) and Linux operating system is recommended. Only image classification models can be built in a reasonable amount of time on CPU-only machines. Directory structure Create the following directory structure for the examples of this tutorial: | +-- applied_deep_learning | +-- data | +-- output | +-- predictions | +-- in | +-- out","title":"Prerequisites"},{"location":"prerequisites/#annotations","text":"In order to annotate data (e.g., for object detection, instance segmentation or image segmentation), you should download the ADAMS Annotator application (you only need Java 11 installed ): Snapshot Release 2021.12.0 or later","title":"Annotations"},{"location":"prerequisites/#format-conversions","text":"For turning annotations from one format into another, you need to install the wai.annotations Python library: wai.annotations project wai.annotations manual NB: it is recommended to install it in a virtual environment to avoid any conflicts with the host system.","title":"Format conversions"},{"location":"prerequisites/#hardware","text":"For building models, a computer with NVIDIA GPU (8GB+) and Linux operating system is recommended. Only image classification models can be built in a reasonable amount of time on CPU-only machines.","title":"Hardware"},{"location":"prerequisites/#directory-structure","text":"Create the following directory structure for the examples of this tutorial: | +-- applied_deep_learning | +-- data | +-- output | +-- predictions | +-- in | +-- out","title":"Directory structure"},{"location":"image_classification/","text":"Image classification is the simplest and least computational expensive task, as it classifies whole images, assigning them a category. Before you delve into training models, take a look at how you need to annotate your data . The following frameworks are available: wai.pytorchimageclass wai.tfimageclass wai.tflite_model_maker","title":"Introduction"},{"location":"image_classification/annotate/","text":"Annotating data for image classification is straight forward. Since we are classifying whole images, we can simply use the directory layout for annotating images. In the example below, we have a dataset called flowers . The sub-directories below the flowers directory represent the labels for the images containing within the sub-directories. | +- flowers | +- daisy | +- dandelion | +- roses | +- sunflowers | +- tulip NB: To avoid any issues, the labels should be lowercase and underscores should be used instead of blanks/spaces.","title":"Annotate"},{"location":"image_classification/wai.pytorchimageclass/","text":"With wai.pytorchimageclass it is possible to train various image classification network architectues and also perform inference. The code is based on this Pytorch example: https://github.com/pytorch/examples/tree/master/imagenet Specifically, this commit: 49e1a8847c8c4d8d3c576479cb2fe2fd2ac583de","title":"wai.pytorchimageclass"},{"location":"image_classification/wai.tfimageclass/","text":"The wai.tfimageclass Python library ( PyPI ) can be used for training various models that are available from the Tensorflow Hub . Prerequisites Make sure you have the directory structure created as outlined in the Prerequisites . Data In this example, we will use the 102 flowers dataset, which consists of 102 different categories (~ species) of flowers. More precisely, we will download the dataset with the flowers already split into categories from which will use a subset to speed up the training process. Download the dataset from the following URL and extract it: http://datasets.cms.waikato.ac.nz/ufdl/data/image_classification/102flowers/102flowers-categories.tgz Once extracted, you can delete all sub-directories apart from: alpine_sea_holly anthurium artichoke Rename the categories directory to 3flowers and move it into the data folder of our directory structure outlined. Training For training, we will use the following docker image: waikatodatamining/tf_image_classification:1.14 If you only have a CPU machine available, then use this one instead: waikatodatamining/tf_image_classification:1.14_cpu The training script is called tfic-retrain , for which we can invoke the help screen as follows: docker run -t waikatodatamining/tf_image_classification:1.14_cpu tfic-retrain --help It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: 3flowers-default The following command, issued in the applied_deep_learning directory, will map the applied_deep_learning directory (= the current working directory) onto the /workspace directory within the docker container, giving us access to all the sub-directories there, and train our 3flowsers dataset: docker run \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_image_classification:1.14_cpu \\ tfic-retrain \\ --image_dir /workspace/data/3flowers \\ --output_graph /workspace/output/3flowers-default/graph.pb \\ --training_steps 500 Predicting","title":"wai.tfimageclass"},{"location":"image_classification/wai.tfimageclass/#prerequisites","text":"Make sure you have the directory structure created as outlined in the Prerequisites .","title":"Prerequisites"},{"location":"image_classification/wai.tfimageclass/#data","text":"In this example, we will use the 102 flowers dataset, which consists of 102 different categories (~ species) of flowers. More precisely, we will download the dataset with the flowers already split into categories from which will use a subset to speed up the training process. Download the dataset from the following URL and extract it: http://datasets.cms.waikato.ac.nz/ufdl/data/image_classification/102flowers/102flowers-categories.tgz Once extracted, you can delete all sub-directories apart from: alpine_sea_holly anthurium artichoke Rename the categories directory to 3flowers and move it into the data folder of our directory structure outlined.","title":"Data"},{"location":"image_classification/wai.tfimageclass/#training","text":"For training, we will use the following docker image: waikatodatamining/tf_image_classification:1.14 If you only have a CPU machine available, then use this one instead: waikatodatamining/tf_image_classification:1.14_cpu The training script is called tfic-retrain , for which we can invoke the help screen as follows: docker run -t waikatodatamining/tf_image_classification:1.14_cpu tfic-retrain --help It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: 3flowers-default The following command, issued in the applied_deep_learning directory, will map the applied_deep_learning directory (= the current working directory) onto the /workspace directory within the docker container, giving us access to all the sub-directories there, and train our 3flowsers dataset: docker run \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_image_classification:1.14_cpu \\ tfic-retrain \\ --image_dir /workspace/data/3flowers \\ --output_graph /workspace/output/3flowers-default/graph.pb \\ --training_steps 500","title":"Training"},{"location":"image_classification/wai.tfimageclass/#predicting","text":"","title":"Predicting"},{"location":"image_classification/wai.tflite_model_maker/","text":"","title":"wai.tflite_model_maker"},{"location":"image_segmentation/","text":"Image segmentation classifies individual pixels within an image, assigning them a label (i.e., a color). Input Annotation/Output Before you delve into training models, take a look at how you need to annotate your data . The following frameworks are available: Image Segmentation Keras","title":"Introduction"},{"location":"image_segmentation/annotate/","text":"You can use the ADAMS framework for annotating your images. ADAMS comes pre-bundled in various setups and we need the adams-annotator bundle. You can get either: snapshot release (21.12.0 or later) In the Flow editor (available from the Tools menu in ADAMS), you can load and execute the follow workflow (which is part of your ADAMS installation when downloading it as zip file): adams-imaging-image_segmentation_annotation.flow","title":"Annotate"},{"location":"image_segmentation/image-segmentation-keras/","text":"","title":"Image Segmentation Keras"},{"location":"instance_segmentation/","text":"Instance segmentation not only finds objects within images, it also learns the shape of the objects rather than just a simple rectangle/bounding box. However, this makes the algorithms computationally more expensive and more memory hungry. Before you delve into training models, take a look at how you need to annotate your data . The following frameworks are available: Detectron2","title":"Introduction"},{"location":"instance_segmentation/annotate/","text":"You can use the ADAMS framework for annotating your images. ADAMS comes pre-bundled in various setups and we need the adams-annotator bundle. You can get either: snapshot release (21.12.0 or later) In the Flow editor (available from the Tools menu in ADAMS), you can load and execute the follow workflow (which is part of your ADAMS installation when downloading it as zip file): adams-imaging-annotate_objects.flow When prompted, use object_shape as Selection type .","title":"Annotate"},{"location":"instance_segmentation/detectron2/","text":"","title":"Detectron2"},{"location":"object_detection/","text":"Object detection detects one or more objects within an image and assigns each one of them a label, as opposed to image classification which assigns a lable to the whole image. The predicted locations are typically rectangles (aka bounding boxes). If you are looking for shapes or polygons, then have a look at instance segmentation . Before you delve into training models, take a look at how you need to annotate your data . The following frameworks are available: MMDetection wai.tflite_model_maker","title":"Introduction"},{"location":"object_detection/annotate/","text":"You can use the ADAMS framework for annotating your images. ADAMS comes pre-bundled in various setups and we need the adams-annotator bundle. You can get either: snapshot release (21.12.0 or later) In the Flow editor (available from the Tools menu in ADAMS), you can load and execute the follow workflow (which is part of your ADAMS installation when downloading it as zip file): adams-imaging-annotate_objects.flow When prompted, use bounding_box as Selection type .","title":"Annotate"},{"location":"object_detection/mmdetection/","text":"","title":"MMDetection"},{"location":"object_detection/wai.tflite_model_maker/","text":"","title":"wai.tflite_model_maker"}]}