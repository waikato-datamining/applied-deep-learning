{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Over the years, we have been working on a number of projects that involved applying deep learning algorithms to image problems. Setting up deep learning frameworks is always a slow and painstaking process, getting all the library dependencies right (CUDA, cuDNN, numpy, etc). To speed things up, we have developed (and maintain) a number of Python libraries and Docker images that can be used for various deep learning tasks. Here we are showing examples on how you can use these images on datasets to train your own models and how to apply them. The use of Docker made it a lot easier and faster to apply algorithms to new datasets. If you have not used Docker before, we recommend you to have a look at our introduction called Docker for Data Scientists . The following domains are covered with examples: Image classification Object detection Instance segmentation Image segmentation","title":"Home"},{"location":"prerequisites/","text":"The following prerequisites apply to most data domains: Annotations In order to annotate data (e.g., for object detection, instance segmentation or image segmentation), you should download the ADAMS Annotator application (you only need Java 11 installed ): Snapshot Release 2021.12.0 or later Format conversions For turning annotations from one format into another, you need to install the wai.annotations Python library: wai.annotations project wai.annotations manual NB: it is recommended to install it in a virtual environment to avoid any conflicts with the host system. Hardware For building models, a computer with NVIDIA GPU (8GB+) and Linux operating system is recommended. Only image classification models can be built in a reasonable amount of time on CPU-only machines. Directory structure Create the following directory structure for the examples of this tutorial: | +-- applied_deep_learning | +-- cache | +-- data | +-- output | +-- predictions | +-- in | +-- out Docker notes To make the Docker commands as easy as possible, they will all get issued from the within the applied_deep_learning directory. In order to get access to the data , output and predictions directories, we use the following mapping: -v `pwd`:/workspace This will map the applied_deep_learning directory onto the /workspace directory within the container. Since Docker usually runs as root within the container, we want to make sure that the user group of any files that get generated are being owned by the current user. This can be achieved by adding the following to the Docker command: -u $(id -u):$(id -g) -e USER=$USER","title":"Prerequisites"},{"location":"prerequisites/#annotations","text":"In order to annotate data (e.g., for object detection, instance segmentation or image segmentation), you should download the ADAMS Annotator application (you only need Java 11 installed ): Snapshot Release 2021.12.0 or later","title":"Annotations"},{"location":"prerequisites/#format-conversions","text":"For turning annotations from one format into another, you need to install the wai.annotations Python library: wai.annotations project wai.annotations manual NB: it is recommended to install it in a virtual environment to avoid any conflicts with the host system.","title":"Format conversions"},{"location":"prerequisites/#hardware","text":"For building models, a computer with NVIDIA GPU (8GB+) and Linux operating system is recommended. Only image classification models can be built in a reasonable amount of time on CPU-only machines.","title":"Hardware"},{"location":"prerequisites/#directory-structure","text":"Create the following directory structure for the examples of this tutorial: | +-- applied_deep_learning | +-- cache | +-- data | +-- output | +-- predictions | +-- in | +-- out","title":"Directory structure"},{"location":"prerequisites/#docker-notes","text":"To make the Docker commands as easy as possible, they will all get issued from the within the applied_deep_learning directory. In order to get access to the data , output and predictions directories, we use the following mapping: -v `pwd`:/workspace This will map the applied_deep_learning directory onto the /workspace directory within the container. Since Docker usually runs as root within the container, we want to make sure that the user group of any files that get generated are being owned by the current user. This can be achieved by adding the following to the Docker command: -u $(id -u):$(id -g) -e USER=$USER","title":"Docker notes"},{"location":"image_classification/","text":"Image classification is the simplest and least computational expensive task, as it classifies whole images, assigning them a category. Before you delve into training models, take a look at how you need to annotate your data . The following frameworks are available: MMClassification wai.tfimageclass","title":"Introduction"},{"location":"image_classification/annotate/","text":"Annotating data for image classification is straight forward. Since we are classifying whole images, we can simply use the directory layout for annotating images. In the example below, we have a dataset called flowers . The sub-directories below the flowers directory represent the labels for the images containing within the sub-directories. | +- flowers | +- daisy | +- dandelion | +- roses | +- sunflowers | +- tulip NB: To avoid any issues, the labels should be lowercase and underscores should be used instead of blanks/spaces.","title":"Annotate"},{"location":"image_classification/mmclassification/","text":"","title":"\u00bb MMClassification"},{"location":"image_classification/wai.tfimageclass/","text":"The wai.tfimageclass Python library ( PyPI ) can be used for training various models that are available from the Tensorflow Hub . Prerequisites Make sure you have the directory structure created as outlined in the Prerequisites . Data In this example, we will use the 102 flowers dataset, which consists of 102 different categories (~ species) of flowers. More precisely, we will download the dataset with the flowers already split into categories from which we will use a subset to speed up the training process. Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/102flowers/102flowers-subdir.zip Once extracted, you can delete all sub-directories apart from: alpine_sea_holly anthurium artichoke Rename the subdir directory to 3flowers and move it into the data folder of our directory structure outlined. Training For training, we will use the following docker image: waikatodatamining/tf_image_classification:1.14 If you only have a CPU machine available, then use this one instead: waikatodatamining/tf_image_classification:1.14_cpu The training script is called tfic-retrain , for which we can invoke the help screen as follows: docker run -t waikatodatamining/tf_image_classification:1.14 tfic-retrain --help # GPU docker run -t waikatodatamining/tf_image_classification:1.14_cpu tfic-retrain --help # CPU It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: 3flowers-tf-default The following command will train an Inception v3 model for 500 steps: GPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules -t waikatodatamining/tf_image_classification:1.14 \\ tfic-retrain \\ --image_dir /workspace/data/3flowers \\ --output_graph /workspace/output/3flowers-tf-default/graph.pb \\ --output_info /workspace/output/3flowers-tf-default/graph.json \\ --saved_model_dir /workspace/output/3flowers-tf-default/saved_model \\ --image_lists_dir /workspace/output/3flowers-tf-default \\ --tfhub_module https://tfhub.dev/google/imagenet/inception_v3/feature_vector/3 \\ --training_steps 500 CPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules -t waikatodatamining/tf_image_classification:1.14_cpu \\ tfic-retrain \\ --image_dir /workspace/data/3flowers \\ --output_graph /workspace/output/3flowers-tf-default/graph.pb \\ --output_info /workspace/output/3flowers-tf-default/graph.json \\ --saved_model_dir /workspace/output/3flowers-tf-default/saved_model \\ --image_lists_dir /workspace/output/3flowers-tf-default \\ --tfhub_module https://tfhub.dev/google/imagenet/inception_v3/feature_vector/3 \\ --training_steps 500 Exporting model Before we can use the model, we need to export it to Tensorflow lite or tflite : GPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules -t waikatodatamining/tf_image_classification:1.14 \\ tfic-export \\ --saved_model_dir /workspace/output/3flowers-tf-default/saved_model \\ --tflite_model /workspace/output/3flowers-tf-default/graph.tflite CPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules -t waikatodatamining/tf_image_classification:1.14_cpu \\ tfic-export \\ --saved_model_dir /workspace/output/3flowers-tf-default/saved_model \\ --tflite_model /workspace/output/3flowers-tf-default/graph.tflite Predicting For making predictions for a single image, you can use the script tfic-labelimage . Since we will want to batch predict multiple images, will use the script tfic-poll instead: GPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_image_classification:1.14 \\ tfic-poll \\ --graph /workspace/output/3flowers-tf-default/graph.tflite \\ --graph_type tflite \\ --info /workspace/output/3flowers-tf-default/graph.json \\ --in_dir /workspace/predictions/in \\ --out_dir /workspace/predictions/out CPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_image_classification:1.14_cpu \\ tfic-poll \\ --graph /workspace/output/3flowers-tf-default/graph.tflite \\ --graph_type tflite \\ --info /workspace/output/3flowers-tf-default/graph.json \\ --in_dir /workspace/predictions/in \\ --out_dir /workspace/predictions/out E.g., for the image_01965.jpg from the anthurium class, we will get a CSV file similar to this one : label probability anthurium 0.989429 artichoke 0.0060946 alpine_sea_holly 0.00447612","title":"\u00bb wai.tfimageclass"},{"location":"image_classification/wai.tfimageclass/#prerequisites","text":"Make sure you have the directory structure created as outlined in the Prerequisites .","title":"Prerequisites"},{"location":"image_classification/wai.tfimageclass/#data","text":"In this example, we will use the 102 flowers dataset, which consists of 102 different categories (~ species) of flowers. More precisely, we will download the dataset with the flowers already split into categories from which we will use a subset to speed up the training process. Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/102flowers/102flowers-subdir.zip Once extracted, you can delete all sub-directories apart from: alpine_sea_holly anthurium artichoke Rename the subdir directory to 3flowers and move it into the data folder of our directory structure outlined.","title":"Data"},{"location":"image_classification/wai.tfimageclass/#training","text":"For training, we will use the following docker image: waikatodatamining/tf_image_classification:1.14 If you only have a CPU machine available, then use this one instead: waikatodatamining/tf_image_classification:1.14_cpu The training script is called tfic-retrain , for which we can invoke the help screen as follows: docker run -t waikatodatamining/tf_image_classification:1.14 tfic-retrain --help # GPU docker run -t waikatodatamining/tf_image_classification:1.14_cpu tfic-retrain --help # CPU It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: 3flowers-tf-default The following command will train an Inception v3 model for 500 steps: GPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules -t waikatodatamining/tf_image_classification:1.14 \\ tfic-retrain \\ --image_dir /workspace/data/3flowers \\ --output_graph /workspace/output/3flowers-tf-default/graph.pb \\ --output_info /workspace/output/3flowers-tf-default/graph.json \\ --saved_model_dir /workspace/output/3flowers-tf-default/saved_model \\ --image_lists_dir /workspace/output/3flowers-tf-default \\ --tfhub_module https://tfhub.dev/google/imagenet/inception_v3/feature_vector/3 \\ --training_steps 500 CPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules -t waikatodatamining/tf_image_classification:1.14_cpu \\ tfic-retrain \\ --image_dir /workspace/data/3flowers \\ --output_graph /workspace/output/3flowers-tf-default/graph.pb \\ --output_info /workspace/output/3flowers-tf-default/graph.json \\ --saved_model_dir /workspace/output/3flowers-tf-default/saved_model \\ --image_lists_dir /workspace/output/3flowers-tf-default \\ --tfhub_module https://tfhub.dev/google/imagenet/inception_v3/feature_vector/3 \\ --training_steps 500","title":"Training"},{"location":"image_classification/wai.tfimageclass/#exporting-model","text":"Before we can use the model, we need to export it to Tensorflow lite or tflite : GPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules -t waikatodatamining/tf_image_classification:1.14 \\ tfic-export \\ --saved_model_dir /workspace/output/3flowers-tf-default/saved_model \\ --tflite_model /workspace/output/3flowers-tf-default/graph.tflite CPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/tfhub_modules -t waikatodatamining/tf_image_classification:1.14_cpu \\ tfic-export \\ --saved_model_dir /workspace/output/3flowers-tf-default/saved_model \\ --tflite_model /workspace/output/3flowers-tf-default/graph.tflite","title":"Exporting model"},{"location":"image_classification/wai.tfimageclass/#predicting","text":"For making predictions for a single image, you can use the script tfic-labelimage . Since we will want to batch predict multiple images, will use the script tfic-poll instead: GPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_image_classification:1.14 \\ tfic-poll \\ --graph /workspace/output/3flowers-tf-default/graph.tflite \\ --graph_type tflite \\ --info /workspace/output/3flowers-tf-default/graph.json \\ --in_dir /workspace/predictions/in \\ --out_dir /workspace/predictions/out CPU: docker run \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -t waikatodatamining/tf_image_classification:1.14_cpu \\ tfic-poll \\ --graph /workspace/output/3flowers-tf-default/graph.tflite \\ --graph_type tflite \\ --info /workspace/output/3flowers-tf-default/graph.json \\ --in_dir /workspace/predictions/in \\ --out_dir /workspace/predictions/out E.g., for the image_01965.jpg from the anthurium class, we will get a CSV file similar to this one : label probability anthurium 0.989429 artichoke 0.0060946 alpine_sea_holly 0.00447612","title":"Predicting"},{"location":"image_segmentation/","text":"Image segmentation classifies individual pixels within an image, assigning them a label (i.e., a color). Before you delve into training models, take a look at how you need to annotate your data . The following frameworks are available: Image Segmentation Keras MMSegmentation","title":"Introduction"},{"location":"image_segmentation/annotate/","text":"You can use the ADAMS framework for annotating your images. ADAMS comes pre-bundled in various setups and we need the adams-annotator bundle. You can get either: snapshot release (post 21.12.0 release) In the Flow editor (available from the Tools menu in ADAMS), you can load and execute the follow workflow (which is part of your ADAMS installation when downloading it as zip file): adams-imaging-image_segmentation_annotation.flow The following video takes you through the process:","title":"Annotate"},{"location":"image_segmentation/image-segmentation-keras/","text":"Image Segmenation Keras is a Keras/Tensorflow based image segmentation framework. Custom docker images with additional tools are available from here: https://github.com/waikato-datamining/tensorflow/tree/master/image-segmentation-keras Prerequisites Make sure you have the directory structure created as outlined in the Prerequisites . Data In this example, we will use the CamVid-12 dataset, which consists of still images from dashcam videos from a city environment (12 different labels). Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/camvid12/camvid12-grayscale.zip Once extracted, we have to convert the format from grayscale into blue channel , which the framework uses. We can do this by using the wai.annotations library. At the same time, we can split the dataset into train , validation and test subsets. From within the applied_deep_learning directory, run the following command: docker run -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:latest \\ wai-annotations convert \\ from-grayscale-is \\ -i \"/workspace/data/grayscale/*.png\" \\ --labels sky building pole road pavement tree signsymbol fence car pedestrian bicyclist unlabelled \\ to-blue-channel-is \\ -o /workspace/data/bluechannel-split \\ --split-names train val test \\ --split-ratios 70 15 15 Training For training, we will use the following docker image: waikatodatamining/image-segmentation-keras:1.14.0_0.3.0 The training script is called keras_seg_train , for which we can invoke the help screen as follows: docker run -t waikatodatamining/image-segmentation-keras:1.14.0_0.3.0 keras_seg_train --help It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: camvid12-keras-unet50 Image Segmentation Keras does not use a config file, but can be configured via command-line parameters instead: the number of classes ( --n_classes ) is actual classes plus background the width/height of the inputs must be multiples of 32 ( --input_height , --input_width ) the output directory is specified via --checkpoints_path (make sure to have a trailing slash!) in our case, the images and the annotations reside in the same directories, so --train_images and --train_annotations point to the same directory (analog for --val_images and --val_annotations ) with --epochs you can specify for how long the model will get trained --model_name specifies the architecture and backend (see here for available options) Kick off the training of a U-Net with a ResNet50 backend using the following command: docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/.keras \\ -t waikatodatamining/image-segmentation-keras:1.14.0_0.3.0 \\ keras_seg_train \\ --checkpoints_path /workspace/output/camvid12-keras-unet50/ \\ --train_images /workspace/data/bluechannel-split/train \\ --train_annotations /workspace/data/bluechannel-split/train \\ --val_images /workspace/data/bluechannel-split/val \\ --val_annotations /workspace/data/bluechannel-split/val \\ --epochs 10 \\ --n_classes 13 \\ --input_height 384 \\ --input_width 480 \\ --model_name resnet50_unet Predicting Using the keras_seg_poll script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/.keras \\ -t waikatodatamining/image-segmentation-keras:1.14.0_0.3.0 \\ keras_seg_poll \\ --checkpoints_path /workspace/output/camvid12-keras-unet50/ \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Example prediction","title":"\u00bb Image Segmentation Keras"},{"location":"image_segmentation/image-segmentation-keras/#prerequisites","text":"Make sure you have the directory structure created as outlined in the Prerequisites .","title":"Prerequisites"},{"location":"image_segmentation/image-segmentation-keras/#data","text":"In this example, we will use the CamVid-12 dataset, which consists of still images from dashcam videos from a city environment (12 different labels). Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/camvid12/camvid12-grayscale.zip Once extracted, we have to convert the format from grayscale into blue channel , which the framework uses. We can do this by using the wai.annotations library. At the same time, we can split the dataset into train , validation and test subsets. From within the applied_deep_learning directory, run the following command: docker run -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:latest \\ wai-annotations convert \\ from-grayscale-is \\ -i \"/workspace/data/grayscale/*.png\" \\ --labels sky building pole road pavement tree signsymbol fence car pedestrian bicyclist unlabelled \\ to-blue-channel-is \\ -o /workspace/data/bluechannel-split \\ --split-names train val test \\ --split-ratios 70 15 15","title":"Data"},{"location":"image_segmentation/image-segmentation-keras/#training","text":"For training, we will use the following docker image: waikatodatamining/image-segmentation-keras:1.14.0_0.3.0 The training script is called keras_seg_train , for which we can invoke the help screen as follows: docker run -t waikatodatamining/image-segmentation-keras:1.14.0_0.3.0 keras_seg_train --help It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: camvid12-keras-unet50 Image Segmentation Keras does not use a config file, but can be configured via command-line parameters instead: the number of classes ( --n_classes ) is actual classes plus background the width/height of the inputs must be multiples of 32 ( --input_height , --input_width ) the output directory is specified via --checkpoints_path (make sure to have a trailing slash!) in our case, the images and the annotations reside in the same directories, so --train_images and --train_annotations point to the same directory (analog for --val_images and --val_annotations ) with --epochs you can specify for how long the model will get trained --model_name specifies the architecture and backend (see here for available options) Kick off the training of a U-Net with a ResNet50 backend using the following command: docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/.keras \\ -t waikatodatamining/image-segmentation-keras:1.14.0_0.3.0 \\ keras_seg_train \\ --checkpoints_path /workspace/output/camvid12-keras-unet50/ \\ --train_images /workspace/data/bluechannel-split/train \\ --train_annotations /workspace/data/bluechannel-split/train \\ --val_images /workspace/data/bluechannel-split/val \\ --val_annotations /workspace/data/bluechannel-split/val \\ --epochs 10 \\ --n_classes 13 \\ --input_height 384 \\ --input_width 480 \\ --model_name resnet50_unet","title":"Training"},{"location":"image_segmentation/image-segmentation-keras/#predicting","text":"Using the keras_seg_poll script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/tmp/.keras \\ -t waikatodatamining/image-segmentation-keras:1.14.0_0.3.0 \\ keras_seg_poll \\ --checkpoints_path /workspace/output/camvid12-keras-unet50/ \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Example prediction","title":"Predicting"},{"location":"image_segmentation/mmsegmentation/","text":"MMSegmentation is a comprehensive and flexible framework for image segmentation that offers a wide variety of architectures. Custom docker images with additional tools are available from here: https://github.com/waikato-datamining/mmsegmentation Prerequisites Make sure you have the directory structure created as outlined in the Prerequisites . Data In this example, we will use the CamVid-12 dataset, which consists of still images from dashcam videos from a city environment (12 different labels). Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/camvid12/camvid12-grayscale.zip Once extracted, we have to convert the format from grayscale into indexed PNG , which MMSegmentation uses. We can do this by using the wai.annotations library. At the same time, we can split the dataset into train , validation and test subsets. From within the applied_deep_learning directory, run the following command: docker run -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:latest \\ wai-annotations convert \\ from-grayscale-is \\ -i \"/workspace/data/grayscale/*.png\" \\ --labels sky building pole road pavement tree signsymbol fence car pedestrian bicyclist unlabelled \\ to-indexed-png-is \\ -o /workspace/data/indexed-split \\ --split-names train val test \\ --split-ratios 70 15 15 Training For training, we will use the following docker image: waikatodatamining/mmsegmentation:0.25.0_cuda11.1 The training script is called mmseg_train , for which we can invoke the help screen as follows: docker run -t waikatodatamining/mmsegmentation:0.25.0_cuda11.1 mmseg_train --help It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: camvid12-mmseg-pspnet50 Before we can train, we will need to obtain and customize a config file. Within the container, you can find example configurations for various architectures in the following directory: /mmsegmentation/configs Using the mmseg_config command, we can expand and dump one of these configurations for our own purposes: docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -t waikatodatamining/mmsegmentation:0.25.0_cuda11.1 \\ mmseg_config \\ --config /mmsegmentation/configs/pspnet/pspnet_r50-d8_512x1024_40k_cityscapes.py \\ --output_config /workspace/output/camvid12-mmseg-pspnet50/pspnet_r50.py Open the pspnet_r50.py file in a text editor and perform the following operations: change num_classes to 12 (background not counted). change dataset_type to ExternalDataset and any occurrences of type in the train , test , val sections of the data dictionary. change data_root occurrences to /workspace/data/indexed-split (the directory above the train and val directories). change img_dir occurrences to img_dir=data_root+'/DIR', with DIR being the appropriate train , val or test do the same with ann_dir Kick off the training with the following command: docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMSEG_CLASSES=\"sky,building,pole,road,pavement,tree,signsymbol,fence,car,pedestrian,bicyclist,unlabelled\" \\ -t waikatodatamining/mmsegmentation:0.25.0_cuda11.1 \\ mmseg_train \\ /workspace/output/camvid12-mmseg-pspnet50/pspnet_r50.py \\ --work-dir /workspace/output/camvid12-mmseg-pspnet50 Predicting Using the mmseg_predict_poll script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMSEG_CLASSES=\"sky,building,pole,road,pavement,tree,signsymbol,fence,car,pedestrian,bicyclist,unlabelled\" \\ -t waikatodatamining/mmsegmentation:0.25.0_cuda11.1 \\ mmseg_predict_poll \\ --model /workspace/output/camvid12-mmseg-pspnet50/latest.pth \\ --config /workspace/output/camvid12-mmseg-pspnet50/pspnet_r50.py \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Notes The predictions can either be output in grayscale (default) or bluechannel format ( --prediction_format ). You can view the predictions with the ADAMS Preview browser and, e.g., the SimpleImageSegmentationAnnotationsHandler (which will overlay the PNG transparently on the JPG; may require some configuring via the ... button). Example prediction","title":"\u00bb MMSegmentation"},{"location":"image_segmentation/mmsegmentation/#prerequisites","text":"Make sure you have the directory structure created as outlined in the Prerequisites .","title":"Prerequisites"},{"location":"image_segmentation/mmsegmentation/#data","text":"In this example, we will use the CamVid-12 dataset, which consists of still images from dashcam videos from a city environment (12 different labels). Download the dataset from the following URL into the data directory and extract it: https://datasets.cms.waikato.ac.nz/ufdl/data/camvid12/camvid12-grayscale.zip Once extracted, we have to convert the format from grayscale into indexed PNG , which MMSegmentation uses. We can do this by using the wai.annotations library. At the same time, we can split the dataset into train , validation and test subsets. From within the applied_deep_learning directory, run the following command: docker run -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -t waikatoufdl/wai.annotations:latest \\ wai-annotations convert \\ from-grayscale-is \\ -i \"/workspace/data/grayscale/*.png\" \\ --labels sky building pole road pavement tree signsymbol fence car pedestrian bicyclist unlabelled \\ to-indexed-png-is \\ -o /workspace/data/indexed-split \\ --split-names train val test \\ --split-ratios 70 15 15","title":"Data"},{"location":"image_segmentation/mmsegmentation/#training","text":"For training, we will use the following docker image: waikatodatamining/mmsegmentation:0.25.0_cuda11.1 The training script is called mmseg_train , for which we can invoke the help screen as follows: docker run -t waikatodatamining/mmsegmentation:0.25.0_cuda11.1 mmseg_train --help It is good practice creating a separate sub-directory for each training run, with a directory name that hints at what dataset and model were used. So for our first training run, which will use mainly default parameters, we will create the following directory in the output folder: camvid12-mmseg-pspnet50 Before we can train, we will need to obtain and customize a config file. Within the container, you can find example configurations for various architectures in the following directory: /mmsegmentation/configs Using the mmseg_config command, we can expand and dump one of these configurations for our own purposes: docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -t waikatodatamining/mmsegmentation:0.25.0_cuda11.1 \\ mmseg_config \\ --config /mmsegmentation/configs/pspnet/pspnet_r50-d8_512x1024_40k_cityscapes.py \\ --output_config /workspace/output/camvid12-mmseg-pspnet50/pspnet_r50.py Open the pspnet_r50.py file in a text editor and perform the following operations: change num_classes to 12 (background not counted). change dataset_type to ExternalDataset and any occurrences of type in the train , test , val sections of the data dictionary. change data_root occurrences to /workspace/data/indexed-split (the directory above the train and val directories). change img_dir occurrences to img_dir=data_root+'/DIR', with DIR being the appropriate train , val or test do the same with ann_dir Kick off the training with the following command: docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMSEG_CLASSES=\"sky,building,pole,road,pavement,tree,signsymbol,fence,car,pedestrian,bicyclist,unlabelled\" \\ -t waikatodatamining/mmsegmentation:0.25.0_cuda11.1 \\ mmseg_train \\ /workspace/output/camvid12-mmseg-pspnet50/pspnet_r50.py \\ --work-dir /workspace/output/camvid12-mmseg-pspnet50","title":"Training"},{"location":"image_segmentation/mmsegmentation/#predicting","text":"Using the mmseg_predict_poll script, we can batch-process images placed in the predictions/in directory as follows (e.g., from our test subset): docker run \\ -u $(id -u):$(id -g) \\ --gpus=all \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/cache/torch:/.cache/torch \\ -e MMSEG_CLASSES=\"sky,building,pole,road,pavement,tree,signsymbol,fence,car,pedestrian,bicyclist,unlabelled\" \\ -t waikatodatamining/mmsegmentation:0.25.0_cuda11.1 \\ mmseg_predict_poll \\ --model /workspace/output/camvid12-mmseg-pspnet50/latest.pth \\ --config /workspace/output/camvid12-mmseg-pspnet50/pspnet_r50.py \\ --prediction_in /workspace/predictions/in \\ --prediction_out /workspace/predictions/out Notes The predictions can either be output in grayscale (default) or bluechannel format ( --prediction_format ). You can view the predictions with the ADAMS Preview browser and, e.g., the SimpleImageSegmentationAnnotationsHandler (which will overlay the PNG transparently on the JPG; may require some configuring via the ... button). Example prediction","title":"Predicting"},{"location":"instance_segmentation/","text":"Instance segmentation not only finds objects within images, it also learns the shape of the objects rather than just a simple rectangle/bounding box. However, this makes the algorithms computationally more expensive and more memory hungry. Before you delve into training models, take a look at how you need to annotate your data . The following frameworks are available: Detectron2","title":"Introduction"},{"location":"instance_segmentation/annotate/","text":"You can use the ADAMS framework for annotating your images. ADAMS comes pre-bundled in various setups and we need the adams-annotator bundle. You can get either: snapshot release (21.12.0 or later) In the Flow editor (available from the Tools menu in ADAMS), you can load and execute the follow workflow (which is part of your ADAMS installation when downloading it as zip file): adams-imaging-annotate_objects.flow When prompted, use object_shape as Selection type . The following video takes you through the process:","title":"Annotate"},{"location":"instance_segmentation/detectron2/","text":"","title":"\u00bb Detectron2"},{"location":"object_detection/","text":"Object detection detects one or more objects within an image and assigns each one of them a label, as opposed to image classification which assigns a lable to the whole image. The predicted locations are typically rectangles (aka bounding boxes). If you are looking for shapes or polygons, then have a look at instance segmentation . Before you delve into training models, take a look at how you need to annotate your data . The following frameworks are available: MMDetection Yolov5","title":"Introduction"},{"location":"object_detection/annotate/","text":"You can use the ADAMS framework for annotating your images. ADAMS comes pre-bundled in various setups and we need the adams-annotator bundle. You can get either: snapshot release (21.12.0 or later) In the Flow editor (available from the Tools menu in ADAMS), you can load and execute the follow workflow (which is part of your ADAMS installation when downloading it as zip file): adams-imaging-annotate_objects.flow When prompted, use bounding_box as Selection type . The following video takes you through the process:","title":"Annotate"},{"location":"object_detection/mmdetection/","text":"","title":"\u00bb MMDetection"},{"location":"object_detection/yolov5/","text":"","title":"\u00bb Yolov5"}]}